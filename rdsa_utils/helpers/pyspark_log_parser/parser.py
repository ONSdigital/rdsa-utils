"""Module to parse and load JSON logs in PySpark."""

import logging
from collections import defaultdict
from datetime import timedelta
from typing import Any, Dict, List

import boto3

from rdsa_utils.cdp.helpers.s3_utils import list_files, load_json
from rdsa_utils.helpers.pyspark_log_parser.ec2_pricing import calculate_pipeline_cost

logger = logging.getLogger(__name__)


def convert_value(value: int, unit: str) -> float:
    """Convert values based on unit type using built-in libraries.

    Parameters
    ----------
    value
        Raw value from logs.
    unit
        Type of conversion: 'ms' -> minutes, 'ns' -> minutes, 'bytes' -> megabytes.

    Returns
    -------
    float
        Converted value.

    Examples
    --------
    >>> convert_value(60000, 'ms')
    1.0
    >>> convert_value(60000000000, 'ns')
    1.0
    >>> convert_value(1048576, 'bytes')
    1.0
    """
    if not isinstance(value, (int, float)):  # Ensure value is numeric
        return 0.0

    if unit == "ms":
        return timedelta(milliseconds=value).total_seconds() / 60  # Convert to minutes
    elif unit == "ns":
        return value / 6e10  # Convert nanoseconds to minutes
    elif unit == "bytes":
        return value / (1024 * 1024)  # Convert bytes to MB
    return float(value)


def parse_pyspark_logs(
    log_data: Dict[str, Any],
    log_summary: bool = True,
) -> Dict[str, Any]:
    r"""Parse PySpark event log data and return a summary of key execution metrics.

    This function processes a Python dictionary containing parsed JSON log data.
    It expects the data structure to match the event log format generated by Spark
    when `spark.eventLog.enabled = true` is set.

    The function works on `events_1_spark-xxxx` JSON log file found in the folder
    created inside `spark.eventLog.dir`, which should be called something like
    `eventlog_v2_spark-xxxx`.

    To enable Spark event logging, configure your SparkSession as follows:

    ```python
    from pyspark.sql import SparkSession

    spark = (
        SparkSession.builder
        .appName("YourApp")
        .config("spark.eventLog.enabled", "true")
        .config("spark.eventLog.dir", "s3a://your-bucket/path")
        .getOrCreate()
    )
    ```

    This generates a folder inside `"s3a://your-bucket/path"`, where JSON logs for Spark
    jobs are stored.

    For a detailed explanation of available Spark task metrics, refer to:
    [Spark Monitoring Documentation](https://spark.apache.org/docs/latest/monitoring.html#executor-task-metrics)

    Parameters
    ----------
    log_data
        Parsed JSON log data from Spark event logs.
    log_summary
        Whether to log the summary in a human-readable format (default is True).

    Returns
    -------
    Dict[str, Any]
        A dictionary containing aggregated summary metrics.

    Examples
    --------
    >>> log_data = [
    ...     {
    ...         "Event": "SparkListenerTaskEnd",
    ...         "Task Metrics": {
    ...             "Executor Run Time": 60000,  # 1 min
    ...             "Executor CPU Time": 60000000000,  # 1 min
    ...             "Peak Execution Memory": 1048576  # 1 MB
    ...         }
    ...     },
    ...     {
    ...         "Event": "SparkListenerTaskEnd",
    ...         "Task Metrics": {
    ...             "Executor Run Time": 120000,  # 2 min
    ...             "Executor CPU Time": 120000000000,  # 2 min
    ...             "Peak Execution Memory": 2097152  # 2 MB
    ...         }
    ...     },
    ...     {
    ...         "Event": "SparkListenerApplicationStart",
    ...         "Timestamp": 1739793526775
    ...     },
    ...     {
    ...         "Event": "SparkListenerExecutorAdded",
    ...         "Executor Info": {"Total Cores": 4}
    ...     },
    ...     {
    ...         "Event": "SparkListenerStageSubmitted",
    ...         "Properties": {
    ...             "spark.executor.memory": "4g",
    ...             "spark.yarn.executor.memoryOverhead": "2g",
    ...             "spark.executor.cores": "4"
    ...         }
    ...     }
    ... ]
    >>> summary = parse_pyspark_logs(log_data)
    >>> print(summary)
    >>> {
    ...     'Executor Deserialize Time': 0.0,
    ...     'Executor Deserialize CPU Time': 0.0,
    ...     'Executor Run Time': 3.0,  # 1 min + 2 min
    ...     'Executor CPU Time': 3.0,  # 1 min + 2 min
    ...     'Peak Execution Memory': 2.0,  # Max of 1 MB and 2 MB
    ...     'Result Size': 0.0,
    ...     'JVM GC Time': 0.0,
    ...     'Result Serialization Time': 0.0,
    ...     'Memory Bytes Spilled': 0.0,
    ...     'Disk Bytes Spilled': 0.0,
    ...     'Shuffle Bytes Written': 0.0,
    ...     'Shuffle Write Time': 0.0,
    ...     'Shuffle Records Written': 0,
    ...     'Bytes Read': 0.0,
    ...     'Records Read': 0,
    ...     'Bytes Written': 0.0,
    ...     'Records Written': 0,
    ...     'Timestamp': 1739793526775,
    ...     'Total Cores': 4,
    ...     'Total Executors': 1,
    ...     'Memory Per Executor': 6.0,  # 4 GB + 2 GB overhead
    ...     'Total Memory': 6.0  # Memory Per Executor * Total Executors
    ... }
    """
    summary_metrics = defaultdict(
        int,
        {
            "Timestamp": None,
            "Total Cores": 0,
            "Total Executors": 0,
            "Memory Per Executor": 0,
            "Total Memory": 0,
        },
    )

    metric_mappings = {
        "Executor Deserialize Time": ("Executor Deserialize Time", "ms"),
        "Executor Deserialize CPU Time": ("Executor Deserialize CPU Time", "ns"),
        "Executor Run Time": ("Executor Run Time", "ms"),
        "Executor CPU Time": ("Executor CPU Time", "ns"),
        "Result Size": ("Result Size", "bytes"),
        "JVM GC Time": ("JVM GC Time", "ms"),
        "Result Serialization Time": ("Result Serialization Time", "ms"),
        "Memory Bytes Spilled": ("Memory Bytes Spilled", "bytes"),
        "Disk Bytes Spilled": ("Disk Bytes Spilled", "bytes"),
    }

    def update_metrics(task_metrics: Dict[str, Any]) -> None:
        for metric, (key, unit) in metric_mappings.items():
            summary_metrics[metric] += convert_value(task_metrics.get(key, 0), unit)

        summary_metrics["Peak Execution Memory"] = max(
            summary_metrics["Peak Execution Memory"],
            convert_value(task_metrics.get("Peak Execution Memory", 0), "bytes"),
        )

        summary_metrics["Shuffle Bytes Written"] += convert_value(
            task_metrics.get("Shuffle Write Metrics", {}).get(
                "Shuffle Bytes Written",
                0,
            ),
            "bytes",
        )
        summary_metrics["Shuffle Write Time"] += convert_value(
            task_metrics.get("Shuffle Write Metrics", {}).get("Shuffle Write Time", 0),
            "ns",
        )
        summary_metrics["Shuffle Records Written"] += task_metrics.get(
            "Shuffle Write Metrics",
            {},
        ).get("Shuffle Records Written", 0)

        summary_metrics["Bytes Read"] += convert_value(
            task_metrics.get("Input Metrics", {}).get("Bytes Read", 0),
            "bytes",
        )
        summary_metrics["Records Read"] += task_metrics.get(
            "Input Metrics",
            {},
        ).get("Records Read", 0)

        summary_metrics["Bytes Written"] += convert_value(
            task_metrics.get("Output Metrics", {}).get("Bytes Written", 0),
            "bytes",
        )
        summary_metrics["Records Written"] += task_metrics.get(
            "Output Metrics",
            {},
        ).get("Records Written", 0)

    for event in log_data:
        event_type = event.get("Event")

        if event_type == "SparkListenerTaskEnd":
            update_metrics(event.get("Task Metrics", {}))

        elif event_type == "SparkListenerApplicationStart":
            summary_metrics["Timestamp"] = event.get("Timestamp")

        elif event_type == "SparkListenerExecutorAdded":
            summary_metrics["Total Executors"] += 1
            summary_metrics["Total Cores"] += event["Executor Info"]["Total Cores"]

        elif event_type == "SparkListenerStageSubmitted":
            props = event.get("Properties", {})
            mem, overhead = props.get("spark.executor.memory", "0g"), props.get(
                "spark.yarn.executor.memoryOverhead",
                "0g",
            )

            # Keep memory values in gigabytes
            memory_value = int(mem[:-1])  # Remove 'g' and convert to int
            overhead_value = int(overhead[:-1])  # Remove 'g' and convert to int

            summary_metrics["Memory Per Executor"] = memory_value + overhead_value
            summary_metrics["Total Memory"] = (
                summary_metrics["Memory Per Executor"]
                * summary_metrics["Total Executors"]
            )
            summary_metrics["Total Cores"] = (
                int(props.get("spark.executor.cores", 0))
                * summary_metrics["Total Executors"]
            )

    if log_summary:
        logger.info("Summary of Task Metrics: %s", summary_metrics)

    return dict(summary_metrics)


def find_pyspark_log_files(
    client: boto3.client,
    bucket_name: str,
    folder: str,
) -> List[str]:
    """Find all PySpark log files in the specified folder.

    Parameters
    ----------
    client
        The boto3 S3 client instance.
    bucket_name
        The name of the S3 bucket.
    folder
        The folder to search for PySpark log files.

    Returns
    -------
    List[str]
        A list of S3 object keys for the PySpark log files.

    Examples
    --------
    >>> client = boto3.client('s3')
    >>> bucket_name = 'your-bucket-name'
    >>> folder = 'user/dominic.bean'
    >>> log_files = find_pyspark_log_files(client, bucket_name, folder)
    >>> print(log_files)
    ['user/dominic.bean/eventlog_v2_spark-1234/events_1_spark-1234', ...]
    """
    prefix = f"{folder}/"
    all_files = list_files(client, bucket_name, prefix)
    log_files = [
        file
        for file in all_files
        if file.startswith(f"{folder}/eventlog_v2_spark-") and "events_1_spark" in file
    ]
    return log_files


def process_pyspark_logs(
    client: boto3.client,
    s3_bucket: str,
    user_folder: str,
) -> Dict[str, List[Any]]:
    """Find all PySpark log files in specified S3 folder & calculate pipeline costs.

    Parameters
    ----------
    client
        The boto3 S3 client instance.
    s3_bucket
        The name of the S3 bucket.
    user_folder
        The folder to search for PySpark log files.

    Returns
    -------
    Dict[str, List[Any]]
        A dictionary containing aggregated log metrics and cost metrics.

    Examples
    --------
    >>> client = boto3.client('s3')
    >>> s3_bucket = "your-bucket-name"
    >>> user_folder = 'user/dominic.bean'
    >>> result = process_pyspark_logs(client, s3_bucket, user_folder)
    >>> print(result)
    {
        'agg_log_metrics': [...],
        'agg_cost_metrics': [...]
    }
    """
    log_files = find_pyspark_log_files(client, s3_bucket, user_folder)

    agg_log_metrics = []
    agg_cost_metrics = []
    for json_object_file_path in log_files:
        log_data = load_json(
            client,
            s3_bucket,
            json_object_file_path,
            multi_line=True,
        )

        metrics_dict = parse_pyspark_logs(log_data)
        agg_log_metrics.append(metrics_dict)

        agg_cost_metrics.append(calculate_pipeline_cost(metrics_dict, fetch_data=False))

    return {"agg_log_metrics": agg_log_metrics, "agg_cost_metrics": agg_cost_metrics}
