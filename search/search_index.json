{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"rdsa-utils","text":"<p>This site contains the project documentation for <code>rdsa-utils</code>, a suite of PySpark, Pandas, and general pipeline utils for Reproducible Data Science and Analysis (RDSA) projects.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ol> <li>API Reference</li> <li>Contribution Guide</li> <li>Branching &amp; Deployment Guide</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites are required for <code>rdsa-utils</code>:</p> <ul> <li>Python 3.8 or higher</li> </ul>"},{"location":"#dependency-update-pyspark","title":"Dependency Update: PySpark","text":"<p>To optimise the installation process and accommodate users with pre-installed environments, <code>pyspark</code> is now classified as a development dependency. This adjustment avoids potential conflicts in environments where <code>pyspark</code> is already available, such as Cloudera Data Platform.</p>"},{"location":"#for-users","title":"For Users:","text":"<ul> <li> <p>If your environment does not have <code>pyspark</code> pre-installed: You will need to manually install <code>pyspark</code> to utilise features dependent on it. This can be done by running <code>pip install pyspark==&lt;version&gt;</code> when setting up your environment, replacing <code>&lt;version&gt;</code> with the specific version required for your project.</p> </li> <li> <p>If <code>pyspark</code> is pre-installed in your environment: No additional action is required. This change ensures seamless integration without overwriting or conflicting with the existing <code>pyspark</code> installation.</p> </li> </ul> <p>This modification streamlines <code>rdsa-utils</code> for various use cases, enhancing both flexibility and user experience.</p>"},{"location":"#contact","title":"\ud83d\udcec Contact","text":"<p>For questions, support, or feedback about <code>rdsa-utils</code>, please email RDSA.Support@ons.gov.uk.</p>"},{"location":"branch_and_deploy_guide/","title":"Branching and Deployment Guide","text":""},{"location":"branch_and_deploy_guide/#overview","title":"Overview","text":"<p>Our branching strategy is designed to support Continuous Integration and  Continuous Deployment, ensuring smooth transitions between development,  testing, and production. </p> <p>This framework aims to maintain a stable codebase and streamline collaboration,  by separating in-progress work from production-ready content. </p> <p>The goal is to streamline our workflow, making it easier to integrate new features,  fix bugs, and release updates promptly.</p>"},{"location":"branch_and_deploy_guide/#branches","title":"Branches","text":"<ul> <li>Main Branch: Stable codebase reflecting the current production state.</li> <li>Development Branch: Active development branch where new features, bug fixes,  and improvements are merged.</li> </ul>"},{"location":"branch_and_deploy_guide/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Feature Branches: </p> <ul> <li>All new features and bug fixes are developed in separate branches created from   the <code>development</code> branch.</li> <li>Branch Naming Conventions:<ul> <li>Feature Branches: <code>feature-&lt;feature_name&gt;</code> for introducing new features.</li> <li>Bug Fixes: <code>fix-&lt;bug_description&gt;</code> for resolving bugs.</li> <li>Hotfixes: <code>hotfix-&lt;issue&gt;</code> for urgent fixes that go straight to production.</li> <li>Improvements/Refactors: <code>refactor-&lt;description&gt;</code> or <code>improvement-&lt;description&gt;</code> for code improvements.</li> <li>Documentation: <code>docs-&lt;change_description&gt;</code> for updates to documentation.</li> <li>Experimental: <code>experiment-&lt;experiment_name&gt;</code> for trial and exploratory work.</li> </ul> </li> </ul> </li> <li> <p>Merging to Development:</p> <ul> <li>Once a feature is complete and tested, it is merged into the <code>development</code> branch  through a pull request.</li> <li>Pull requests should be reviewed and approved by at least one other developer.</li> </ul> </li> <li> <p>Version Bumping:</p> <ul> <li>Before merging <code>development</code> into <code>main</code>, update the package version.  Follow semantic versioning principles (MAJOR.MINOR.PATCH).</li> <li>Use <code>bump2version</code> to bump <code>rdsa-utils</code> package version.  Example: <code>bump2version patch</code> (for a patch update),  <code>bump2version minor</code> (for a minor update),  or <code>bump2version major</code> (for a major update).</li> </ul> </li> <li> <p>Merging to Main:</p> <ul> <li>After a set of features is finalised in the <code>development</code> branch, and  the package version is bumped, merge <code>development</code> into <code>main</code>.</li> <li>This action triggers the automated deployment process  through GitHub Actions.</li> </ul> </li> <li> <p>Post-Merge Update:</p> <ul> <li>After merging into <code>main</code>, update the <code>development</code> branch with the  latest <code>main</code> branch changes. This ensures <code>development</code> is aligned with production.</li> </ul> </li> </ol>"},{"location":"branch_and_deploy_guide/#deployment-process-using-github-actions","title":"Deployment Process Using GitHub Actions","text":""},{"location":"branch_and_deploy_guide/#overview_1","title":"Overview","text":"<p>The deployment process is automated using GitHub Actions. This CI/CD pipeline is  triggered upon merging changes into the <code>main</code> branch.</p>"},{"location":"branch_and_deploy_guide/#steps-in-the-deployment-pipeline","title":"Steps in the Deployment Pipeline","text":"<ol> <li> <p>Trigger:</p> <ul> <li>The pipeline is triggered when a merge into <code>main</code> is detected.</li> </ul> </li> <li> <p>Build:</p> <ul> <li>The <code>rdsa-utils</code> package is built. Ensure that all tests are passed  and quality checks are satisfied.</li> </ul> </li> <li> <p>Publish to PyPI:</p> <ul> <li>The built package is published to the Python Package Index (PyPI).</li> </ul> </li> <li> <p>Create GitHub Release:</p> <ul> <li>A GitHub Release is created with the new version tag.</li> <li>Built artifacts, such as wheels or source distributions, are uploaded to this release.</li> </ul> </li> </ol>"},{"location":"branch_and_deploy_guide/#git-workflow-diagram","title":"Git Workflow Diagram","text":"<p>Below is a visual representation of our Git workflow, illustrating the process  from feature development through to deployment using GitHub Actions.</p> <pre><code>graph TD\n    A[Start Feature Development] --&gt; B[Create Feature Branch from Development]\n    B --&gt; C{Feature Complete and Tested?}\n    C --&gt;|No| B\n    C --&gt;|Yes| D[Merge Feature into Development via Pull Request]\n    D --&gt; E[Review and Approve Pull Request]\n    E --&gt; F[Development Branch: Ready for Release?]\n    F --&gt;|No| A\n    F --&gt;|Yes| G[Update Package Version -- semver]\n    G --&gt; H[Merge Development into Main]\n    H --&gt; I[Trigger Automated Deployment via GitHub Actions]\n    I --&gt; J[Build and Test rdsa-utils Package]\n    J --&gt; K[Publish to PyPI]\n    K --&gt; L[Create GitHub Release with Version Tag]\n    L --&gt; M[Update Development Branch with Main]\n    M --&gt; N[Continue Development or Start New Features]\n    N --&gt; A\n</code></pre>"},{"location":"branch_and_deploy_guide/#merging-development-to-main-a-guide-for-maintainers","title":"Merging Development to Main: A Guide for Maintainers","text":"<p>As <code>rdsa-utils</code> maintainers, ensuring a seamless transition from <code>development</code> to the <code>main</code> branch is essential. This process extends beyond mere code merging; it encompasses careful  preparation, version management, and detailed documentation to preserve the codebase's integrity  and reliability. Below is a straightforward guide on the procedure:</p>"},{"location":"branch_and_deploy_guide/#preparation","title":"Preparation","text":"<ul> <li>Initiate Merge Request:<ul> <li>Navigate to the GitHub repository's page and access the 'Pull Requests' section.</li> <li>Click on 'New Pull Request' to start the merging process. Select the <code>development</code>    branch as the source and the <code>main</code> branch as the target.</li> <li>Title the Merge Request with a relevant name that succinctly describes the set of    features, fixes, or improvements being merged.    Example: \"Release 1.2.0: Feature Enhancements and Bug Fixes\".</li> </ul> </li> </ul>"},{"location":"branch_and_deploy_guide/#review-and-approval","title":"Review and Approval","text":"<ul> <li> <p>Review Changes:</p> <ul> <li>Utilise GitHub's User Interface (UI) to review the changes introduced. This is    critical for spotting any potential issues before they make it into the <code>main</code> branch.</li> <li>Cross-reference the changes against the <code>CHANGELOG.md</code> file to ensure all updates,    fixes, and new features are properly documented.</li> </ul> </li> <li> <p>Approve Changes:</p> <ul> <li>Once satisfied with the review, click on the \"Review changes\" button in GitHub    and select \"Approve\" from the options. This indicates that the changes have been    reviewed and are considered ready for merging. If you're reviewing multiple files,    click on the \"Viewed\" checkbox for each file as you review them. This helps    manage and streamline the review process by marking files that have already been    checked.</li> </ul> </li> </ul>"},{"location":"branch_and_deploy_guide/#version-management-and-documentation","title":"Version Management and Documentation","text":"<ul> <li> <p>Bump Version:</p> <ul> <li>Before merging, it's essential to update the package version. Use the <code>bump2version</code>   command line tool to increment the version according to the nature of the changes    (patch, minor, or major). For example, run <code>bump2version patch</code> for a patch update    in your local development environment.</li> </ul> </li> <li> <p>Update CHANGELOG.md:</p> <ul> <li>In the <code>CHANGELOG.md</code> file, create a new header/section for the newly bumped version.</li> <li>Move all entries from the \"Unreleased\" section to the new version section.  This action effectively transfers the documentation of changes from being pending  release to being part of the new version's official changelog.</li> <li>Ensure the \"Unreleased\" section is left empty after this process,  ready for documenting future changes.</li> </ul> </li> <li> <p>Update <code>CHANGELOG.md</code> Release Links:</p> <ul> <li> <p>After bumping the version and updating the <code>CHANGELOG.md</code> with the new version  header and changes, proceed to update the \"Release Links\" section at the bottom  of the document. Add links to the new version's GitHub Release page and its PyPI listing,  following the existing format. For example:</p> <p><code>- rdsa-utils v0.1.9: [GitHub Release](https://github.com/ONSdigital/rdsa-utils/releases/tag/v0.1.9) |  [PyPI](https://pypi.org/project/rdsa-utils/0.1.9/)</code></p> </li> </ul> <p>This step ensures users and developers can easily find and access the specific  versions of <code>rdsa-utils</code> through their respective release pages and download links,  maintaining a comprehensive and navigable documentation.</p> </li> <li> <p>Final Review and Push:</p> <ul> <li>Review the changes one more time, ensuring that the version bump and <code>CHANGELOG.md</code>    updates are correctly applied.</li> <li>Push the commit(s) to the <code>development</code> branch. This action updates the    branch with the version change and changelog updates.</li> </ul> </li> </ul>"},{"location":"branch_and_deploy_guide/#merging-and-deployment","title":"Merging and Deployment","text":"<ul> <li>Merge to Main:<ul> <li>With all preparations complete and changes reviewed, proceed to merge    the <code>development</code> branch into the <code>main</code> branch.</li> <li>This action can be done through the GitHub UI by completing the pull request    initiated in the Preparation section of the guide.</li> <li>Merging to <code>main</code> automatically triggers the GitHub Actions workflow for deployment,    which includes building the package, publishing to PyPI, and creating a    GitHub Release with the new version tag.</li> </ul> </li> </ul>"},{"location":"branch_and_deploy_guide/#synchronizing-development-branch-post-merge","title":"Synchronizing Development Branch Post-Merge","text":"<p>After the pull request from <code>development</code> to <code>main</code> has been merged, it is crucial to  synchronise the <code>development</code> branch with the changes in <code>main</code>. Perform the following  steps to ensure that <code>development</code> stays up-to-date:</p> <ul> <li> <p>Pull Changes from Main:</p> <ul> <li>Execute <code>git pull origin main</code> to fetch and merge the latest changes from    the <code>main</code> branch into your current branch.</li> </ul> </li> <li> <p>Switch to Development Branch:</p> <ul> <li>Use <code>git checkout development</code> to switch from your current branch    to the <code>development</code> branch.</li> </ul> </li> <li> <p>Merge Main into Development:</p> <ul> <li>Run <code>git merge main</code> while on the <code>development</code> branch to merge the    changes from the <code>main</code> branch into <code>development</code>.</li> </ul> </li> <li> <p>Push Updated Development:</p> <ul> <li>After merging, push the updated <code>development</code> branch back to the remote    repository using <code>git push origin development</code>.</li> </ul> </li> </ul> <p>These commands ensure that the <code>development</code> branch reflects the latest  state of <code>main</code>, maintaining consistency across your project's branches.</p> <p>By adhering to these steps, you'll make the transition from development to production  smooth and efficient, ensuring the codebase remains stable and the release process flows  seamlessly. As maintainers, your pivotal role guarantees the <code>rdsa-utils</code> package's  reliability and efficiency for all users.</p>"},{"location":"contribution_guide/","title":"Contribution Guide","text":"<p>We welcome contributions. To contribute, please follow these guidelines:</p>"},{"location":"contribution_guide/#pull-requests","title":"Pull Requests","text":"<ul> <li>All pull requests should be made to the <code>development</code> branch. Please use the following Naming Conventions for your branches:<ul> <li>Feature Branches: <code>feature-&lt;feature_name&gt;</code> for introducing new features.</li> <li>Bug Fixes: <code>fix-&lt;bug_description&gt;</code> for resolving bugs.</li> <li>Hotfixes: <code>hotfix-&lt;issue&gt;</code> for urgent fixes that go straight to production.</li> <li>Improvements/Refactors: <code>refactor-&lt;description&gt;</code> or <code>improvement-&lt;description&gt;</code> for code improvements.</li> <li>Documentation: <code>docs-&lt;change_description&gt;</code> for updates to documentation.</li> <li>Experimental: <code>experiment-&lt;experiment_name&gt;</code> for trial and exploratory work.</li> </ul> </li> <li>Please make sure that your code passes all unit tests before submitting a pull request.</li> <li>Include unit tests with your code changes whenever possible, preferably written in pytest format.</li> <li>Make sure that all existing unit tests still pass with your code changes.</li> <li>Please ensure that your code is compliant with the project's coding style guidelines, which include:</li> <li>Writing docstrings in Scipy/numpy style format.</li> <li>Using type hints in Python functions.</li> <li>Adhering to the PEP 8 style guide for Python code.</li> <li>No wildcard imports.</li> <li>Import PySpark functions as <code>F</code>.</li> <li>Providing well-documented and easy-to-understand code, including clear variable and function names, as well as explanatory comments where necessary.</li> <li>If you are making a significant change to the codebase, please make sure to update the documentation to reflect the changes.</li> <li>If you are adding new functionality, please provide examples of how to use it in the project's documentation or in a separate README file.</li> <li>If you are fixing a bug, please include a description of the bug and how your changes address it.</li> <li>If you are adding a new dependency, please include a brief explanation of why it is necessary and what it does.</li> <li>If you are making significant changes to the project's architecture or design, please discuss your ideas with the project maintainers first to ensure they align with the project's goals and vision.</li> </ul>"},{"location":"contribution_guide/#house-style-for-example-functions-and-unit-tests","title":"House Style for Example Functions and Unit Tests","text":""},{"location":"contribution_guide/#function-house-style","title":"Function House Style","text":"<p>When writing functions in <code>rdsa-utils</code>, please follow these guidelines to ensure clarity, consistency, and ease of use for others:</p> <ul> <li>Use type hints in the function signature to clearly indicate input and   output types. Avoid repeating type information in the docstring \u2014 this reduces   duplication and keeps documentation clean.</li> <li>Docstrings must follow the Numpy/Scipy format, with clearly   defined <code>Parameters</code>, <code>Returns</code>, and an <code>Examples</code> section.</li> <li>Try to include an Examples section showing how the function can be   used in practice. These examples should be minimal, runnable, and   demonstrate typical use cases.</li> <li>Use clear, descriptive, and self-explanatory parameter and variable   names \u2014 avoid overly abbreviated or cryptic names.</li> <li>If a function is complex, split it into smaller helper functions   where possible, and ensure that each part has a clear purpose.</li> <li>Keep explanatory comments focused and purposeful. Comment on the   why rather than the what when the code\u2019s intent isn't obvious.</li> <li>Ensure the function is readable, maintainable, and easy for others   to understand \u2014 readability is preferred over overly clever one-liners.</li> </ul> <p>Note: If your function raises any exceptions, make sure to include a <code>Raises</code> section in the docstring describing the exception type and when it is triggered.</p>"},{"location":"contribution_guide/#example","title":"Example:","text":"<pre><code>def delete_file(\n    client: boto3.client,\n    bucket_name: str,\n    object_name: str,\n    overwrite: bool = False,\n) -&gt; bool:\n    \"\"\"Delete a file from an AWS S3 bucket.\n\n    Parameters\n    ----------\n    client\n        The boto3 S3 client instance.\n    bucket_name\n        The name of the bucket from which the file will be deleted.\n    object_name\n        The S3 object name of the file to delete.\n    overwrite\n        If False, the function will not delete the file if it does not exist;\n        set to True to ignore non-existence on delete.\n\n    Returns\n    -------\n    bool\n        True if the file was deleted successfully, otherwise False.\n\n    Examples\n    --------\n    &gt;&gt;&gt; client = boto3.client('s3')\n    &gt;&gt;&gt; delete_file(client, 'mybucket', 'folder/s3_file.txt')\n    True\n    \"\"\"\n</code></pre>"},{"location":"contribution_guide/#function-writing-checklist","title":"\u2705 Function Writing Checklist","text":"Requirement Check before submitting Type hints used in function signature Clearly specify input and output types; no duplication in docstring Docstring format Follow Numpy/Scipy style with <code>Parameters</code>, <code>Returns</code>, <code>Examples</code> Examples section included Example is runnable, minimal, and shows common usage Parameter and variable names are descriptive Avoid cryptic or overly abbreviated names Complex logic is broken into helpers Keep functions focused and small where possible Comments explain why, not what Only where the intent isn\u2019t obvious Code prioritises readability Prefer clear structure over clever one-liners"},{"location":"contribution_guide/#unit-test-house-style","title":"Unit Test House Style","text":"<ul> <li>Use test classes with clear, one-line docstrings explaining the   purpose of the class, where possible. For more complex test classes,   a short descriptive multi-line docstring is acceptable.</li> <li>Write test functions with concise, descriptive names and one-line docstrings   explaining the scenario being tested, where possible.   Longer explanations are fine if needed.</li> <li>Write separate test methods for each scenario.</li> <li>Use assertions that clearly show expected vs. actual results.</li> <li>Aim for full test coverage, including edge cases to ensure robustness   and handle unexpected input.</li> </ul>"},{"location":"contribution_guide/#example_1","title":"Example:","text":"<pre><code>class TestParseYaml:\n    \"\"\"Tests for parse_yaml function.\"\"\"\n\n    def test_expected(\n        self,\n        yaml_config_string,\n        expected_standard_config,\n    ):\n        \"\"\"Test expected functionality.\"\"\"\n        actual = parse_yaml(yaml_config_string)\n\n        assert actual == expected_standard_config\n\n    def test_invalid_yaml_string(self):\n        \"\"\"Test that invalid YAML input raises YAMLError.\"\"\"\n        with pytest.raises(YAMLError):\n            parse_yaml('invalid: [unclosed_list')\n</code></pre>"},{"location":"contribution_guide/#testing-checklist","title":"\u2705 Testing Checklist","text":"Requirement Check before submitting Test classes used where applicable One-line descriptive docstring where possible, multi-line if needed Test functions are clear and focused One scenario per test, descriptive function names Assertions are clear and meaningful Shows expected vs. actual results Edge cases are covered Aim for full coverage, not just the happy path Parameterised tests used if helpful Reduce duplication for similar test scenarios"},{"location":"contribution_guide/#issues","title":"Issues","text":"<p>If you find a bug or would like to request a feature, please open an issue on the project's GitHub page. When opening an issue, please provide as much detail as possible, including:</p> <ul> <li>A clear and descriptive title.</li> <li>A description of the problem you're experiencing, including steps to reproduce it.</li> <li>Any error messages or logs related to the issue.</li> <li>Your operating system and Python version (if relevant).</li> </ul> <p>Please search through the existing issues before opening a new one to avoid duplicates. If you find an existing issue that covers your problem, please add any additional information as a comment. Issues will be triaged and prioritized by the project maintainers.</p> <p>If you would like to contribute to the project by fixing an existing issue, please leave a comment on the issue to let the maintainers know that you are working on it.</p>"},{"location":"contribution_guide/#getting-started","title":"Getting Started","text":""},{"location":"contribution_guide/#installing-python","title":"Installing Python","text":"<p>Before getting started, you need to have Python 3.8 or higher installed on your system. You can download Python from the official website. Make sure to add Python to your <code>PATH</code> during the installation process.</p> <p>Alternatively, you can use Anaconda to create a Python 3.8 or higher virtual environment. Anaconda is a popular Python distribution that comes with many pre-installed scientific computing packages and tools. Here's how to create a new environment with Anaconda:</p> <ol> <li>Download and install Anaconda from the official website.</li> <li>Open the Anaconda prompt.</li> <li> <p>Create a new virtual environment with Python 3.8 or higher:</p> <p><code>conda create --name myenv python=3.8</code></p> </li> <li> <p>Activate the virtual environment:</p> <p><code>conda activate myenv</code></p> </li> </ol>"},{"location":"contribution_guide/#clone-the-repository","title":"Clone the Repository","text":"<p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/ONSdigital/rdsa-utils.git\ncd rdsa-utils\n</code></pre>"},{"location":"contribution_guide/#set-up-the-development-environment","title":"Set Up the Development Environment","text":"<p>We use a traditional <code>setup.py</code> approach for managing dependencies. To set up your development environment, first, ensure you have Python 3.8 to 3.13 installed.</p> <p>Then, to install the package in editable mode along with all development dependencies, run the following command:</p> <pre><code>pip3 install -e .[dev]\n</code></pre> <p>The <code>-e</code> (or <code>--editable</code>) option is used to install the package in a way that allows you to modify the source code and see the changes directly without having to reinstall the package. This is particularly useful for development.</p>"},{"location":"contribution_guide/#running-tests","title":"Running Tests","text":"<p>To run tests, ensure you're in the top-level directory of the project and execute:</p> <pre><code>pytest\n</code></pre> <p>This will run all the tests using the configurations set in the project.</p>"},{"location":"contribution_guide/#installing-pre-commit-hooks-in-your-development-environment","title":"Installing Pre-commit Hooks in Your Development Environment","text":"<p>Pre-commit hooks are used to automate checks and formatting before commits. Follow these steps to set them up:</p>"},{"location":"contribution_guide/#installation-steps","title":"Installation Steps","text":"<ol> <li>Install pre-commit: If you haven't already, install the pre-commit package:</li> </ol> <pre><code>pip install pre-commit\n</code></pre> <ol> <li>Install pre-commit hooks: Install the hooks defined in <code>.pre-commit-config.yaml</code>:</li> </ol> <pre><code>pre-commit install\n</code></pre> <p>This sets up the hooks to run automatically before each commit.</p>"},{"location":"contribution_guide/#usage","title":"Usage","text":"<p>The pre-commit hooks will automatically run on your modified files whenever you commit. To manually run all hooks on all files, use:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This can be useful for checking your codebase.</p> <p>By following these steps, your development environment for <code>rdsa-utils</code> will be ready, and you can start contributing to the project with ease.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the<code>rdsa-utils</code> codebase.</p>"},{"location":"reference/#general","title":"General","text":""},{"location":"reference/#rdsa_utils.exceptions","title":"<code>rdsa_utils.exceptions</code>","text":"<p>Common custom exceptions that can be raised in pipelines.</p> <p>The purpose of these is to provide a clearer indication of why an error is being raised over the standard builtin errors (e.g. <code>ColumnNotInDataframeError</code> vs <code>ValueError</code>).</p>"},{"location":"reference/#rdsa_utils.exceptions.ColumnNotInDataframeError","title":"<code>ColumnNotInDataframeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when a column is not present in dataframe.</p>"},{"location":"reference/#rdsa_utils.exceptions.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when there is an issue in a config object.</p>"},{"location":"reference/#rdsa_utils.exceptions.DataframeEmptyError","title":"<code>DataframeEmptyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when a dataframe is empty.</p>"},{"location":"reference/#rdsa_utils.exceptions.InvalidBucketNameError","title":"<code>InvalidBucketNameError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when an AWS S3 or GCS bucket name is invalid.</p>"},{"location":"reference/#rdsa_utils.exceptions.InvalidS3FilePathError","title":"<code>InvalidS3FilePathError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when an AWS S3 file path is invalid.</p>"},{"location":"reference/#rdsa_utils.exceptions.PipelineError","title":"<code>PipelineError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when there is a generic pipeline issue.</p>"},{"location":"reference/#rdsa_utils.exceptions.TableNotFoundError","title":"<code>TableNotFoundError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to raise when a table to be read is not found.</p>"},{"location":"reference/#rdsa_utils.logging","title":"<code>rdsa_utils.logging</code>","text":"<p>Contains the logging configuration for files and method to initialise it.</p>"},{"location":"reference/#rdsa_utils.logging.add_warning_message_to_function","title":"<code>add_warning_message_to_function(_func: Callable = None, *, message: Optional[str] = None) -&gt; Callable</code>","text":"<p>Apply decorator to log a warning message.</p> <p>If a message is passed, this decorator adds a warning log of the form function_name: message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str]</code> <p>The message to be logged along with the function name.</p> <code>None</code> Notes <p>Explainer on complex decorators (and template for decorator structure): https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread</p> Usage <p>To use decorator to log a warning:</p> <pre><code>&gt;&gt;&gt; @_add_warning_message_to_function(message='here be dragons...')\n&gt;&gt;&gt; def my_func(some_args, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_output = my_func(...)\n\nWarning my_func: here be dragons...\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.init_logger_advanced","title":"<code>init_logger_advanced(log_level: int, handlers: Optional[List[logging.Handler]] = None, log_format: str = None, date_format: str = None) -&gt; None</code>","text":"<p>Set up the root logger with specified settings, avoiding duplicate handlers.</p> <p>This function initialises the root logger at the desired log_level, applies a consistent message and date format, and attaches any provided handlers. If the root logger already has handlers (e.g., from a previous call or another module), it returns immediately to prevent duplicate setup.</p> <p>Recommended usage: call this function once during application startup\u2014 ideally in your package\u2019s top-level <code>__init__.py</code> \u2014- to enforce consistent logging configuration across all modules.</p> <p>After initialisation, modules should obtain their own logger via: <code>logger = logging.getLogger(__name__)</code></p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>The logging level (e.g., logging.INFO, logging.DEBUG) for all messages emitted by the root logger.</p> required <code>handlers</code> <code>Optional[List[Handler]]</code> <p>A list of handler instances (e.g., StreamHandler, FileHandler) to attach. If None or empty, <code>logging.basicConfig</code> is used with the given formats.</p> <code>None</code> <code>log_format</code> <code>str</code> <p>A <code>logging.Formatter</code> format string for log messages. Defaults to: \"%(asctime)s %(levelname)s %(name)s: %(message)s\".</p> <code>None</code> <code>date_format</code> <code>str</code> <p>A strftime format string for timestamps. Defaults to: \"%Y-%m-%d %H:%M:%S\".</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any item in handlers is not an instance of <code>logging.Handler</code>.</p> Notes <ul> <li>Checks <code>root_logger.hasHandlers()</code>: if True, exits without changes.</li> <li>Sets the root logger's level to log_level before adding handlers.</li> <li>If no handlers provided, calls <code>logging.basicConfig</code> after formatter setup.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import logging\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; from rdsa_utils.logging import init_logger_advanced\n&gt;&gt;&gt; # Initialise once in your package\u2019s __init__.py\n&gt;&gt;&gt; init_logger_advanced(\n...     log_level=logging.INFO,\n...     handlers=[logging.StreamHandler(sys.stdout)],\n...     log_format=\"%(levelname)s: %(message)s\",\n...     date_format=\"%H:%M:%S\"\n... )\n&gt;&gt;&gt; # In other modules, get a named logger\n&gt;&gt;&gt; logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.init_logger_basic","title":"<code>init_logger_basic(log_level: int) -&gt; None</code>","text":"<p>Instantiate a basic logger object to be used across modules.</p> <p>By using this function to instantiate the logger, you also have access to <code>logger.dev</code> for log_level=15, as this is defined in the same module scope as this function.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>The level of logging to be recorded. Can be defined either as the integer level or the logging. values in line with the definitions of the logging module (see - https://docs.python.org/3/library/logging.html#levels) required <p>Returns:</p> Type Description <code>None</code> <p>The logger created by this function is available in any other modules by using <code>logger = logging.getLogger(__name__)</code> at the global scope level in a module (i.e. below imports, not in a function).</p>"},{"location":"reference/#rdsa_utils.logging.log_dev","title":"<code>log_dev(self, message, *args, **kwargs)</code>","text":"<p>Create a custom log level between INFO and DEBUG named DEV.</p> <p>This is lifted from: https://stackoverflow.com/a/13638084</p>"},{"location":"reference/#rdsa_utils.logging.log_rows_in_spark_df","title":"<code>log_rows_in_spark_df(func: Callable) -&gt; Callable</code>","text":"<p>Apply decorator to log dataframe row count before and after a function.</p> <p>Requires that the function being decorated has a parameter called <code>df</code> and that the function is called with <code>df</code> being a keyword argument (e.g. <code>df=df</code>). If not the decorator will report back that it could not count the number of rows of the dataframe before running the decorated function.</p> Usage <pre><code>@log_rows_in_spark_df\ndef my_func_that_changes_no_rows(some_args, df, some_other_args):\n   ...\n   returns final_df\n\nsome_df = my_func_that_changes_no_rows(\n    some_args='hello',\n    df=input_df,\n    some_other_args='world'\n)\n\n&gt;&gt;&gt; Rows in dataframe before my_func_that_changes_no_rows : 12345\n&gt;&gt;&gt; Rows in dataframe after my_func_that_changes_no_rows  : 6789\n</code></pre> Warning: <p><code>.count()</code> is an expensive spark operation to perform. Overuse of this decorator can be detrimental to performance. This decorator will cache the input dataframe prior to running the count and decorated function, as well as persisting the output dataframe prior to counting. The input dataframe is also unpersisted from memory prior to the decorator completing.</p>"},{"location":"reference/#rdsa_utils.logging.log_spark_df_schema","title":"<code>log_spark_df_schema(_func: Callable = None, *, log_schema_on_input: bool = True) -&gt; Callable</code>","text":"<p>Apply decorator to log dataframe schema before and after a function.</p> <p>If you use the `df.printSchema() method directly in a print/log statement the code is processed and printed regardless of logging leve. Instead you need to capture the output and pass this to the logger. See explanaition here - https://stackoverflow.com/a/59935109</p> <p>Requires that the function being decorated has a parameter called <code>df</code> and that the function is called with <code>df</code> being a keyword argument (e.g. <code>df=df</code>). If not the decorator will report back that it could not count the number of rows of the dataframe before running the decorated function.</p> <p>Parameters:</p> Name Type Description Default <code>log_schema_on_input</code> <code>bool</code> <p>If set to false, then no schema is attempted to be printed for the decorated function on input. This is useful for instance where function has no df input but does return one (such as when reading a table).</p> <code>True</code> Notes <p>Explainer on complex decorators (and template for decorator structure): https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread</p> Usage <p>To use decorator to record input and output schema:</p> <pre><code>&gt;&gt;&gt; @log_spark_df_schema\n&gt;&gt;&gt; def my_func_that_changes_some_columns(some_args, df, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;    returns final_df\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_df = my_func_that_changes_some_columns(\n&gt;&gt;&gt;     some_args='hello',\n&gt;&gt;&gt;     df=input_df,\n&gt;&gt;&gt;     some_other_args='world'\n&gt;&gt;&gt; )\n\nSchema of dataframe before my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n</code></pre> <p>To use decorator to record output schema only:</p> <pre><code>&gt;&gt;&gt; @log_spark_df_schema(log_schema_on_input=False)\n&gt;&gt;&gt; def my_func_that_changes_some_columns(some_args, df, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;    returns final_df\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_df = my_func_that_changes_some_columns(\n&gt;&gt;&gt;     some_args='hello',\n&gt;&gt;&gt;     df=input_df,\n&gt;&gt;&gt;     some_other_args='world'\n&gt;&gt;&gt; )\n\nNot printing schema of dataframe before my_func_that_changes_some_columns\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.print_full_table_and_raise_error","title":"<code>print_full_table_and_raise_error(df: pd.DataFrame, message: str, stop_pipeline: bool = False, show_records: bool = False) -&gt; None</code>","text":"<p>Output dataframe records to logger.</p> <p>The purpose of this function is to enable a user to output a message to the logger with the added functionality of stopping the pipeline and showing dataframe records in a table format. It may be used for instance if a user wants to check the records in a dataframe when it expected to be empty.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to display records from.</p> required <code>message</code> <code>str</code> <p>The message to output to the logger.</p> required <code>stop_pipeline</code> <code>bool</code> <p>Switch for the user to stop the pipeline and raise an error.</p> <code>False</code> <code>show_records</code> <code>bool</code> <p>Switch to show records in a dataframe.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays message to user however nothing is returned from function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises error and stops pipeline if switch applied.</p>"},{"location":"reference/#rdsa_utils.logging.timer_args","title":"<code>timer_args(name: str, logger: Optional[Callable[[str], None]] = logger.info) -&gt; Dict[str, str]</code>","text":"<p>Initialise timer args workaround for 'text' args in codetiming package.</p> <p>Works with codetiming==1.4.0</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the specific timer log.</p> required <code>logger</code> <code>Optional[Callable[[str], None]]</code> <p>Optional logger function that can accept a string argument.</p> <code>info</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of arguments to pass to specifc codetiming package Timer.</p>"},{"location":"reference/#rdsa_utils.test_utils","title":"<code>rdsa_utils.test_utils</code>","text":"<p>Functions and fixtures used with test suites.</p>"},{"location":"reference/#rdsa_utils.test_utils.Case","title":"<code>Case(label: Optional[str] = None, marks: Optional[MarkDecorator] = None, **kwargs)</code>","text":"<p>Container for a test case, with optional test ID.</p> <p>The Case class is to be used in conjunction with <code>parameterize_cases</code>.</p> <p>Attributes:</p> Name Type Description <code>    label</code> <pre><code>Optional test ID. Will be displayed for each test when\nrunning `pytest -v`.\n</code></pre> <p>marks     Optional pytest marks to denote any tests to skip etc. kwargs     Parameters used for the test cases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Case(label=\"some test name\", foo=10, bar=\"some value\")\n\n&gt;&gt;&gt; Case(\n&gt;&gt;&gt;     label=\"some test name\",\n&gt;&gt;&gt;     marks=pytest.mark.skip(reason='not implemented'),\n&gt;&gt;&gt;     foo=10,\n&gt;&gt;&gt;     bar=\"some value\"\n&gt;&gt;&gt; )\n</code></pre> See Also <p>Modified from https://github.com/ckp95/pytest-parametrize-cases to allow pytest mark usage.</p> <p>Initialise objects.</p>"},{"location":"reference/#rdsa_utils.test_utils.Case.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Return string.</p>"},{"location":"reference/#rdsa_utils.test_utils.create_dataframe","title":"<code>create_dataframe(data: List[Tuple[str]], **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Create pandas df from tuple data with a header.</p>"},{"location":"reference/#rdsa_utils.test_utils.create_spark_df","title":"<code>create_spark_df(spark_session)</code>","text":"<p>Create Spark DataFrame from tuple data with first row as schema.</p> Example: <p>create_spark_df([     ('column1', 'column2', 'column3'),     ('aaaa', 1, 1.1) ])</p> <p>Can specify the schema alongside the column names: create_spark_df([     ('column1 STRING, column2 INT, column3 DOUBLE'),     ('aaaa', 1, 1.1) ])</p>"},{"location":"reference/#rdsa_utils.test_utils.parametrize_cases","title":"<code>parametrize_cases(*cases: Case)</code>","text":"<p>More user friendly parameterize cases testing.</p> <p>Utilise as a decorator on top of test function.</p> <p>Examples:</p> <pre><code>@parameterize_cases(\n    Case(\n        label=\"some test name\",\n        foo=10,\n        bar=\"some value\"\n    ),\n    Case(\n        label=\"some test name #2\",\n        foo=20,\n        bar=\"some other value\"\n    ),\n)\ndef test(foo, bar):\n    ...\n</code></pre> See Also <p>Source: https://github.com/ckp95/pytest-parametrize-cases</p>"},{"location":"reference/#rdsa_utils.test_utils.spark_session","title":"<code>spark_session()</code>","text":"<p>Set up spark session fixture.</p>"},{"location":"reference/#rdsa_utils.test_utils.suppress_py4j_logging","title":"<code>suppress_py4j_logging()</code>","text":"<p>Suppress spark logging.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_date","title":"<code>to_date(dt: str) -&gt; datetime.date</code>","text":"<p>Convert date string to datetime.date type.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_datetime","title":"<code>to_datetime(dt: str) -&gt; datetime.datetime</code>","text":"<p>Convert datetime string to datetime.datetime type.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_spark","title":"<code>to_spark(spark_session)</code>","text":"<p>Convert pandas df to spark.</p>"},{"location":"reference/#rdsa_utils.typing","title":"<code>rdsa_utils.typing</code>","text":"<p>Contains custom types for type hinting.</p>"},{"location":"reference/#rdsa_utils.validation","title":"<code>rdsa_utils.validation</code>","text":"<p>Functions that support the use of pydantic validators.</p>"},{"location":"reference/#rdsa_utils.validation.allowed_date_format","title":"<code>allowed_date_format(date: str) -&gt; str</code>","text":"<p>Ensure that the date string can be converted to a useable datetime.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>The specified date string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input date.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the date is not one of the predefined allowed formats.</p>"},{"location":"reference/#rdsa_utils.validation.apply_validation","title":"<code>apply_validation(config: Mapping[str, Any], Validator: Optional[BaseModel]) -&gt; Mapping[str, Any]</code>","text":"<p>Apply validation model to config.</p> <p>If no Validator is passed, then a warning will be logged and the input config returned without validation. This mechanism is to allow the use of this function to aid in tracking config sections that are unvalidated.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Mapping[str, Any]</code> <p>The config for validating.</p> required <code>Validator</code> <code>Optional[BaseModel]</code> <p>Validator class for the config.</p> required <code>optional</code> <code>Optional[BaseModel]</code> <p>Validator class for the config.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Any]</code> <p>The input config after being passed through the validator.</p>"},{"location":"reference/#rdsa_utils.validation.list_convert_validator","title":"<code>list_convert_validator(*args, **kwargs) -&gt; Callable</code>","text":"<p>Wrapper to set kwargs for list_convert validator.</p>"},{"location":"reference/#cdp","title":"CDP","text":""},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils","title":"<code>rdsa_utils.cdp.helpers.s3_utils</code>","text":"<p>Utility functions for interacting with AWS S3.</p> <p>To initialise a boto3 client for S3 and configure it with Ranger RAZ and SSL certificate, you can use the following code snippet:</p> <pre><code>\nimport boto3\nimport raz_client\n\nssl_file_path = \"/path/to/your/ssl_certificate.crt\"\n\n# Create a boto3 client for S3\nclient = boto3.client(\"s3\")\n\n# Configure the client with RAZ and SSL certificate\nraz_client.configure_ranger_raz(client, ssl_file=ssl_file_path)\n\nNote:\n- The `raz-client` library is required only when running in a\n  managed Cloudera environment.\n- You can install it using `pip install raz-client` when needed.\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.check_file","title":"<code>check_file(client: boto3.client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Check if a file exists in an S3 bucket and meets specific criteria.</p> <p>Verifies that the given path corresponds to a file in an S3 bucket, ensuring it exists, is not a directory, and has a size greater than 0.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The path to a file in s3 bucket.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, is not a directory, and size &gt; 0, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; check_file(client, 'mybucket', 'folder/file.txt')\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; check_file(client, 'mybucket', 'folder/nonexistent_file.txt')\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; check_file(client, 'mybucket', 'folder/')\nFalse\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.copy_file","title":"<code>copy_file(client: boto3.client, source_bucket_name: str, source_object_name: str, destination_bucket_name: str, destination_object_name: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Copy a file from one aWS S3 bucket to another.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>source_bucket_name</code> <code>str</code> <p>The name of the source bucket.</p> required <code>source_object_name</code> <code>str</code> <p>The S3 object name of the source file.</p> required <code>destination_bucket_name</code> <code>str</code> <p>The name of the destination bucket.</p> required <code>destination_object_name</code> <code>str</code> <p>The S3 object name of the destination file.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the destination file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was copied successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; copy_file(\n...     client,\n...     'source-bucket',\n...     'source_file.txt',\n...     'destination-bucket',\n...     'destination_file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.create_folder","title":"<code>create_folder(client: boto3.client, bucket_name: str, folder_path: str) -&gt; bool</code>","text":"<p>Create a folder in an AWS S3 bucket if it doesn't already exist.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket where the folder will be created.</p> required <code>folder_path</code> <code>str</code> <p>The name of the folder to create.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was created successfully or already exists, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; create_folder(client, 'mybucket', 'new_folder/')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.delete_file","title":"<code>delete_file(client: boto3.client, bucket_name: str, object_name: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Delete a file from an AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket from which the file will be deleted.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name of the file to delete.</p> required <code>overwrite</code> <code>bool</code> <p>If False, the function will not delete the file if it does not exist; set to True to ignore non-existence on delete.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was deleted successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; delete_file(client, 'mybucket', 'folder/s3_file.txt')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.delete_folder","title":"<code>delete_folder(client: boto3.client, bucket_name: str, folder_path: str) -&gt; bool</code>","text":"<p>Delete a folder in an AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>folder_path</code> <code>str</code> <p>The path of the folder to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was deleted successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; delete_folder(client, 'mybucket', 'path/to/folder/')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.delete_old_objects_and_folders","title":"<code>delete_old_objects_and_folders(client: boto3.client, bucket_name: str, prefix: str, age: str, dry_run: bool = False) -&gt; bool</code>","text":"<p>Delete objects and folders in an S3 bucket that are older than a specified age.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix to filter objects.</p> required <code>age</code> <code>str</code> <p>The age threshold for deleting objects. Supported formats: - \"1 day\", \"2 days\", etc. - \"1 week\", \"2 weeks\", etc. - \"1 month\", \"2 months\", etc.</p> required <code>dry_run</code> <code>bool</code> <p>If True, the function will only log the objects and folders that would be deleted, without actually performing the deletion. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the objects and folders were (or would be) deleted successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; # This will actually delete objects:\n&gt;&gt;&gt; delete_old_objects_and_folders(client, 'mybucket', 'folder/', '1 week')\nTrue\n&gt;&gt;&gt; # This will only log the objects/folders to be deleted:\n&gt;&gt;&gt; delete_old_objects_and_folders(\n...     client,\n...     'mybucket',\n...     'folder/',\n...     '1 week',\n...     dry_run=True\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.download_file","title":"<code>download_file(client: boto3.client, bucket_name: str, object_name: str, local_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Download a file from an AWS S3 bucket to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket from which to download the file.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name of the file to download.</p> required <code>local_path</code> <code>str</code> <p>The local file path where the downloaded file will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the local file if it exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was downloaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; download_file(\n...     client,\n...     'mybucket',\n...     'folder/s3_file.txt',\n...     '/path/to/download.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.download_folder","title":"<code>download_folder(client: boto3.client, bucket_name: str, prefix: str, local_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Download a folder from an AWS S3 bucket to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket from which to download the folder.</p> required <code>prefix</code> <code>str</code> <p>The S3 prefix of the folder to download.</p> required <code>local_path</code> <code>str</code> <p>The local directory path where the downloaded folder will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing local files if they exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was downloaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; download_folder(\n...     client,\n...     'mybucket',\n...     'folder/subfolder/',\n...     '/path/to/local_folder',\n...     overwrite=False\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.file_exists","title":"<code>file_exists(client: boto3.client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Check if a specific file exists in an AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to check for existence.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; file_exists(client, 'mybucket', 'folder/file.txt')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.file_size","title":"<code>file_size(client: boto3.client, bucket_name: str, object_name: str) -&gt; int</code>","text":"<p>Check the size of a file in an AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to check for size.</p> required <p>Returns:</p> Type Description <code>int</code> <p>An integer value indicating the size of the file in bytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; file_size(client, 'mybucket', 'folder/file.txt')\n8\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.is_s3_directory","title":"<code>is_s3_directory(client: boto3.client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Check if an AWS S3 key is a directory by listing its contents.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the key represents a directory, False otherwise.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.list_files","title":"<code>list_files(client: boto3.client, bucket_name: str, prefix: str = '') -&gt; List[str]</code>","text":"<p>List files in an AWS S3 bucket that match a specific prefix.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix to filter files, by default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of S3 object keys matching the prefix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; list_files(client, 'mybucket', 'folder_prefix/')\n['folder_prefix/file1.txt', 'folder_prefix/file2.txt']\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.load_csv","title":"<code>load_csv(client: boto3.client, bucket_name: str, filepath: str, keep_columns: Optional[List[str]] = None, rename_columns: Optional[Dict[str, str]] = None, drop_columns: Optional[List[str]] = None, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Load a CSV file from an S3 bucket into a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>filepath</code> <code>str</code> <p>The key (full path and filename) of the CSV file in the S3 bucket.</p> required <code>keep_columns</code> <code>Optional[List[str]]</code> <p>A list of column names to keep in the DataFrame, dropping all others. Default value is None.</p> <code>None</code> <code>rename_columns</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary to rename columns where keys are existing column names and values are new column names. Default value is None.</p> <code>None</code> <code>drop_columns</code> <code>Optional[List[str]]</code> <p>A list of column names to drop from the DataFrame. Default value is None.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>pd.read_csv</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame containing the data from the CSV file.</p> <p>Raises:</p> Type Description <code>InvalidBucketNameError</code> <p>If the bucket name does not meet AWS specifications.</p> <code>InvalidS3FilePathError</code> <p>If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.</p> <code>Exception</code> <p>If there is an error loading the file.</p> <code>ValueError</code> <p>If a column specified in rename_columns, drop_columns, or keep_columns is not found in the DataFrame.</p> Notes <p>Transformation order: 1. Columns are kept according to <code>keep_columns</code>. 2. Columns are dropped according to <code>drop_columns</code>. 3. Columns are renamed according to <code>rename_columns</code>.</p> <p>Examples:</p> <p>Load a CSV file and rename columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        rename_columns={\"old_name\": \"new_name\"}\n    )\n</code></pre> <p>Load a CSV file and keep only specific columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        keep_columns=[\"col1\", \"col2\"]\n    )\n</code></pre> <p>Load a CSV file and drop specific columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        drop_columns=[\"col1\", \"col2\"]\n    )\n</code></pre> <p>Load a CSV file with custom delimiter:</p> <pre><code>&gt;&gt;&gt; df = load_csv(\n        client,\n        \"my-bucket\",\n        \"path/to/file.csv\",\n        sep=\";\"\n    )\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.load_json","title":"<code>load_json(client: boto3.client, bucket_name: str, filepath: str, encoding: Optional[str] = 'utf-8', multi_line: bool = False) -&gt; Union[Dict, List[Dict]]</code>","text":"<p>Load a JSON file from an S3 bucket, with optional line-by-line parsing.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>filepath</code> <code>str</code> <p>The key (full path and filename) of the JSON file in the S3 bucket.</p> required <code>encoding</code> <code>Optional[str]</code> <p>The encoding of the JSON file.</p> <code>'utf-8'</code> <code>multi_line</code> <code>bool</code> <p>If True, reads the JSON file line by line, treating each line as a separate JSON object.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Dict, List[Dict]]</code> <ul> <li>If <code>multi_line=False</code>: A dictionary containing the parsed JSON data.</li> <li>If <code>multi_line=True</code>: A list of dictionaries, each corresponding to   a JSON object per line.</li> </ul> <p>Raises:</p> Type Description <code>InvalidBucketNameError</code> <p>If the bucket name is invalid according to AWS rules.</p> <code>Exception</code> <p>If there is an error loading the file from S3 or parsing the JSON.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; data = load_json(client, 'my-bucket', 'path/to/file.json')\n&gt;&gt;&gt; print(data)\n{\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"Manchester\"\n}\n</code></pre> <pre><code>&gt;&gt;&gt; log_data = load_json(client, 'my-bucket', 'path/to/log.json', multi_line=True)\n&gt;&gt;&gt; print(log_data)\n[{'event': 'start', 'timestamp': '2025-02-18T12:00:00Z'}, ...]\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.md5_sum","title":"<code>md5_sum(client: boto3.client, bucket_name: str, object_name: str) -&gt; str</code>","text":"<p>Get md5 hash of a specific object on s3.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to create md5 hash from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string value with the MD5 hash of the object data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; md5_sum(client, 'mybucket', 'folder/file.txt')\n\"d41d8cd98f00b204e9800998ecf8427e\"\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.move_file","title":"<code>move_file(client: boto3.client, source_bucket_name: str, source_object_name: str, destination_bucket_name: str, destination_object_name: str) -&gt; bool</code>","text":"<p>Move a file within or between AWS S3 buckets.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>source_bucket_name</code> <code>str</code> <p>The name of the source S3 bucket.</p> required <code>source_object_name</code> <code>str</code> <p>The S3 object name of the source file.</p> required <code>destination_bucket_name</code> <code>str</code> <p>The name of the destination S3 bucket.</p> required <code>destination_object_name</code> <code>str</code> <p>The S3 object name of the destination file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was moved successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; move_file(\n...     client,\n...     'sourcebucket',\n...     'source_folder/file.txt',\n...     'destbucket',\n...     'dest_folder/file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.read_header","title":"<code>read_header(client: boto3.client, bucket_name: str, object_name: str) -&gt; str</code>","text":"<p>Read the first line of a file on s3.</p> <p>Gets the entire file using boto3 get_objects, converts its body into an input stream, reads the first line and remove the carriage return character (backslash-n) from the end.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to read header from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Returns the first line of the file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; read_header(client, 'mybucket', 'folder/file.txt')\n\"First line\"\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.remove_leading_slash","title":"<code>remove_leading_slash(text: str) -&gt; str</code>","text":"<p>Remove the leading forward slash from a string if present.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which the leading slash will be removed.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text stripped of its leading slash.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; remove_leading_slash('/example/path')\n'example/path'\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.s3_walk","title":"<code>s3_walk(client: boto3.client, bucket_name: str, prefix: str) -&gt; Dict</code>","text":"<p>Traverse an S3 bucket and return its structure in a dictionary format.</p> <p>Mimics the functionality of os.walk in s3 bucket using long filenames with slashes. Recursively goes through the long filenames and splits it into subdirectories, and \"files\" - short file names.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix of the object to start the walk from.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary representing the bucket structure where: - Keys are directory paths ending with '/' - Values are tuples of (set(subdirectories), set(files)) where:   - subdirectories: a set of directory names ending with '/'   - files: a set of file paths</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; # For a bucket with files: file5.txt, folder1/file1.txt, folder1/file2.txt,\n&gt;&gt;&gt; # folder1/subfolder1/file3.txt, and folder2/file4.txt\n&gt;&gt;&gt; s3_walk(client, 'test-bucket', '')\n{\n    '': ({\"folder1/\", \"folder2/\"}, {\"file5.txt\"}),\n    'folder1/': (set(), {\"folder1/\"}),\n    'folder2/': (set(), {\"folder2/\"})\n}\n</code></pre> <pre><code>&gt;&gt;&gt; # When using a specific prefix\n&gt;&gt;&gt; s3_walk(client, 'test-bucket', 'folder1/')\n{\n    'folder1/': ({\"subfolder1/\"}, {\"folder1/file1.txt\", \"folder1/file2.txt\"}),\n    'folder1/subfolder1/': (set(), {\"folder1/subfolder1/\"})\n}\n</code></pre> <pre><code>&gt;&gt;&gt; # Empty bucket or nonexistent prefix\n&gt;&gt;&gt; s3_walk(client, 'test-bucket', 'nonexistent/')\n{}\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.upload_file","title":"<code>upload_file(client: boto3.client, bucket_name: str, local_path: str, object_name: Optional[str] = None, overwrite: bool = False) -&gt; bool</code>","text":"<p>Upload a file to an Amazon S3 bucket from local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target S3 bucket.</p> required <code>local_path</code> <code>str</code> <p>The file path on the local system to upload.</p> required <code>object_name</code> <code>Optional[str]</code> <p>The target S3 object name. If None, uses the base name of the local file path.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, the existing file on S3 will be overwritten.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was uploaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; upload_file(\n...     client,\n...     'mybucket',\n...     '/path/to/file.txt',\n...     'folder/s3_file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.upload_folder","title":"<code>upload_folder(client: boto3.client, bucket_name: str, local_path: str, prefix: str = '', overwrite: bool = False) -&gt; bool</code>","text":"<p>Upload an entire folder from the local file system to an AWS S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket to which the folder will be uploaded.</p> required <code>local_path</code> <code>str</code> <p>The path to the local folder to upload.</p> required <code>prefix</code> <code>str</code> <p>The prefix to prepend to each object name when uploading to S3.</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files in the bucket.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was uploaded successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; upload_folder(\n...     client,\n...     'mybucket',\n...     '/path/to/local/folder',\n...     'folder_prefix',\n...     True\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.validate_bucket_name","title":"<code>validate_bucket_name(bucket_name: str) -&gt; str</code>","text":"<p>Validate the format of an AWS S3 bucket name according to AWS rules.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated bucket name if valid.</p> <p>Raises:</p> Type Description <code>InvalidBucketNameError</code> <p>If the bucket name does not meet AWS specifications.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_bucket_name('valid-bucket-name')\n'valid-bucket-name'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_bucket_name('Invalid_Bucket_Name')\nInvalidBucketNameError: Bucket name must not contain underscores.\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.validate_s3_file_path","title":"<code>validate_s3_file_path(file_path: str, allow_s3_scheme: bool) -&gt; str</code>","text":"<p>Validate the file path based on the S3 URI scheme.</p> <p>If <code>allow_s3_scheme</code> is True, the file path must contain an S3 URI scheme (either 's3://' or 's3a://').</p> <p>If <code>allow_s3_scheme</code> is False, the file path should not contain an S3 URI scheme.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The file path to validate.</p> required <code>allow_s3_scheme</code> <code>bool</code> <p>Whether or not to allow an S3 URI scheme in the file path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated file path if valid.</p> <p>Raises:</p> Type Description <code>InvalidS3FilePathError</code> <p>If the validation fails based on the value of <code>allow_s3_scheme</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_s3_file_path('data_folder/data.csv', allow_s3_scheme=False)\n'data_folder/data.csv'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=True)\n's3a://bucket-name/data.csv'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_s3_file_path('s3a://bucket-name/data.csv', allow_s3_scheme=False)\nInvalidS3FilePathError: The file_path should not contain an S3 URI scheme\nlike 's3://' or 's3a://'.\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.write_csv","title":"<code>write_csv(client: boto3.client, bucket_name: str, data: pd.DataFrame, filepath: str, **kwargs) -&gt; bool</code>","text":"<p>Write a Pandas Dataframe to csv in an S3 bucket.</p> <p>Uses StringIO library as a RAM buffer, so at first Pandas writes data to the buffer, then the buffer returns to the beginning, and then it is sent to the S3 bucket using the boto3.put_object method.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>data</code> <code>DataFrame</code> <p>The dataframe to write to the specified path.</p> required <code>filepath</code> <code>str</code> <p>The filepath to save the dataframe to.</p> required <code>kwargs</code> <p>Optional dictionary of Pandas to_csv arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the dataframe is written successfully. False if it was not possible to serialise or write the file.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error writing the file to S3.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'column1': [1, 2, 3],\n&gt;&gt;&gt;     'column2': ['a', 'b', 'c']\n&gt;&gt;&gt; })\n&gt;&gt;&gt; write_csv(client, 'my_bucket', data, 'path/to/file.csv')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.write_excel","title":"<code>write_excel(client: boto3.client, bucket_name: str, data: pd.DataFrame, filepath: str, **kwargs) -&gt; bool</code>","text":"<p>Write a Pandas DataFrame to an Excel file in an S3 bucket.</p> <p>Uses BytesIO as a RAM buffer. Pandas writes data to the buffer, the buffer rewinds to the beginning, and then it is sent to S3 using the boto3.put_object method.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>data</code> <code>DataFrame</code> <p>The dataframe to write to the specified path.</p> required <code>filepath</code> <code>str</code> <p>The filepath to save the dataframe to in the S3 bucket.</p> required <code>kwargs</code> <code>dict</code> <p>Optional dictionary of Pandas <code>to_excel</code> arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the dataframe is written successfully, False otherwise.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error writing the file to S3.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; data = pd.DataFrame({\n&gt;&gt;&gt;     'column1': [1, 2, 3],\n&gt;&gt;&gt;     'column2': ['a', 'b', 'c']\n&gt;&gt;&gt; })\n&gt;&gt;&gt; write_excel(client, 'my_bucket', data, 'path/to/file.xlsx')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.write_string_to_file","title":"<code>write_string_to_file(client: boto3.client, bucket_name: str, object_name: str, object_content: bytes) -&gt; None</code>","text":"<p>Write a string into the specified object in the s3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The S3 object name to write into.</p> required <code>object_content</code> <code>bytes</code> <p>The content (str) to be written to \"object_name\".</p> required <p>Returns:</p> Type Description <code>None</code> <p>The outcome of this operation is the string written into the object in the s3 bucket. It will overwrite anything in the object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; write_string_to_file(client, 'mybucket', 'folder/file.txt', b'example content')\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.zip_local_directory_to_s3","title":"<code>zip_local_directory_to_s3(client: boto3.client, local_directory_path: Union[str, Path], bucket_name: str, object_name: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Zips a local directory and uploads it to AWS S3.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>local_directory_path</code> <code>Union[str, Path]</code> <p>Path to the local directory to be zipped.</p> required <code>bucket_name</code> <code>str</code> <p>Name of the S3 bucket.</p> required <code>object_name</code> <code>str</code> <p>S3 key (path) where the zip file will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If False, will not upload if the file already exists in S3. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if upload was successful, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import boto3\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; client = boto3.client('s3')\n&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; zip_local_directory_to_s3(\n...     client,\n...     '/path/to/local/dir',\n...     'my-bucket',\n...     'backups/mydir.zip'\n... )\nTrue\n&gt;&gt;&gt; # With overwrite parameter\n&gt;&gt;&gt; zip_local_directory_to_s3(\n...     client,\n...     Path('/path/to/local/dir'),\n...     'my-bucket',\n...     'backups/mydir.zip',\n...     overwrite=True\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.s3_utils.zip_s3_directory_to_s3","title":"<code>zip_s3_directory_to_s3(client: boto3.client, source_bucket_name: str, source_prefix: str, destination_bucket_name: str, destination_object_name: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Zips a directory that exists in S3 and saves it to another location in S3.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>client</code> <p>Initialised boto3 S3 client.</p> required <code>source_bucket_name</code> <code>str</code> <p>Name of the source S3 bucket.</p> required <code>source_prefix</code> <code>str</code> <p>Prefix (directory path) in the source bucket to zip.</p> required <code>destination_bucket_name</code> <code>str</code> <p>Name of the destination S3 bucket.</p> required <code>destination_object_name</code> <code>str</code> <p>S3 key (path) where the zip file will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If False, will not upload if the file already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if operation was successful, False otherwise</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import boto3\n&gt;&gt;&gt; s3 = boto3.client('s3')\n&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; zip_s3_directory_to_s3(\n...     s3,\n...     'source-bucket',\n...     'data/logs/',\n...     'dest-bucket',\n...     'archives/logs.zip'\n... )\nTrue\n&gt;&gt;&gt; # With overwrite parameter\n&gt;&gt;&gt; zip_s3_directory_to_s3(\n...     s3,\n...     'source-bucket',\n...     'data/logs/',\n...     'dest-bucket',\n...     'archives/logs.zip',\n...     overwrite=True\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils","title":"<code>rdsa_utils.cdp.helpers.hdfs_utils</code>","text":"<p>Utility functions for interacting with HDFS.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.change_permissions","title":"<code>change_permissions(path: str, permission: str, recursive: bool = False) -&gt; bool</code>","text":"<p>Change directory and file permissions in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file or directory in HDFS.</p> required <code>permission</code> <code>str</code> <p>The permission to be set, e.g., 'go+rwx' or '777'.</p> required <code>recursive</code> <code>bool</code> <p>If True, changes permissions for all subdirectories and files within a directory.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.copy","title":"<code>copy(from_path: str, to_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Copy a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The source path of the file in HDFS.</p> required <code>to_path</code> <code>str</code> <p>The target path of the file in HDFS.</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing file at the target path will be overwritten, default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.copy_local_to_hdfs","title":"<code>copy_local_to_hdfs(from_path: str, to_path: str) -&gt; bool</code>","text":"<p>Copy a local file to HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the local file.</p> required <code>to_path</code> <code>str</code> <p>The path to the HDFS directory where the file will be copied.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.create_dir","title":"<code>create_dir(path: str) -&gt; bool</code>","text":"<p>Create a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path where the directory should be created.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory created), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.create_txt_from_string","title":"<code>create_txt_from_string(path: str, string_to_write: str, replace: Optional[bool] = False) -&gt; None</code>","text":"<p>Create and populate a text file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the new file to be created, for example, '/some/directory/newfile.txt'.</p> required <code>string_to_write</code> <code>str</code> <p>The string that will populate the new text file.</p> required <code>replace</code> <code>Optional[bool]</code> <p>Flag determining whether an existing file should be replaced. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>This function doesn't return anything; it's used for its side effect of creating a text file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>replace</code> is False and the file already exists.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.delete_dir","title":"<code>delete_dir(path: str) -&gt; bool</code>","text":"<p>Delete an empty directory from HDFS.</p> <p>This function attempts to delete an empty directory in HDFS. If the directory is not empty, the deletion will fail.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path to the directory to be deleted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory deleted), otherwise False.</p> Note <p>This function will only succeed if the directory is empty. To delete directories containing files or other directories, consider using <code>delete_path</code> instead.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.delete_file","title":"<code>delete_file(path: str) -&gt; bool</code>","text":"<p>Delete a specific file in HDFS.</p> <p>This function is used to delete a single file located at the specified HDFS path. If the path points to a directory, the command will fail.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file in HDFS to be deleted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was successfully deleted (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p> Note <p>This function is intended for files only. For directory deletions, use <code>delete_dir</code> or <code>delete_path</code>.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.delete_path","title":"<code>delete_path(path: str) -&gt; bool</code>","text":"<p>Delete a file or directory in HDFS, including non-empty directories.</p> <p>This function is capable of deleting both files and directories. When applied to directories, it will recursively delete all contents within the directory, making it suitable for removing directories regardless of whether they are empty or contain files or other directories.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file or directory in HDFS to be deleted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was successfully deleted (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p> Warning <p>Use with caution: applying this function to a directory will remove all contained files and subdirectories without confirmation.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.file_exists","title":"<code>file_exists(path: str) -&gt; bool</code>","text":"<p>Check whether a file exists in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file in HDFS to be checked for existence.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.get_date_modified","title":"<code>get_date_modified(filepath: str) -&gt; str</code>","text":"<p>Return the last modified date of a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file in HDFS.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The date the file was last modified.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.is_dir","title":"<code>is_dir(path: str) -&gt; bool</code>","text":"<p>Test if a directory exists in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path to the directory to be tested.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory exists), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.move_local_to_hdfs","title":"<code>move_local_to_hdfs(from_path: str, to_path: str) -&gt; bool</code>","text":"<p>Move a local file to HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the local file.</p> required <code>to_path</code> <code>str</code> <p>The path to the HDFS directory where the file will be moved.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.read_dir","title":"<code>read_dir(path: str) -&gt; List[str]</code>","text":"<p>Read the contents of a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of full paths of the items found in the directory.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.read_dir_files","title":"<code>read_dir_files(path: str) -&gt; List[str]</code>","text":"<p>Read the filenames in a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of filenames in the directory.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.read_dir_files_recursive","title":"<code>read_dir_files_recursive(path: str, return_path: bool = True) -&gt; List[str]</code>","text":"<p>Recursively reads the contents of a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <code>return_path</code> <code>bool</code> <p>If True, returns the full path of the files, otherwise just the filename.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of files in the directory.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.hdfs_utils.rename","title":"<code>rename(from_path: str, to_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Rename (i.e., move using full path) a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The source path of the file in HDFS.</p> required <code>to_path</code> <code>str</code> <p>The target path of the file in HDFS.</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing file at the target path will be overwritten, default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.impala","title":"<code>rdsa_utils.cdp.helpers.impala</code>","text":"<p>Utilities for working with Impala.</p>"},{"location":"reference/#rdsa_utils.cdp.helpers.impala.invalidate_impala_metadata","title":"<code>invalidate_impala_metadata(table: str, impalad_address_port: str, impalad_ca_cert: str, keep_stderr: Optional[bool] = False)</code>","text":"<p>Automate the invalidation of a table's metadata using impala-shell.</p> <p>This function uses the impala-shell command with the given impalad_address_port and impalad_ca_cert, to invalidate a specified table's metadata.</p> <p>It proves useful during a data pipeline's execution after writing to an intermediate Hive table. Using Impala Query Editor in Hue, end-users often need to run \"INVALIDATE METADATA\" command to refresh a table's metadata. However, this manual step can be missed, leading to potential use of outdated metadata.</p> <p>The function automates the \"INVALIDATE METADATA\" command for a given table, ensuring up-to-date metadata for future queries. This reduces manual intervention, making outdated metadata issues less likely to occur.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Name of the table for metadata invalidation.</p> required <code>impalad_address_port</code> <code>str</code> <p>'address:port' of the impalad instance.</p> required <code>impalad_ca_cert</code> <code>str</code> <p>Path to impalad's CA certificate file.</p> required <code>keep_stderr</code> <code>Optional[bool]</code> <p>If True, will print impala-shell command's stderr output.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem'\n... )\n&gt;&gt;&gt; invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem',\n...     keep_stderr=True\n... )\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog","title":"<code>rdsa_utils.cdp.io.pipeline_runlog</code>","text":"<p>Utilities for managing a Pipeline Runlog using Hive Tables.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.add_runlog_entry","title":"<code>add_runlog_entry(spark: SparkSession, desc: str, version: str, config: Union[ConfigParser, Dict[str, str]], pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog', run_id: Optional[int] = None) -&gt; DataFrame</code>","text":"<p>Add an entry to a target runlog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session.</p> required <code>desc</code> <code>str</code> <p>Description to attach to the log entry.</p> required <code>version</code> <code>str</code> <p>Version of the pipeline.</p> required <code>config</code> <code>Union[ConfigParser, Dict[str, str]]</code> <p>Configuration object for the run.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>Pipeline name. If None, uses the spark application name.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>Target runlog table. If database not set, this should include the database.</p> <code>'pipeline_runlog'</code> <code>run_id</code> <code>Optional[int]</code> <p>Run id to use if already reserved. If not specified, a new one is generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The log entry returned as a spark dataframe.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.create_runlog_entry","title":"<code>create_runlog_entry(spark: SparkSession, run_id: int, desc: str, version: str, config: Union[ConfigParser, Dict[str, str]], pipeline: Optional[str] = None) -&gt; DataFrame</code>","text":"<p>Create an entry for the runlog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session.</p> required <code>run_id</code> <code>int</code> <p>Entry run id.</p> required <code>desc</code> <code>str</code> <p>Description to attach to the log entry.</p> required <code>version</code> <code>str</code> <p>Version of the pipeline.</p> required <code>config</code> <code>Union[ConfigParser, Dict[str, str]]</code> <p>Configuration object for the run.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>Pipeline name. If None, derives from spark app name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The log entry returned as a spark dataframe.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.create_runlog_table","title":"<code>create_runlog_table(spark: SparkSession, database: str, tablename: Optional[str] = 'pipeline_runlog') -&gt; None</code>","text":"<p>Create runlog and _reserved_ids tables in the target database if needed.</p> <p>This function executes two SQL queries to create two tables, if they do not already exist in the target database. The first table's structure includes columns for run_id, desc, user, datetime, pipeline_name, pipeline_version, and config, while the second table includes run_id and reserved_date.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session which will be used to execute SQL queries.</p> required <code>database</code> <code>str</code> <p>The name of the target database where tables will be created.</p> required <code>tablename</code> <code>Optional[str]</code> <p>The name of the main table to be created (default is \"pipeline_runlog\"). The associated _reserved_ids table will be suffixed with this name.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.appName(\"test_session\").getOrCreate()\n&gt;&gt;&gt; create_runlog_table(spark, \"test_db\", \"test_table\")\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.get_last_run_id","title":"<code>get_last_run_id(spark: SparkSession, pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog') -&gt; Optional[int]</code>","text":"<p>Retrieve the last run_id, either in general or for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running Spark session.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>If specified, the result will be for the listed pipeline only.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>The target runlog table. If the database is not set, this should include the database.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The id of the last run. Returns None if the log table is empty.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.get_penultimate_run_id","title":"<code>get_penultimate_run_id(spark: SparkSession, pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog') -&gt; Optional[int]</code>","text":"<p>Retrieve penultimate run_id in general or a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running Spark session.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>If specified, the result will be for the listed pipeline only.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>The target runlog table. If the database is not set, this should include the database.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The id of the penultimate run. Returns None if the log table is empty or has less than two entries.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.reserve_id","title":"<code>reserve_id(spark: SparkSession, log_table: Optional[str] = 'pipeline_runlog') -&gt; int</code>","text":"<p>Reserve a run id in the reserved ids table linked to the runlog table.</p> <p>The function reads the last run id from the reserved ids table, increments it to create a new id,and writes the new id with the current timestamp to the reserved ids table.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running SparkSession instance.</p> required <code>log_table</code> <code>Optional[str]</code> <p>The name of the main pipeline runlog table associated with this reserved id table, by default \"pipeline_runlog\".</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int</code> <p>The new run id.</p>"},{"location":"reference/#rdsa_utils.cdp.io.pipeline_runlog.write_runlog_file","title":"<code>write_runlog_file(spark: SparkSession, runlog_table: str, runlog_id: int, path: str) -&gt; None</code>","text":"<p>Write metadata from runlog entry to a text file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running SparkSession instance.</p> required <code>runlog_table</code> <code>str</code> <p>The name of the table containing the runlog entries.</p> required <code>runlog_id</code> <code>int</code> <p>The id of the desired entry.</p> required <code>path</code> <code>str</code> <p>The HDFS path where the file will be written.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function doesn't return anything; it's used for its side effect of creating a text file.</p>"},{"location":"reference/#rdsa_utils.cdp.io.input","title":"<code>rdsa_utils.cdp.io.input</code>","text":"<p>Read inputs on CDP.</p>"},{"location":"reference/#rdsa_utils.cdp.io.input.extract_database_name","title":"<code>extract_database_name(spark: SparkSession, long_table_name: str) -&gt; Tuple[str, str]</code>","text":"<p>Extract the database component and table name from a compound table name.</p> <p>This function can handle multiple scenarios:</p> <ol> <li> <p>For GCP's naming format '..',    the function will return the database and table name. <li> <p>If the name is formatted as 'db_name.table_name', the function will    extract and return the database and table names.</p> </li> <li> <p>If the long_table_name contains only the table name (e.g., 'table_name'),    the function will use the current database of the SparkSession.</p> </li> <li> <p>For any other incorrectly formatted names, the function will raise    a ValueError.</p> </li> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>long_table_name</code> <code>str</code> <p>Full name of the table, which can include the GCP project and/or database name.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>A tuple containing the name of the database and the table name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table name doesn't match any of the expected formats.</p>"},{"location":"reference/#rdsa_utils.cdp.io.input.get_current_database","title":"<code>get_current_database(spark: SparkSession) -&gt; str</code>","text":"<p>Retrieve the current database from the active SparkSession.</p>"},{"location":"reference/#rdsa_utils.cdp.io.input.get_tables_in_database","title":"<code>get_tables_in_database(spark: SparkSession, database_name: str) -&gt; List[str]</code>","text":"<p>Get a list of tables in a given database.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>database_name</code> <code>str</code> <p>The name of the database from which to list tables.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of table names in the specified database.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is an error fetching tables from the specified database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tables = get_tables_in_database(spark, \"default\")\n&gt;&gt;&gt; print(tables)\n['table1', 'table2', 'table3']\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.input.load_and_validate_table","title":"<code>load_and_validate_table(spark: SparkSession, table_name: str, skip_validation: bool = False, err_msg: str = None, filter_cond: str = None, keep_columns: Optional[List[str]] = None, rename_columns: Optional[Dict[str, str]] = None, drop_columns: Optional[List[str]] = None) -&gt; SparkDF</code>","text":"<p>Load a table, apply transformations, and validate if it is not empty.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>table_name</code> <code>str</code> <p>Name of the table to load.</p> required <code>skip_validation</code> <code>bool</code> <p>If True, skips validation step, by default False.</p> <code>False</code> <code>err_msg</code> <code>str</code> <p>Error message to return if table is empty, by default None.</p> <code>None</code> <code>filter_cond</code> <code>str</code> <p>Condition to apply to SparkDF once read, by default None.</p> <code>None</code> <code>keep_columns</code> <code>Optional[List[str]]</code> <p>A list of column names to keep in the DataFrame, dropping all others. Default value is None.</p> <code>None</code> <code>rename_columns</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary to rename columns where keys are existing column names and values are new column names. Default value is None.</p> <code>None</code> <code>drop_columns</code> <code>Optional[List[str]]</code> <p>A list of column names to drop from the DataFrame. Default value is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded SparkDF if validated, subject to options above.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If there's an issue accessing the table or if the table does not exist in the specified database.</p> <code>ValueError</code> <p>If the table is empty after loading, becomes empty after applying a filter condition, or if columns specified in keep_columns, drop_columns, or rename_columns do not exist in the DataFrame.</p> Notes <p>Transformation order: 1. Columns are kept according to <code>keep_columns</code>. 2. Columns are dropped according to <code>drop_columns</code>. 3. Columns are renamed according to <code>rename_columns</code>.</p> <p>Examples:</p> <p>Load a table, apply a filter, and validate it:</p> <pre><code>&gt;&gt;&gt; df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        filter_cond=\"age &gt; 21\"\n    )\n</code></pre> <p>Load a table and keep only specific columns:</p> <pre><code>&gt;&gt;&gt; df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        keep_columns=[\"name\", \"age\", \"city\"]\n    )\n</code></pre> <p>Load a table, drop specific columns, and rename a column:</p> <pre><code>&gt;&gt;&gt; df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        drop_columns=[\"extra_column\"],\n        rename_columns={\"name\": \"full_name\"}\n    )\n</code></pre> <p>Load a table, skip validation, and apply all transformations:</p> <pre><code>&gt;&gt;&gt; df = load_and_validate_table(\n        spark=spark,\n        table_name=\"my_table\",\n        skip_validation=True,\n        keep_columns=[\"name\", \"age\", \"city\"],\n        drop_columns=[\"extra_column\"],\n        rename_columns={\"name\": \"full_name\"},\n        filter_cond=\"age &gt; 21\"\n    )\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.output","title":"<code>rdsa_utils.cdp.io.output</code>","text":"<p>Write outputs on CDP.</p>"},{"location":"reference/#rdsa_utils.cdp.io.output.insert_df_to_hive_table","title":"<code>insert_df_to_hive_table(spark: SparkSession, df: SparkDF, table_name: str, overwrite: bool = False, fill_missing_cols: bool = False, repartition_data_by: Union[int, str, None] = None) -&gt; None</code>","text":"<p>Write SparkDF to Hive table with optional configuration.</p> <p>This function writes data from a SparkDF into a Hive table, handling missing columns and optional repartitioning. It ensures the table's column order matches the DataFrame and manages different overwrite behaviors for partitioned and non-partitioned data.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>df</code> <code>DataFrame</code> <p>SparkDF containing data to be written.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive table to write data into.</p> required <code>overwrite</code> <code>bool</code> <p>Controls how existing data is handled, default is False:</p> <p>For non-partitioned data: - True: Replaces entire table with DataFrame data. - False: Appends DataFrame data to existing table.</p> <p>For partitioned data: - True: Replaces data only in partitions present in DataFrame. - False: Appends data to existing partitions or creates new ones.</p> <code>False</code> <code>fill_missing_cols</code> <code>bool</code> <p>If True, adds missing columns as NULL values. If False, raises an error on schema mismatch, default is False.</p> <ul> <li>Explicitly casts DataFrame columns to match the Hive table schema to   avoid type mismatch errors.</li> <li>Adds missing columns as NULL values when <code>fill_missing_cols</code> is True,   regardless of their data type (e.g., String, Integer, Double, Boolean, etc.).</li> </ul> <code>False</code> <code>repartition_data_by</code> <code>Union[int, str, None]</code> <p>Controls data repartitioning, default is None: - int: Sets target number of partitions. - str: Specifies column to repartition by. - None: No repartitioning performed.</p> <code>None</code> Notes <p>When using repartition with a number: - Affects physical file structure but preserves Hive partitioning scheme. - Controls number of output files per write operation per Hive partition. - Maintains partition-based query optimisation.</p> <p>When repartitioning by column: - Helps balance file sizes across Hive partitions. - Reduces creation of small files.</p> <p>Raises:</p> Type Description <code>AnalysisException</code> <p>If there's an error reading the table. This can occur if the table doesn't exist or if there's no access to it.</p> <code>ValueError</code> <p>If the SparkDF schema does not match the Hive table schema and 'fill_missing_cols' is set to False.</p> <code>DataframeEmptyError</code> <p>If input DataFrame is empty.</p> <code>Exception</code> <p>For other general exceptions when writing data to the table.</p> <p>Examples:</p> <p>Write a DataFrame to a Hive table without overwriting:</p> <pre><code>&gt;&gt;&gt; insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\"\n... )\n</code></pre> <p>Overwrite an existing table with a DataFrame:</p> <pre><code>&gt;&gt;&gt; insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     overwrite=True\n... )\n</code></pre> <p>Write a DataFrame to a Hive table with missing columns filled:</p> <pre><code>&gt;&gt;&gt; insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     fill_missing_cols=True\n... )\n</code></pre> <p>Repartition by column before writing to Hive:</p> <pre><code>&gt;&gt;&gt; insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     repartition_data_by=\"partition_column\"\n... )\n</code></pre> <p>Repartition into a fixed number of partitions before writing:</p> <pre><code>&gt;&gt;&gt; insert_df_to_hive_table(\n...     spark=spark,\n...     df=df,\n...     table_name=\"my_database.my_table\",\n...     repartition_data_by=10\n... )\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.output.save_csv_to_hdfs","title":"<code>save_csv_to_hdfs(df: SparkDF, file_name: str, file_path: str, overwrite: bool = True) -&gt; None</code>","text":"<p>Save DataFrame as CSV on HDFS, coalescing to a single partition.</p> <p>This function saves a PySpark DataFrame to HDFS in CSV format. By coalescing the DataFrame into a single partition before saving, it accomplishes two main objectives:</p> <ol> <li> <p>Single Part File: The output is a single CSV file rather than    multiple part files. This method reduces complexity and    cuts through the clutter of multi-part files, offering users    and systems a more cohesive and hassle-free experience.</p> </li> <li> <p>Preserving Row Order: Coalescing into a single partition maintains    the order of rows as they appear in the DataFrame. This is essential    when the row order matters for subsequent processing or analysis.    It's important to note, however, that coalescing can have performance    implications for very large DataFrames by concentrating    all data processing on a single node.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be saved.</p> required <code>file_name</code> <code>str</code> <p>Name of the CSV file. Must include the \".csv\" extension.</p> required <code>file_path</code> <code>str</code> <p>HDFS path where the CSV file should be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite any existing file with the same name. If False and the file exists, the function will raise an error.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file_name does not end with \".csv\".</p> <code>IOError</code> <p>If overwrite is False and the target file already exists.</p> <p>Examples:</p> <p>Saving to an S3 bucket using the <code>s3a://</code> scheme:</p> <pre><code># Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"s3a://my-bucket/data_folder/\"\nsave_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n</code></pre> <p>Saving to a normal HDFS path:</p> <pre><code># Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"/user/hdfs/data_folder/\"\nsave_csv_to_hdfs(df, file_name, file_path, overwrite=True)\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.output.save_csv_to_s3","title":"<code>save_csv_to_s3(df: SparkDF, bucket_name: str, file_name: str, file_path: str, s3_client: boto3.client, overwrite: bool = True) -&gt; None</code>","text":"<p>Save DataFrame as CSV on S3, coalescing to a single partition.</p> <p>This function saves a PySpark DataFrame to S3 in CSV format. By coalescing the DataFrame into a single partition before saving, it accomplishes two main objectives:</p> <ol> <li> <p>Single Part File: The output is a single CSV file rather than    multiple part files. This method reduces complexity and    cuts through the clutter of multi-part files, offering users    and systems a more cohesive and hassle-free experience.</p> </li> <li> <p>Preserving Row Order: Coalescing into a single partition maintains    the order of rows as they appear in the DataFrame. This is essential    when the row order matters for subsequent processing or analysis.    It's important to note, however, that coalescing can have performance    implications for very large DataFrames by concentrating    all data processing on a single node.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to be saved.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket where the CSV file should be saved.</p> required <code>file_name</code> <code>str</code> <p>Name of the CSV file. Must include the \".csv\" extension.</p> required <code>file_path</code> <code>str</code> <p>S3 path where the CSV file should be saved.</p> required <code>s3_client</code> <code>client</code> <p>The boto3 S3 client instance.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite any existing file with the same name. If False and the file exists, the function will raise an error.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file_name does not end with \".csv\".</p> <code>InvalidBucketNameError</code> <p>If the bucket name does not meet AWS specifications.</p> <code>InvalidS3FilePathError</code> <p>If the file_path contains an S3 URI scheme like 's3://' or 's3a://'.</p> <code>IOError</code> <p>If overwrite is False and the target file already exists.</p> <p>Examples:</p> <p>Saving to an S3 bucket:</p> <pre><code># Assume `df` is a pre-defined PySpark DataFrame\nfile_name = \"data_output.csv\"\nfile_path = \"data_folder/\"\ns3_client = boto3.client('s3')\nsave_csv_to_s3(\n    df,\n    'my-bucket',\n    file_name,\n    file_path,\n    s3_client,\n    overwrite=True\n)\n</code></pre>"},{"location":"reference/#rdsa_utils.cdp.io.output.write_and_read_hive_table","title":"<code>write_and_read_hive_table(spark: SparkSession, df: SparkDF, table_name: str, database: str, filter_id: Union[int, str], filter_col: str = 'run_id', fill_missing_cols: bool = False) -&gt; SparkDF</code>","text":"<p>Write a SparkDF to an existing Hive table and then read it back.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>df</code> <code>DataFrame</code> <p>The SparkDF to be written to the Hive table.</p> required <code>table_name</code> <code>str</code> <p>The name of the Hive table to write to and read from.</p> required <code>database</code> <code>str</code> <p>The Hive database name.</p> required <code>filter_id</code> <code>Union[int, str]</code> <p>The identifier to filter on when reading data back from the Hive table.</p> required <code>filter_col</code> <code>str</code> <p>The column name to use for filtering data when reading back from the Hive table, by default 'run_id'.</p> <code>'run_id'</code> <code>fill_missing_cols</code> <code>bool</code> <p>If True, missing columns in the DataFrame will be filled with nulls when writing to the Hive table, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame read from the Hive table.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified Hive table does not exist in the given database or if the provided DataFrame doesn't contain the specified filter column.</p> <code>Exception</code> <p>For general exceptions encountered during execution.</p> Notes <p>This function assumes the Hive table already exists. The DataFrame <code>df</code> should have the same schema as the Hive table for the write to succeed.</p> <p>The function allows for more effective memory management when dealing with large PySpark DataFrames by leveraging Hive's on-disk storage.</p> <p>Predicate pushdown is used when reading the data back into a PySpark DataFrame, minimizing the memory usage and optimising the read operation.</p> <p>As part of the design, there is always a column called filter_col in the DataFrame and Hive table to track pipeline runs.</p> <p>The Hive table contains all the runs, and we only read back the run that we just wrote to the Hive Table using the <code>filter_id</code> parameter. If no <code>filter_col</code> is specified, 'run_id' is used as default.</p>"},{"location":"reference/#gcp","title":"GCP","text":""},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils","title":"<code>rdsa_utils.gcp.helpers.gcp_utils</code>","text":"<p>Utility functions for interacting with Google Cloud Storage.</p> <p>To initialise a client for GCS and configure it with a service account JSON key file, you can use the following code snippet:</p> <pre><code>from google.cloud import storage\n\n# Create a GCS client\nclient = storage.Client.from_service_account_json('path/to/keyfile.json')\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.copy_file","title":"<code>copy_file(client: storage.Client, source_bucket_name: str, source_object_name: str, destination_bucket_name: str, destination_object_name: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Copy a file from one GCS bucket to another.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>source_bucket_name</code> <code>str</code> <p>The name of the source bucket.</p> required <code>source_object_name</code> <code>str</code> <p>The GCS object name of the source file.</p> required <code>destination_bucket_name</code> <code>str</code> <p>The name of the destination bucket.</p> required <code>destination_object_name</code> <code>str</code> <p>The GCS object name of the destination file.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the destination file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was copied successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; copy_file(\n...     client,\n...     'source-bucket',\n...     'source_file.txt',\n...     'destination-bucket',\n...     'destination_file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.create_folder_on_gcs","title":"<code>create_folder_on_gcs(client: storage.Client, bucket_name: str, folder_path: str) -&gt; bool</code>","text":"<p>Create a folder in a GCS bucket if it doesn't already exist.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket where the folder will be created.</p> required <code>folder_path</code> <code>str</code> <p>The name of the folder to create.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was created successfully or already exists, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; create_folder_on_gcs(client, 'mybucket', 'new_folder/')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.delete_file","title":"<code>delete_file(client: storage.Client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Delete a file from a GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket from which the file will be deleted.</p> required <code>object_name</code> <code>str</code> <p>The GCS object name of the file to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was deleted successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; delete_file(client, 'mybucket', 'folder/gcs_file.txt')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.delete_folder","title":"<code>delete_folder(client: storage.Client, bucket_name: str, folder_path: str) -&gt; bool</code>","text":"<p>Delete a folder in a GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the GCS bucket.</p> required <code>folder_path</code> <code>str</code> <p>The path of the folder to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was deleted successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; delete_folder(client, 'mybucket', 'path/to/folder/')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.download_file","title":"<code>download_file(client: storage.Client, bucket_name: str, object_name: str, local_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Download a file from a GCS bucket to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the GCS bucket from which to download the file.</p> required <code>object_name</code> <code>str</code> <p>The GCS object name of the file to download.</p> required <code>local_path</code> <code>str</code> <p>The local file path where the downloaded file will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the local file if it exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was downloaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; download_file(\n...     client,\n...     'mybucket',\n...     'folder/gcs_file.txt',\n...     '/path/to/download.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.download_folder","title":"<code>download_folder(client: storage.Client, bucket_name: str, prefix: str, local_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Download a folder from a GCS bucket to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the GCS bucket from which to download the folder.</p> required <code>prefix</code> <code>str</code> <p>The GCS prefix of the folder to download.</p> required <code>local_path</code> <code>str</code> <p>The local directory path where the downloaded folder will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing local files if they exist.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was downloaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; download_folder(\n...     client,\n...     'mybucket',\n...     'folder/subfolder/',\n...     '/path/to/local_folder',\n...     overwrite=False\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.file_exists","title":"<code>file_exists(client: storage.Client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Check if a specific file exists in a GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>object_name</code> <code>str</code> <p>The GCS object name to check for existence.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; file_exists(client, 'mybucket', 'folder/file.txt')\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.get_table_columns","title":"<code>get_table_columns(table_path) -&gt; List[str]</code>","text":"<p>Return the column names for given bigquery table.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.is_gcs_directory","title":"<code>is_gcs_directory(client: storage.Client, bucket_name: str, object_name: str) -&gt; bool</code>","text":"<p>Check if a GCS key is a directory by listing its contents.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the GCS bucket.</p> required <code>object_name</code> <code>str</code> <p>The GCS object name to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the key represents a directory, False otherwise.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.list_files","title":"<code>list_files(client: storage.Client, bucket_name: str, prefix: str = '') -&gt; List[str]</code>","text":"<p>List files in a GCS bucket that match a specific prefix.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>prefix</code> <code>str</code> <p>The prefix to filter files, by default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of GCS object keys matching the prefix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; list_files(client, 'mybucket', 'folder_prefix/')\n['folder_prefix/file1.txt', 'folder_prefix/file2.txt']\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.load_config_gcp","title":"<code>load_config_gcp(config_path: str) -&gt; Tuple[Dict, Dict]</code>","text":"<p>Load the config and dev_config files to dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path of the config file in a yaml format.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>The contents of the config files.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.move_file","title":"<code>move_file(client: storage.Client, source_bucket_name: str, source_object_name: str, destination_bucket_name: str, destination_object_name: str) -&gt; bool</code>","text":"<p>Move a file within or between GCS buckets.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>source_bucket_name</code> <code>str</code> <p>The name of the source GCS bucket.</p> required <code>source_object_name</code> <code>str</code> <p>The GCS object name of the source file.</p> required <code>destination_bucket_name</code> <code>str</code> <p>The name of the destination GCS bucket.</p> required <code>destination_object_name</code> <code>str</code> <p>The GCS object name of the destination file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was moved successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; move_file(\n...     client,\n...     'sourcebucket',\n...     'source_folder/file.txt',\n...     'destbucket',\n...     'dest_folder/file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.remove_leading_slash","title":"<code>remove_leading_slash(text: str) -&gt; str</code>","text":"<p>Remove the leading forward slash from a string if present.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text from which the leading slash will be removed.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text stripped of its leading slash.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; remove_leading_slash('/example/path')\n'example/path'\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.run_bq_query","title":"<code>run_bq_query(query: str) -&gt; bigquery.QueryJob</code>","text":"<p>Run an SQL query in BigQuery.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.table_exists","title":"<code>table_exists(table_path: TablePath) -&gt; bool</code>","text":"<p>Check the big query catalogue to see if a table exists.</p> <p>Returns True if a table exists. See code sample explanation here: https://cloud.google.com/bigquery/docs/samples/bigquery-table-exists#bigquery_table_exists-python</p> <p>Parameters:</p> Name Type Description Default <code>table_path</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <p>Returns:</p> Type Description <code>bool</code> <p>Returns True if table exists and False if table does not exist.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.upload_file","title":"<code>upload_file(client: storage.Client, bucket_name: str, local_path: str, object_name: Optional[str] = None, overwrite: bool = False) -&gt; bool</code>","text":"<p>Upload a file to a GCS bucket from local directory.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the target GCS bucket.</p> required <code>local_path</code> <code>str</code> <p>The file path on the local system to upload.</p> required <code>object_name</code> <code>Optional[str]</code> <p>The target GCS object name. If None, uses the base name of the local file path.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>If True, the existing file on GCS will be overwritten.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was uploaded successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; upload_file(\n...     client,\n...     'mybucket',\n...     '/path/to/file.txt',\n...     'folder/gcs_file.txt'\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.upload_folder","title":"<code>upload_folder(client: storage.Client, bucket_name: str, local_path: str, prefix: str = '', overwrite: bool = False) -&gt; bool</code>","text":"<p>Upload an entire folder from the local file system to a GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>The GCS client instance.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket to which the folder will be uploaded.</p> required <code>local_path</code> <code>str</code> <p>The path to the local folder to upload.</p> required <code>prefix</code> <code>str</code> <p>The prefix to prepend to each object name when uploading to GCS.</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files in the bucket.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the folder was uploaded successfully, otherwise False.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; client = storage.Client()\n&gt;&gt;&gt; upload_folder(\n...     client,\n...     'mybucket',\n...     '/path/to/local/folder',\n...     'folder_prefix',\n...     True\n... )\nTrue\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.validate_bucket_name","title":"<code>validate_bucket_name(bucket_name: str) -&gt; str</code>","text":"<p>Validate the format of a GCS bucket name according to GCS rules.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated bucket name if valid.</p> <p>Raises:</p> Type Description <code>InvalidBucketNameError</code> <p>If the bucket name does not meet GCS specifications.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; validate_bucket_name('valid-bucket-name')\n'valid-bucket-name'\n</code></pre> <pre><code>&gt;&gt;&gt; validate_bucket_name('Invalid_Bucket_Name')\nInvalidBucketNameError: Bucket name must not contain underscores.\n</code></pre>"},{"location":"reference/#rdsa_utils.gcp.io.inputs","title":"<code>rdsa_utils.gcp.io.inputs</code>","text":"<p>Read from BigQuery.</p>"},{"location":"reference/#rdsa_utils.gcp.io.inputs.build_sql_query","title":"<code>build_sql_query(table_path: TablePath, columns: Optional[Sequence[str]] = None, date_column: Optional[str] = None, date_range: Optional[Sequence[str]] = None, column_filter_dict: Optional[Dict[str, Sequence[str]]] = None, partition_column: Optional[str] = None, partition_type: Optional[str] = None, partition_value: Optional[Union[Tuple[str, str], str]] = None) -&gt; str</code>","text":"<p>Create SQL query to load data with the specified filter conditions.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>Spark session.</p> required <code>table_path</code> <code>TablePath</code> <p>BigQuery table path in format \"database_name.table_name\".</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>The column selection. Selects all columns if None passed.</p> <code>None</code> <code>date_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter the date range on.</p> <code>None</code> <code>date_range</code> <code>Optional[Sequence[str]]</code> <p>Sequence with two values, a lower and upper value for dates to load in.</p> <code>None</code> <code>column_filter_dict</code> <code>Optional[Dict[str, Sequence[str]]]</code> <p>A dictionary containing column: [values] where the values correspond to terms in the column that are to be filtered by.</p> <code>None</code> <code>partition_column</code> <code>Optional[str]</code> <p>The name of the column that the table is partitioned by.</p> <code>None</code> <code>partition_type</code> <code>Optional[str]</code> <p>The unit of time the table is partitioned by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <code>None</code> <code>partition_value</code> <code>Optional[Union[Tuple[str, str], str]]</code> <p>The value or pair of values for filtering the partition column to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The string containing the SQL query.</p>"},{"location":"reference/#rdsa_utils.gcp.io.inputs.read_table","title":"<code>read_table(spark: SparkSession, table_path: TablePath, columns: Optional[Sequence[str]] = None, date_column: Optional[str] = None, date_range: Optional[Sequence[str]] = None, column_filter_dict: Optional[Dict[str, Sequence[str]]] = None, run_id_column: Optional[str] = 'run_id', run_id: Optional[str] = None, flatten_struct_cols: bool = False, partition_column: Optional[str] = None, partition_type: Optional[BigQueryTimePartitions] = None, partition_value: Optional[Union[Tuple[str, str], str]] = None) -&gt; SparkDF</code>","text":"<p>Read BigQuery table given table path and column selection.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>table_path</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>The column selection. Selects all columns if None passed.</p> <code>None</code> <code>date_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter the date range on.</p> <code>None</code> <code>date_range</code> <code>Optional[Sequence[str]]</code> <p>Sequence with two values, a lower and upper value for dates to load in.</p> <code>None</code> <code>column_filter_dict</code> <code>Optional[Dict[str, Sequence[str]]]</code> <p>A dictionary containing column: [values] where the values correspond to terms in the column that are to be filtered by.</p> <code>None</code> <code>run_id_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter to the specified run_id.</p> <code>'run_id'</code> <code>run_id</code> <code>Optional[str]</code> <p>The unique identifier for a run within the table that the read data is filtered to.</p> <code>None</code> <code>partition_column</code> <code>Optional[str]</code> <p>The name of the column that the table is partitioned by.</p> <code>None</code> <code>partition_type</code> <code>Optional[BigQueryTimePartitions]</code> <p>The unit of time the table is partitioned by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <code>None</code> <code>partition_value</code> <code>Optional[Union[Tuple[str, str], str]]</code> <p>The value or pair of values for filtering the partition column to.</p> <code>None</code> <code>flatten_struct_cols</code> <code>bool</code> <p>When true, any struct type columns in the loaded dataframe are replaced with individual columns for each of the fields in the structs.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code>"},{"location":"reference/#rdsa_utils.gcp.io.outputs","title":"<code>rdsa_utils.gcp.io.outputs</code>","text":"<p>Write outputs to GCP.</p>"},{"location":"reference/#rdsa_utils.gcp.io.outputs.write_table","title":"<code>write_table(df: Union[PandasDF, SparkDF], table_name: TablePath, mode: Literal['append', 'error', 'ignore', 'overwrite'] = 'error', partition_col: Optional[str] = None, partition_type: Optional[BigQueryTimePartitions] = None, partition_expiry_days: Optional[float] = None, clustered_fields: Optional[Union[str, List[str]]] = None) -&gt; None</code>","text":"<p>Write dataframe out to a Google BigQuery table.</p> <p>In the case the table already exists, behavior of this function depends on the save mode, specified by the mode function (default to throwing an exception). When mode is Overwrite, the schema of the DataFrame does not need to be the same as that of the existing table (the column order doesn't need be the same).</p> <p>If you use the <code>df.printSchema()</code> method directly in a print/log statement the code is processed and printed regardless of logging level. Instead you need to capture the output and pass this to the logger. See explanation here - https://stackoverflow.com/a/59935109</p> <p>To learn more about the partitioning of tables and how to use them in BigQuery: https://cloud.google.com/bigquery/docs/partitioned-tables</p> <p>To learn more about the clustering of tables and how to use them in BigQuery: https://cloud.google.com/bigquery/docs/clustered-tables</p> <p>To learn more about how spark dataframes are saved to BigQuery: https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, DataFrame]</code> <p>The dataframe to be saved.</p> required <code>table_name</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <code>mode</code> <code>Literal['append', 'error', 'ignore', 'overwrite']</code> <p>Whether to overwrite or append to the BigQuery table.     * <code>append</code>: Append contents of this :class:<code>DataFrame</code> to table.     * <code>overwrite</code>: Overwrite existing data.     * <code>error</code>: Throw exception if data already exists.     * <code>ignore</code>: Silently ignore this operation if data already exists.</p> <code>'error'</code> <code>partition_col</code> <code>Optional[str]</code> <p>A date or timestamp type column in the dataframe to use for the table partitioning.</p> <code>None</code> <code>partition_type</code> <code>Optional[BigQueryTimePartitions]</code> <p>The unit of time to partition the table by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <p>If <code>partition_col</code> is specified and <code>partition_type = None</code> then BigQuery will default to using <code>day</code> partition type.</p> <p>If 'partition_type<code>is specified and</code>partition_col = None<code>then the table will be partitioned by the ingestion time pseudo column, and can be referenced in BigQuery via either</code>_PARTITIONTIME as pt<code>or</code>_PARTITIONDATE' as pd`.</p> <p>See https://cloud.google.com/bigquery/docs/querying-partitioned-tables for more information on querying partitioned tables.</p> <code>None</code> <code>partition_expiry_days</code> <code>Optional[float]</code> <p>If specified, this is the number of days (any decimal values are converted to that proportion of a day) that BigQuery keeps the data in each partition.</p> <code>None</code> <code>clustered_fields</code> <code>Optional[Union[str, List[str]]]</code> <p>If specified, the columns (up to four) in the dataframe to cluster the data by when outputting. The order the columns are specified is important as will be the ordering of the clustering on the BigQuery table.</p> <p>See: https://cloud.google.com/bigquery/docs/querying-clustered-tables for more information on querying clustered tables.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/#helpers","title":"Helpers","text":""},{"location":"reference/#rdsa_utils.helpers.pyspark","title":"<code>rdsa_utils.helpers.pyspark</code>","text":"<p>A selection of helper functions for building in pyspark.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.aggregate_col","title":"<code>aggregate_col(df: SparkDF, col: str, operation: str) -&gt; float</code>","text":"<p>Aggregate (sum, max, min, or mean) a numeric PySpark column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame containing the column.</p> required <code>col</code> <code>str</code> <p>The name of the numeric column to aggregate.</p> required <code>operation</code> <code>str</code> <p>The type of aggregation to perform. Must be one of 'sum', 'max', 'min', or 'mean'.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The result of the specified aggregation on the column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.apply_col_func","title":"<code>apply_col_func(df: SparkDF, cols: List[str], func: Callable[[SparkDF, str], SparkDF]) -&gt; SparkDF</code>","text":"<p>Apply a function to a list of columns in a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame.</p> required <code>cols</code> <code>List[str]</code> <p>List of column names to apply the function to.</p> required <code>func</code> <code>Callable[[DataFrame, str], DataFrame]</code> <p>The function to apply, which should accept two arguments: (df, col).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF after applying the function to each column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.cache_time_df","title":"<code>cache_time_df(df: SparkDF) -&gt; None</code>","text":"<p>Cache a PySpark DataFrame and print the time taken to cache and count it.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to cache.</p> required <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/#rdsa_utils.helpers.pyspark.calc_median_price","title":"<code>calc_median_price(groups: Union[str, Sequence[str]], price_col: str = 'price') -&gt; SparkCol</code>","text":"<p>Calculate the median price per grouping level.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>Union[str, Sequence[str]]</code> <p>The grouping levels for calculating the average price.</p> required <code>price_col</code> <code>str</code> <p>Column name containing the product prices.</p> <code>'price'</code> <p>Returns:</p> Type Description <code>Column</code> <p>A single entry for each grouping level, and its median price.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.convert_cols_to_struct_col","title":"<code>convert_cols_to_struct_col(df: SparkDF, struct_col_name: str, struct_cols: Optional[Sequence[str]], no_struct_col_type: T.DataTypeSingleton = T.BooleanType(), no_struct_col_value: Any = None) -&gt; SparkDF</code>","text":"<p>Convert specified selection of columns to a single struct column.</p> <p>As BigQuery tables do not take to having an empty struct column appended to them, this function will create a placeholder column to put into the struct column if no column names to combine are passed.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe that contains the columns for combining.</p> required <code>struct_col_name</code> <code>str</code> <p>The name of the resulting struct column.</p> required <code>struct_cols</code> <code>Optional[Sequence[str]]</code> <p>A sequence of columns present in df for combining.</p> required <code>no_struct_col_type</code> <code>DataTypeSingleton</code> <p>If no struct_cols are present, this is the type that the dummy column to place in the struct will be, default = BooleanType.</p> <code>BooleanType()</code> <code>no_struct_col_value</code> <code>Any</code> <p>If no struct_cols are present, this is the value that will be used in the dummy column, default = None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    The input dataframe with the specified struct_cols dropped and replaced</code> <p>with a single struct type column containing those columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not all the specified struct_cols are present in df.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.convert_struc_col_to_columns","title":"<code>convert_struc_col_to_columns(df: SparkDF, convert_nested_structs: bool = False) -&gt; SparkDF</code>","text":"<p>Flatten struct columns in pyspark dataframe to individual columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe that may or may not contain struct type columns.</p> required <code>convert_nested_structs</code> <code>bool</code> <p>If true, function will recursively call until no structs are left. Inversely, when false, only top level structs are flattened; if these contain subsequent structs they would remain.</p> <code>False</code> <p>Returns:</p> Type Description <code>    The input dataframe but with any struct type columns dropped, and in</code> <p>its place the individual fields within the struct column as individual columns.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.count_nulls","title":"<code>count_nulls(df: SparkDF, subset_cols: Optional[Union[List[str], str]] = None) -&gt; pd.DataFrame</code>","text":"<p>Count the number of null values in the specified columns of a SparkDF.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to analyze.</p> required <code>subset_cols</code> <code>Optional[Union[List[str], str]]</code> <p>List of column names or a single column name as a string to count null values for. If not provided, counts are calculated for all columns.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame with the count of null values per column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_colname_to_value_map","title":"<code>create_colname_to_value_map(cols: Sequence[str]) -&gt; SparkCol</code>","text":"<p>Create a column name to value MapType column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_spark_session","title":"<code>create_spark_session(app_name: Optional[str] = None, size: Optional[Literal['small', 'medium', 'large', 'extra-large']] = None, extra_configs: Optional[Dict[str, str]] = None) -&gt; SparkSession</code>","text":"<p>Create a PySpark Session based on the specified size.</p> <p>This function creates a PySpark session with different configurations based on the size specified.</p> <p>The size can be 'default', 'small', 'medium', 'large', or 'extra-large'. Extra Spark configurations can be passed as a dictionary. If no size is given, then a basic Spark session is spun up.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>Optional[str]</code> <p>The spark session app name.</p> <code>None</code> <code>size</code> <code>Optional[Literal['small', 'medium', 'large', 'extra-large']]</code> <p>The size of the spark session to be created. It can be 'default', 'small', 'medium', 'large', or 'extra-large'.</p> <code>None</code> <code>extra_configs</code> <code>Optional[Dict[str, str]]</code> <p>Mapping of additional spark session config settings and the desired value for it. Will override any default settings.</p> <code>None</code> <p>Returns:</p> Type Description <code>SparkSession</code> <p>The created PySpark session.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified 'size' parameter is not one of the valid options: 'small', 'medium', 'large', or 'extra-large'.</p> <code>Exception</code> <p>If any other error occurs during the Spark session creation process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = create_spark_session('medium', {'spark.ui.enabled': 'false'})\n</code></pre> Session Details: <p>'small':     This is the smallest session that will realistically be used. It uses     only 1g of memory and 3 executors, and only 1 core. The number of     partitions are limited to 12, which can improve performance with     smaller data. It's recommended for simple data exploration of small     survey data or for training and demonstrations when several people     need to run Spark sessions simultaneously. 'medium':     A standard session used for analysing survey or synthetic datasets.     Also used for some Production pipelines based on survey and/or smaller     administrative data.It uses 6g of memory and 3 executors, and 3 cores.     The number of partitions are limited to 18, which can improve     performance with smaller data. 'large':     Session designed for running Production pipelines on large     administrative data, rather than just survey data. It uses 10g of     memory and 5 executors, 1g of memory overhead, and 5 cores. It uses the     default number of 200 partitions. 'extra-large':     Used for the most complex pipelines, with huge administrative     data sources and complex calculations. It uses 20g of memory and     12 executors, 2g of memory overhead, and 5 cores. It uses 240     partitions; not significantly higher than the default of 200,     but it is best for these to be a multiple of cores and executors.</p> References <p>The session sizes and their details are taken directly from the following resource: \"https://best-practice-and-impact.github.io/ons-spark/spark-overview/example-spark-sessions.html\"</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.cumulative_array","title":"<code>cumulative_array(df: SparkDF, array_col: str, output_colname: str) -&gt; SparkDF</code>","text":"<p>Convert a PySpark array column to a cumulative array column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame containing the array column.</p> required <code>array_col</code> <code>str</code> <p>The name of the array column to convert.</p> required <code>output_colname</code> <code>str</code> <p>The name of the new column to store the cumulative array.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with the cumulative array column added.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.cut_lineage","title":"<code>cut_lineage(df: SparkDF) -&gt; SparkDF</code>","text":"<p>Convert the SparkDF to a Java RDD and back again.</p> <p>This function is helpful in instances where Catalyst optimiser is causing memory errors or problems, as it only tries to optimise till the conversion point.</p> <p>Note: This uses internal members and may break between versions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>SparkDF to convert.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>New SparkDF created from Java RDD.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the lineage cutting process, particularly during conversion between SparkDF and Java RDD or accessing internal members.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = rdd.toDF()\n&gt;&gt;&gt; new_df = cut_lineage(df)\n&gt;&gt;&gt; new_df.count()\n3\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.drop_duplicates_reproducible","title":"<code>drop_duplicates_reproducible(df: SparkDF, col: str, id_col: Optional[str] = None) -&gt; SparkDF</code>","text":"<p>Remove duplicates from a PySpark DataFrame in a repeatable manner.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame.</p> required <code>col</code> <code>str</code> <p>The column to partition by for removing duplicates.</p> required <code>id_col</code> <code>Optional[str]</code> <p>The column to use for ordering within each partition. If None, a unique ID column is generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with duplicates removed.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.filter_out_values","title":"<code>filter_out_values(df: SparkDF, column: str, values_to_exclude: List[Union[str, int, float]], keep_nulls: bool = True) -&gt; SparkDF</code>","text":"<p>Exclude rows whose column value appears in the exclusion list.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>column</code> <code>str</code> <p>Name of the column to filter on.</p> required <code>values_to_exclude</code> <code>List[Union[str, int, float]]</code> <p>List of values to remove.</p> required <code>keep_nulls</code> <code>bool</code> <p>Whether to preserve NULL values in the column.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>values_to_exclude</code> is empty. If <code>column</code> is not found in <code>df</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered DataFrame with specified values excluded.</p> Notes <ul> <li><code>isin</code> performs exact matching. For reliable filtering of floating-point data,   prefer defining the column as DoubleType.</li> <li>FloatType columns may suffer from binary precision issues,   causing literal comparisons to fail unexpectedly.</li> <li>If you must filter on FloatType with approximate values, consider:</li> <li>Rounding the column to a fixed precision:      <code>python      df = df.withColumn(\"col\", F.round(F.col(\"col\"), 2))      filter_out_values(df, \"col\", [1.23, 4.56])</code></li> <li>Filtering by range to capture an approximate match:      <code>python      df.filter(~((F.col(\"col\") &gt;= 1.229) &amp; (F.col(\"col\") &lt;= 1.231)))</code></li> </ul> <p>Examples:</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.filter_out_values--keep-nulls-default","title":"Keep nulls (default)","text":"<pre><code>&gt;&gt;&gt; data = [(1, \"apple\"), (2, None), (3, \"banana\")]\n&gt;&gt;&gt; df = spark.createDataFrame(data, [\"id\", \"fruit\"])\n&gt;&gt;&gt; filter_out_values(df, \"fruit\", [\"apple\"]).show()\n+---+------+\n| id| fruit|\n+---+------+\n|  2|  null|\n|  3|banana|\n+---+------+\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.filter_out_values--exclude-nulls","title":"Exclude nulls","text":"<pre><code>&gt;&gt;&gt; filter_out_values(df, \"fruit\", [\"apple\"], keep_nulls=False).show()\n+---+------+\n| id| fruit|\n+---+------+\n|  3|banana|\n+---+------+\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.find_spark_dataframes","title":"<code>find_spark_dataframes(locals_dict: Dict[str, Union[SparkDF, Dict]]) -&gt; Dict[str, Union[SparkDF, Dict]]</code>","text":"<p>Extract SparkDF's objects from a given dictionary.</p> <p>This function scans the dictionary and returns another containing only entries where the value is a SparkDF. It also handles dictionaries within the input, including them in the output if their first item is a SparkDF.</p> <p>Designed to be used with locals() in Python, allowing extraction of all SparkDF variables in a function's local scope.</p> <p>Parameters:</p> Name Type Description Default <code>locals_dict</code> <code>Dict[str, Union[DataFrame, Dict]]</code> <p>A dictionary usually returned by locals(), with variable names as keys and their corresponding objects as values.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary with entries from locals_dict where the value is a SparkDF or a dictionary with a SparkDF as its first item.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dfs = find_spark_dataframes(locals())\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.get_unique","title":"<code>get_unique(df: SparkDF, col: str, remove_null: bool = False, verbose: bool = True) -&gt; List[Optional[Union[str, int, float]]]</code>","text":"<p>Return a list of unique values in a PySpark DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame containing the column.</p> required <code>col</code> <code>str</code> <p>The name of the column to analyze.</p> required <code>remove_null</code> <code>bool</code> <p>Whether to remove null values from output. Default is False.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to log the number of unique values. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List</code> <p>A list of unique values from the specified column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.get_window_spec","title":"<code>get_window_spec(partition_cols: Optional[Union[str, Sequence[str]]] = None, order_cols: Optional[Union[str, Sequence[str]]] = None) -&gt; WindowSpec</code>","text":"<p>Return ordered and partitioned WindowSpec, defaulting to whole df.</p> <p>Particularly useful when you don't know if the variable being used for partition_cols will contain values or not in advance.</p> <p>Parameters:</p> Name Type Description Default <code>partition_cols</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>If present the columns to partition a spark dataframe on.</p> <code>None</code> <code>order_cols</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>If present the columns to order a spark dataframe on (where order in sequence is order that orderBy is applied).</p> <code>None</code> <p>Returns:</p> Type Description <code>WindowSpec</code> <p>The WindowSpec object to be applied.</p> Usage <p>window_spec = get_window_spec(...)</p> <p>F.sum(values).over(window_spec)</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.is_df_empty","title":"<code>is_df_empty(df: SparkDF) -&gt; bool</code>","text":"<p>Check whether a spark dataframe contains any records.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.join_multi_dfs","title":"<code>join_multi_dfs(df_list: List[SparkDF], on: Union[str, List[str]], how: str) -&gt; SparkDF</code>","text":"<p>Join multiple Spark SparkDFs together.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of Spark SparkDFs to join.</p> required <code>on</code> <code>Union[str, List[str]]</code> <p>Column(s) on which to join the SparkDFs.</p> required <code>how</code> <code>str</code> <p>Type of join to perform (e.g., 'inner', 'outer', 'left', 'right').</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A SparkDF that is the result of joining all SparkDFs in the list.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.load_csv","title":"<code>load_csv(spark: SparkSession, filepath: str, keep_columns: Optional[List[str]] = None, rename_columns: Optional[Dict[str, str]] = None, drop_columns: Optional[List[str]] = None, **kwargs) -&gt; SparkDF</code>","text":"<p>Load a CSV file into a PySpark DataFrame.</p> <p>spark     Active SparkSession. filepath     The full path and filename of the CSV file to load. keep_columns     A list of column names to keep in the DataFrame, dropping all others.     Default value is None. rename_columns     A dictionary to rename columns where keys are existing column     names and values are new column names.     Default value is None. drop_columns     A list of column names to drop from the DataFrame.     Default value is None. kwargs     Additional keyword arguments to pass to the <code>spark.read.csv</code> method.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>PySpark DataFrame containing the data from the CSV file.</p> Notes <p>Transformation order: 1. Columns are kept according to <code>keep_columns</code>. 2. Columns are dropped according to <code>drop_columns</code>. 3. Columns are renamed according to <code>rename_columns</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error loading the file.</p> <code>ValueError</code> <p>If a column specified in rename_columns, drop_columns, or keep_columns is not found in the DataFrame.</p> <p>Examples:</p> <p>Load a CSV file with multiline and rename columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(\n        spark,\n        \"/path/to/file.csv\",\n        multiLine=True,\n        rename_columns={\"old_name\": \"new_name\"}\n    )\n</code></pre> <p>Load a CSV file with a specific encoding:</p> <pre><code>&gt;&gt;&gt; df = load_csv(spark, \"/path/to/file.csv\", encoding=\"ISO-8859-1\")\n</code></pre> <p>Load a CSV file and keep only specific columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(spark, \"/path/to/file.csv\", keep_columns=[\"col1\", \"col2\"])\n</code></pre> <p>Load a CSV file and drop specific columns:</p> <pre><code>&gt;&gt;&gt; df = load_csv(spark, \"/path/to/file.csv\", drop_columns=[\"col1\", \"col2\"])\n</code></pre> <p>Load a CSV file with custom delimiter and multiline:</p> <pre><code>&gt;&gt;&gt; df = load_csv(spark, \"/path/to/file.csv\", sep=\";\", multiLine=True)\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.map_column_names","title":"<code>map_column_names(df: SparkDF, mapper: Mapping[str, str]) -&gt; SparkDF</code>","text":"<p>Map column names to the given values in the mapper.</p> <p>If the column name is not in the mapper the name doesn't change.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.map_column_values","title":"<code>map_column_values(df: SparkDF, dict_: Dict[str, str], input_col: str, output_col: Union[str, None] = None) -&gt; SparkDF</code>","text":"<p>Map PySpark column to dictionary keys.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to modify.</p> required <code>dict_</code> <code>Dict[str, str]</code> <p>Dictionary for mapping values in input_col to new values.</p> required <code>input_col</code> <code>str</code> <p>The name of the column to replace values in.</p> required <code>output_col</code> <code>Union[str, None]</code> <p>The name of the new column with replaced values. Defaults to input_col if not provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with the new column added.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.melt","title":"<code>melt(df: SparkDF, id_vars: Union[str, Sequence[str]], value_vars: Union[str, Sequence[str]], var_name: str = 'variable', value_name: str = 'value') -&gt; SparkDF</code>","text":"<p>Melt a spark dataframe in a pandas like fashion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark dataframe to melt.</p> required <code>id_vars</code> <code>Union[str, Sequence[str]]</code> <p>The names of the columns to use as identifier variables.</p> required <code>value_vars</code> <code>Union[str, Sequence[str]]</code> <p>The names of the columns containing the data to unpivot.</p> required <code>var_name</code> <code>str</code> <p>The name of the target column containing variable names (i.e. the original column names).</p> <code>'variable'</code> <code>value_name</code> <code>str</code> <p>The name of the target column containing the unpivoted data.</p> <code>'value'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The \"melted\" input data as a pyspark data frame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame(\n...     [[1, 2, 3, 4],\n...      [5, 6, 7, 8],\n...      [9, 10, 11, 12]],\n...     [\"col1\", \"col2\", \"col3\", \"col4\"])\n&gt;&gt;&gt; melt(df=df, id_vars=\"col1\", value_vars=[\"col2\", \"col3\"]).show()\n+----+--------+-----+\n|col1|variable|value|\n+----+--------+-----+\n|   1|    col2|    2|\n|   1|    col3|    3|\n|   5|    col2|    6|\n|   5|    col3|    7|\n|   9|    col2|   10|\n|   9|    col3|   11|\n+----+--------+-----+\n</code></pre> <pre><code>&gt;&gt;&gt; melt(df=df, id_vars=[\"col1\", \"col2\"], value_vars=[\"col3\", \"col4\"]\n... ).show()\n+----+----+--------+-----+\n|col1|col2|variable|value|\n+----+----+--------+-----+\n|   1|   2|    col3|    3|\n|   1|   2|    col4|    4|\n|   5|   6|    col3|    7|\n|   5|   6|    col4|    8|\n|   9|  10|    col3|   11|\n|   9|  10|    col4|   12|\n+----+----+--------+-----+\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.pyspark_random_uniform","title":"<code>pyspark_random_uniform(df: SparkDF, output_colname: str, lower_bound: float = 0, upper_bound: float = 1, seed: Optional[int] = None) -&gt; SparkDF</code>","text":"<p>Mimic numpy.random.uniform for PySpark.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to which the column will be added.</p> required <code>output_colname</code> <code>str</code> <p>The name of the new column to be created.</p> required <code>lower_bound</code> <code>float</code> <p>The lower bound of the uniform distribution. Defaults to 0.</p> <code>0</code> <code>upper_bound</code> <code>float</code> <p>The upper bound of the uniform distribution. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>Optional[int]</code> <p>Seed for random number generation. Defaults to None for non-deterministic results.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with the new column added.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.rank_numeric","title":"<code>rank_numeric(numeric: Union[str, Sequence[str]], group: Union[str, Sequence[str]], ascending: bool = False) -&gt; SparkCol</code>","text":"<p>Rank a numeric and assign a unique value to each row.</p> <p>The <code>F.row_number()</code> method has been selected as a method to rank as gives a unique number to each row. Other methods such as <code>F.rank()</code> and <code>F.dense_rank()</code> do not assign unique values per row.</p> <p>Parameters:</p> Name Type Description Default <code>numeric</code> <code>Union[str, Sequence[str]]</code> <p>The column name or list of column names containing values which will be ranked.</p> required <code>group</code> <code>Union[str, Sequence[str]]</code> <p>The grouping levels to rank the numeric column or columns over.</p> required <code>ascending</code> <code>bool</code> <p>Dictates whether high or low values are ranked as the top value.</p> <code>False</code> <p>Returns:</p> Type Description <code>Column</code> <p>Contains a rank for the row in its grouping level.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.select_first_obs_appearing_in_group","title":"<code>select_first_obs_appearing_in_group(df: SparkDF, group: Sequence[str], date_col: str, ascending: bool) -&gt; SparkDF</code>","text":"<p>Rank and select observation in group based on earliest or latest date.</p> <p>Given that there can be multiple observations per group, select observation that appears first or last (depending on whether ascending is set to True or False, respectively).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe that contains the group and date_col.</p> required <code>group</code> <code>Sequence[str]</code> <p>The grouping levels required to find the observation that appears first or last (depending on whether ascending is set to True or False, respectively)</p> required <code>date_col</code> <code>str</code> <p>Column name containing the dates of each observation.</p> required <code>ascending</code> <code>bool</code> <p>Dictates whether first or last observation within a grouping is selected (depending on whether ascending is set to True or False, respectively).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe that contains each observation per group that appeared first or last (depending on whether ascending is set to True or False, respectively) according to date_col.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.set_df_columns_nullable","title":"<code>set_df_columns_nullable(df: SparkDF, column_list: List[str], nullable: Optional[bool] = True) -&gt; SparkDF</code>","text":"<p>Change specified columns nullable value.</p> <p>Sometimes find that spark creates columns that have the nullable attribute set to False, which can cause issues if this dataframe is saved to a table as it will set the schema for that column to not allow missing values.</p> <p>Changing this parameter for a column appears to be very difficult (and also potentially costly [see so answer comments] - SO USE ONLY IF NEEDED).</p> <p>The solution implemented is taken from Stack Overflow post: https://stackoverflow.com/a/51821437</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe with columns to have nullable attribute changed.</p> required <code>column_list</code> <code>List[str]</code> <p>List of columns to change nullable attribute.</p> required <code>nullable</code> <code>Optional[bool]</code> <p>The value to set the nullable attribute to for the specified columns.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe but with nullable attribute changed for specified columns.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.set_nulls","title":"<code>set_nulls(df: SparkDF, column: str, values: Union[str, List[str]]) -&gt; SparkDF</code>","text":"<p>Replace specified values with nulls in given column of PySpark df.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to modify.</p> required <code>column</code> <code>str</code> <p>The name of the column in which to replace values.</p> required <code>values</code> <code>Union[str, List[str]]</code> <p>The value(s) to replace with nulls.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with specified values replaced by nulls.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.smart_coalesce","title":"<code>smart_coalesce(df: SparkDF, target_file_size_mb: int = 512) -&gt; SparkDF</code>","text":"<p>Coalesce a Spark DataFrame to an appropriate number of partitions.</p> <p>Coalesces a Spark DataFrame to an appropriate number of partitions based on its estimated size using Spark's Catalyst optimiser and a user-defined target file size.</p> <p>This function helps to reduce the number of output files written when saving a DataFrame to storage systems such as Hive or Amazon S3 by adjusting the number of partitions using <code>.coalesce()</code>. It is especially useful for avoiding the \"small files problem\", which can negatively affect performance, metadata management, and query planning.</p> <p>It leverages Spark Catalyst's query plan statistics to get a logical estimate of the DataFrame's size in bytes without triggering a full job or action. Based on the provided <code>target_file_size_mb</code>, it calculates how many output files are needed and reduces the number of partitions accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame that will be written to storage.</p> required <code>target_file_size_mb</code> <code>int</code> <p>The desired maximum size of each output file in megabytes. This controls the number of output files by estimating how many are needed to approximately match the total data volume. Default is 512 MB.</p> <code>512</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Spark DataFrame with a reduced number of partitions, ready to be written to disk using <code>.write()</code>. The number of partitions is chosen to produce output files close to the target size.</p> Notes <ul> <li>This function uses Spark Catalyst's logical plan statistics. These may be   outdated or unavailable if statistics haven't been collected   (e.g., ANALYZE TABLE not run).</li> <li>If the estimated size is zero or unavailable, it defaults to a single partition.</li> <li>This function uses <code>.coalesce()</code> which avoids a shuffle but can cause skew if the   data is unevenly distributed. For very large datasets, consider using   <code>repartition()</code> instead.</li> <li>This function is best used as a final optimisation before writing output files,   especially to S3, Hive, or HDFS.</li> </ul> Why Small Files Are a Problem <p>Writing many small files (e.g., thousands of files per partition) negatively impacts:</p> <ol> <li>Hive Metastore:</li> <li>Hive must track every individual file in the metastore.</li> <li> <p>Too many files lead to slow table listings, metadata queries,      and planning time.</p> </li> <li> <p>Spark Performance:</p> </li> <li>During reads, Spark spawns a task per file.</li> <li> <p>Thousands of tiny files = thousands of tasks =      job scheduling overhead + slow query startup.</p> </li> <li> <p>S3 Performance:</p> </li> <li>S3 is object storage, not a filesystem. Each file written = one PUT request.</li> <li>Too many files increase write latency and cost.</li> <li>During reads, many GET requests slow down performance.</li> </ol> <p>Examples:</p> <p>Reduce number of output files for a moderate-sized DataFrame:</p> <pre><code>&gt;&gt;&gt; coalesced_df = smart_coalesce(df, target_file_size_mb=200)\n&gt;&gt;&gt; coalesced_df.write.mode(\"overwrite\").saveAsTable(\"my_optimised_table\")\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.sum_columns","title":"<code>sum_columns(df: SparkDF, cols_to_sum: List[str], output_col: str) -&gt; SparkDF</code>","text":"<p>Calculate row-wise sum of specified PySpark columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The PySpark DataFrame to modify.</p> required <code>cols_to_sum</code> <code>List[str]</code> <p>List of column names to sum together.</p> required <code>output_col</code> <code>str</code> <p>The name of the new column to create with the sum.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The SparkDF with the new column added.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.to_list","title":"<code>to_list(df: SparkDF) -&gt; List[Union[Any, List[Any]]]</code>","text":"<p>Convert Spark DF to a list.</p> <p>Returns:</p> Type Description <code>list or list of lists</code> <p>If the input DataFrame has a single column then a list of column values will be returned. If the DataFrame has multiple columns then a list of row data as lists will be returned.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.to_spark_col","title":"<code>to_spark_col(_func=None, *, exclude: Sequence[str] = None) -&gt; Callable</code>","text":"<p>Convert str args to Spark Column if not already.</p> Usage <p>Use as a decorator on a function.</p> <p>To convert all string arguments to spark column</p> <p>@to_spark_col def my_func(arg1, arg2)</p> <p>To exclude a string arguments from being converted to a spark column</p> <p>@to_spark_col(exclude=['arg2']) def my_func(arg1, arg2)</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.transform","title":"<code>transform(self, f, *args, **kwargs)</code>","text":"<p>Chain Pyspark function.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.truncate_external_hive_table","title":"<code>truncate_external_hive_table(spark: SparkSession, table_identifier: str) -&gt; None</code>","text":"<p>Truncate an External Hive table stored on S3 or HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>table_identifier</code> <code>str</code> <p>The name of the Hive table to truncate. This can either be in the format '.' or simply '' if the current Spark session has a database set. required <p>Returns:</p> Type Description <code>None</code> <p>This function does not return any value. It performs an action of truncating the table.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table name is incorrectly formatted, the database is not provided when required, or if the table does not exist.</p> <code>AnalysisException</code> <p>If there is an issue with partition operations or SQL queries.</p> <code>Exception</code> <p>If there is a general failure during the truncation process.</p> <p>Examples:</p> <p>Truncate a Hive table named 'my_database.my_table':</p> <pre><code>&gt;&gt;&gt; truncate_external_hive_table(spark, 'my_database.my_table')\n</code></pre> <p>Or, if the current Spark session already has a database set:</p> <pre><code>&gt;&gt;&gt; spark.catalog.setCurrentDatabase('my_database')\n&gt;&gt;&gt; truncate_external_hive_table(spark, 'my_table')\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.union_mismatched_dfs","title":"<code>union_mismatched_dfs(df1: SparkDF, df2: SparkDF) -&gt; SparkDF</code>","text":"<p>Perform a union between PySpark DataFrames with mismatched column names.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>DataFrame</code> <p>The first PySpark DataFrame.</p> required <code>df2</code> <code>DataFrame</code> <p>The second PySpark DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A SparkDF resulting from the union of df1 and df2, with missing columns filled with null values.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.union_multi_dfs","title":"<code>union_multi_dfs(df_list: List[SparkDF]) -&gt; SparkDF</code>","text":"<p>Perform a union on all SparkDFs in the provided list.</p> Note <p>All SparkDFs must have the same columns.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>List[DataFrame]</code> <p>List of PySpark DataFrames to union.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A SparkDF that is the result of the union of all SparkDFs in the list.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.unpack_list_col","title":"<code>unpack_list_col(df: SparkDF, list_col: str, unpacked_col: str) -&gt; SparkDF</code>","text":"<p>Unpack a spark column containing a list into multiple rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Contains the list column to unpack.</p> required <code>list_col</code> <code>str</code> <p>The name of the column which contains lists.</p> required <code>unpacked_col</code> <code>str</code> <p>The name of the column containing the unpacked list items.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Contains a new row for each unpacked list item.</p>"},{"location":"reference/#rdsa_utils.helpers.python","title":"<code>rdsa_utils.helpers.python</code>","text":"<p>Miscellaneous helper functions for Python.</p>"},{"location":"reference/#rdsa_utils.helpers.python.always_iterable_local","title":"<code>always_iterable_local(obj: Any) -&gt; Callable</code>","text":"<p>Supplement more-itertools <code>always_iterable</code> to also exclude dicts.</p> <p>By default it would convert a dictionary to an iterable of just its keys, dropping all the values. This change makes it so dictionaries are not altered (similar to how strings aren't broken down).</p>"},{"location":"reference/#rdsa_utils.helpers.python.calc_product_of_dict_values","title":"<code>calc_product_of_dict_values(**kwargs: Mapping[str, Union[str, float, Iterable]]) -&gt; Mapping[str, any]</code>","text":"<p>Create cartesian product of values for each kwarg.</p> <p>In order to create product of values, the values are converted to a list so that product of values can be derived.</p> <p>Yields:</p> Type Description <code>    Next result of cartesian product of kwargs values.</code> Example <p>my_dict = {     'key1': 1,     'key2': [2, 3, 4] }</p> <p>list(calc_product_of_dict_values(**my_dict))</p> <p>[{'key1': 1, 'key2': 2}, {'key1': 1, 'key2': 3}, {'key1': 1, 'key2': 4}]</p> Notes <p>Modified from: https://stackoverflow.com/a/5228294</p>"},{"location":"reference/#rdsa_utils.helpers.python.check_file","title":"<code>check_file(filepath: str) -&gt; bool</code>","text":"<p>Check if a file exists on the local file system and meets specific criteria.</p> <p>This function checks whether the given path corresponds to a valid file on the local or network file system. It ensures the file exists, is not a directory, and its size is greater than zero bytes.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to a local/network file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, is not a directory, and size &gt; 0, otherwise False.</p> Example <p>check_file(\"folder/file.txt\") True check_file(\"folder/file_0_bytes.txt\") False</p>"},{"location":"reference/#rdsa_utils.helpers.python.convert_date_strings_to_datetimes","title":"<code>convert_date_strings_to_datetimes(start_date: str, end_date: str) -&gt; Tuple[pd.Timestamp, pd.Timestamp]</code>","text":"<p>Convert start and end dates from strings to timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Datetime like object which is used to define the start date for filter. Acceptable string formats include (but not limited to): MMMM YYYY, YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified the start_date is set as first day of month.</p> required <code>end_date</code> <code>str</code> <p>Datetime like object which is used to define the start date for filter. Acceptable string formats include (but not limited to): MMMM YYYY, YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified the end_date is set as final day of month.</p> required <p>Returns:</p> Type Description <code>tuple[Timestamp, Timestamp]</code> <p>Tuple where the first value is the start date and the second the end date.</p>"},{"location":"reference/#rdsa_utils.helpers.python.convert_types_iterable","title":"<code>convert_types_iterable(lst: Iterable, dtype: type = float) -&gt; List</code>","text":"<p>Convert the data type of elements in an iterable.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>Iterable</code> <p>The iterable whose elements are to be converted.</p> required <code>dtype</code> <code>type</code> <p>The target data type to which elements in the iterable should be converted. Defaults to <code>float</code>.</p> <code>float</code> <p>Returns:</p> Type Description <code>list</code> <p>A new list with elements converted to the specified data type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; convert_types_iterable([1, 2, 3])\n[1.0, 2.0, 3.0]\n</code></pre> <pre><code>&gt;&gt;&gt; convert_types_iterable((10, 20, 30), dtype=str)\n['10', '20', '30']\n</code></pre> <pre><code>&gt;&gt;&gt; convert_types_iterable({'a', 'b', 'c'}, dtype=ord)\n[97, 98, 99]\n</code></pre> <pre><code>&gt;&gt;&gt; convert_types_iterable(['10', '20', '30'], dtype=int)\n[10, 20, 30]\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.create_folder","title":"<code>create_folder(dirpath: str) -&gt; None</code>","text":"<p>Create a directory on a local network drive.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>str</code> <p>The path to the directory to create.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The directory will be created if it does not already exist, including parent directories.</p> Example <p>create_folder(\"example_folder/subfolder\")</p>"},{"location":"reference/#rdsa_utils.helpers.python.create_folder--the-directory-example_foldersubfolder-will-be-created-if-it-does-not-exist","title":"The directory \"example_folder/subfolder\" will be created if it does not exist.","text":""},{"location":"reference/#rdsa_utils.helpers.python.directory_exists","title":"<code>directory_exists(dirpath: str) -&gt; bool</code>","text":"<p>Test if given path is a directory on the local file system.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>str</code> <p>The directory path to check exists.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the dirpath is a directory, False otherwise.</p> Example <p>directory_exists(\"folder\") True directory_exists(\"non_existing_folder\") dirpath='.../non_existing_folder' cannot be found. False</p>"},{"location":"reference/#rdsa_utils.helpers.python.dump_environment_requirements","title":"<code>dump_environment_requirements(output_file: str, tool: str = 'pip', args: List[str] = ['list', '--format=freeze']) -&gt; None</code>","text":"<p>Dump the current Python environment dependencies to a text file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>Path to the output text file where the list of dependencies will be saved. If the directory does not exist, it will be created.</p> required <code>tool</code> <code>str</code> <p>The command-line tool to use for exporting dependencies (e.g. 'pip', 'poetry', or 'uv'). Default is 'pip'.</p> <code>'pip'</code> <code>args</code> <code>List[str]</code> <p>The arguments to pass to the selected tool. For pip, the default is ['list', '--format=freeze']. For poetry, a common option is ['export', '--without-hashes']. For uv, you might use ['pip', 'freeze'].</p> <code>['list', '--format=freeze']</code> <p>Returns:</p> Type Description <code>None</code> <p>This function writes to the specified file and does not return anything.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dump_environment_requirements(\"requirements.txt\")\n&gt;&gt;&gt; dump_environment_requirements(\n...     \"requirements.txt\",\n...     tool=\"pip\",\n...     args=[\"freeze\"]\n... )\n&gt;&gt;&gt; dump_environment_requirements(\n...     \"requirements.txt\",\n...     tool=\"poetry\",\n...     args=[\"export\", \"--without-hashes\"]\n... )\n&gt;&gt;&gt; dump_environment_requirements(\n...     \"requirements.txt\",\n...     tool=\"uv\",\n...     args=[\"pip\", \"freeze\"]\n... )\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.extend_lists","title":"<code>extend_lists(sections: List[List[str]], elements_to_add: List[str]) -&gt; None</code>","text":"<p>Check list elements are unique then append to existing list.</p> <p>Note the <code>.extend</code> method in Python overwrites each section. There is no need to assign a variable to this function, the section will update automatically.</p> <p>The function can be used with the <code>load_config</code> function to extend a value list in a config yaml file. For example with a <code>config.yaml</code> file as per below:</p> <pre><code>input_columns\n    - col_a\n    - col_b\n\noutput_columns\n    - col_b\n</code></pre> <p>To add column <code>col_c</code> the function can be utilised as follows:</p> <pre><code>config = load_config(\"config.yaml\")\n\nsections = [config['input_columns'], config['output_columns']]\nelements_to_add = ['col_c']\n\nextend_lists(sections, elements_to_add)\n</code></pre> <p>The output will be as follows.</p> <pre><code>input_columns\n    - col_a\n    - col_b\n    - col_c\n\noutput_columns\n    - col_b\n    - col_c\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>List[List[str]]</code> <p>The section to be updated with the extra elements.</p> required <code>elements_to_add</code> <code>List[str]</code> <p>The new elements to add to the specified sections.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Note the <code>.extend</code> method in Python overwrites the sections. There is no need to assign a variable to this function, the section will update automatically.</p>"},{"location":"reference/#rdsa_utils.helpers.python.file_exists","title":"<code>file_exists(filepath: str) -&gt; bool</code>","text":"<p>Test if file exists on the local file system.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Filepath of file check exists.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists, else False.</p> Example <p>file_exists(\"folder/file.txt\") True file_exists(\"folder/non_existing_file.txt\") filepath='.../folder/non_existing_file.txt' cannot be found. False</p>"},{"location":"reference/#rdsa_utils.helpers.python.file_size","title":"<code>file_size(filepath: str) -&gt; int</code>","text":"<p>Return the size of the file from the network drive in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The filepath of file to check for size.</p> required <p>Returns:</p> Type Description <code>int</code> <p>An integer value indicating the size of the file in bytes</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Example <p>file_size(\"folder/file.txt\") 90 file_size(\"folder/non_existing_file.txt\") FileNotFoundError: filepath='.../folder/non_existing_file.txt' cannot be found.</p>"},{"location":"reference/#rdsa_utils.helpers.python.flatten_iterable","title":"<code>flatten_iterable(iterable: Iterable, types_to_flatten: Union[type, Tuple] = (list, tuple)) -&gt; List</code>","text":"<p>Flatten an iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>An iterable that may contain elements of various types.</p> required <code>types_to_flatten</code> <code>Union[type, Tuple]</code> <p>Data type(s) that should be flattened. Defaults to (list, tuple).</p> <code>(list, tuple)</code> <p>Returns:</p> Type Description <code>list</code> <p>A flattened list with all elements from the input iterable, with specified types unpacked.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; flatten_iterable([1, [2, 3], (4, 5), 'abc'])\n[1, 2, 3, 4, 5, 'abc']\n&gt;&gt;&gt; flatten_iterable([1, [2, 3], (4, 5), 'abc'], types_to_flatten=list)\n[1, 2, 3, (4, 5), 'abc']\n&gt;&gt;&gt; flatten_iterable(['a', 'bc', ['d', 'e']], types_to_flatten=str)\n['a', 'b', 'c', 'd', 'e']\n&gt;&gt;&gt; flatten_iterable((1, [2, 3], (4, 5), 'abc'), types_to_flatten=(list, tuple))\n(1, 2, 3, 4, 5, 'abc')\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.interleave_iterables","title":"<code>interleave_iterables(iterable1: Iterable, iterable2: Iterable) -&gt; List</code>","text":"<p>Interleave two iterables element by element.</p> <p>Parameters:</p> Name Type Description Default <code>iterable1</code> <code>Iterable</code> <p>The first iterable to interleave.</p> required <code>iterable2</code> <code>Iterable</code> <p>The second iterable to interleave.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A new list with elements from <code>iterable1</code> and <code>iterable2</code> interleaved.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If either of the inputs is not an iterable of types: list, tuple, string, or range.</p> <code>ValueError</code> <p>If the lengths of the two iterables do not match.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; interleave_iterables([1, 2, 3], [4, 5, 6])\n[1, 4, 2, 5, 3, 6]\n</code></pre> <pre><code>&gt;&gt;&gt; interleave_iterables((1, 2, 3), ('a', 'b', 'c'))\n[1, 'a', 2, 'b', 3, 'c']\n</code></pre> <pre><code>&gt;&gt;&gt; interleave_iterables('ABC', '123')\n['A', '1', 'B', '2', 'C', '3']\n</code></pre> <pre><code>&gt;&gt;&gt; interleave_iterables(range(3), range(10, 13))\n[0, 10, 1, 11, 2, 12]\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.list_convert","title":"<code>list_convert(obj: Any) -&gt; List[Any]</code>","text":"<p>Convert object to list using more-itertools' <code>always_iterable</code>.</p>"},{"location":"reference/#rdsa_utils.helpers.python.md5_sum","title":"<code>md5_sum(filepath: str) -&gt; str</code>","text":"<p>Get md5sum of a specific file on the local file system.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Filepath of file to create md5 hash from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The md5sum of the file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Example <p>md5_sum(\"folder/file.txt\") \"d41d8cd98f00b204e9800998ecf8427e\" md5_sum(\"folder/non_existing_file.txt\") FileNotFoundError: filepath='../folder/non_existing_file.txt' cannot be found.</p>"},{"location":"reference/#rdsa_utils.helpers.python.merge_multi_dfs","title":"<code>merge_multi_dfs(df_list: list, on: Union[str, list], how: str, fillna_val: Union[None, object] = None) -&gt; pd.DataFrame</code>","text":"<p>Perform consecutive merges on a list of pandas DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>df_list</code> <code>list</code> <p>A list of DataFrames to be merged.</p> required <code>on</code> <code>Union[str, list]</code> <p>Column name(s) to merge on.</p> required <code>how</code> <code>str</code> <p>Type of merge to be performed. Must be one of 'left', 'right', 'outer', 'inner'.</p> required <code>fillna_val</code> <code>Union[None, object]</code> <p>Value to replace missing values with. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resulting DataFrame after merging and optional filling of missing values.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>df_list</code> is not a list of pandas DataFrames, or <code>on</code> is not a string or list of strings, or <code>how</code> is not a string.</p> <code>ValueError</code> <p>If the <code>how</code> argument is not one of 'left', 'right', 'outer', or 'inner'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n&gt;&gt;&gt; df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n&gt;&gt;&gt; df3 = pd.DataFrame({'key': ['A'], 'value3': [6]})\n&gt;&gt;&gt; merge_multi_dfs([df1, df2, df3], on='key', how='inner')\n  key  value1  value2  value3\n0   A       1       4       6\n</code></pre> <pre><code>&gt;&gt;&gt; df1 = pd.DataFrame({'key': ['A', 'B', 'C'], 'value1': [1, 2, 3]})\n&gt;&gt;&gt; df2 = pd.DataFrame({'key': ['A', 'B'], 'value2': [4, 5]})\n&gt;&gt;&gt; merge_multi_dfs([df1, df2], on='key', how='outer',  fillna_val=0)\n  key  value1  value2\n0   A        1        4\n1   B        2        5\n2   C        3        0\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.overwrite_dictionary","title":"<code>overwrite_dictionary(base_dict: Mapping[str, Any], override_dict: Mapping[str, Any]) -&gt; Dict[str, Any]</code>","text":"<p>Overwrite dictionary values with user defined values.</p> <p>The following restrictions are in place: * base_dict and override_dict have the same value which is not dictionary   then override_dict has priority. * If base_dict contains dictionary and override_dict contains a value (e.g.   string or list) with the same key, priority is upon base_dict and   the override_dict value is ignored. * If key is in override_dict but not in base_dict then an Exception is   raised and code stops. * Any other restrictions will require code changes.</p> <p>Parameters:</p> Name Type Description Default <code>base_dict</code> <code>Mapping[str, Any]</code> <p>Dictionary containing existing key value pairs.</p> required <code>override_dict</code> <code>Mapping[str, Any]</code> <p>Dictionary containing new keys/values to inset into base_dict.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The base_dict with any keys matching the override_dict being replaced. Any keys not present in base_dict are appended.</p> Example <p>dic1 = {\"var1\": \"value1\", \"var2\": {\"var3\": 1.1, \"var4\": 4.4}, \"var5\": [1, 2, 3]} dic2 = {\"var2\": {\"var3\": 9.9}}</p> <p>overwrite_dictionary(dic1, dic2) {'var1': 'value1', 'var2': {'var3': 9.9, 'var4': 4.4}, 'var5': [1, 2, 3]}</p> <p>dic3 = {\"var2\": {\"var3\": 9.9}, \"var6\": -1} overwrite_dictionary(dic1, dic3) ERROR main: ('var6', -1) not in base_dict</p> Notes <p>Modified from: https://stackoverflow.com/a/58742155</p> Warning <p>Due to recursive nature of function, the function will overwrite the base_dict object that is passed into the original function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a key is present in override_dict but not base_dict.</p>"},{"location":"reference/#rdsa_utils.helpers.python.pairwise_iterable","title":"<code>pairwise_iterable(iterable: Iterable) -&gt; zip</code>","text":"<p>Return pairs of adjacent values from the input iterable.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>An iterable object (e.g., list, tuple, string) from which pairs of adjacent values will be generated.</p> required <p>Returns:</p> Type Description <code>zip</code> <p>An iterator of tuples, each containing a pair of adjacent values from the input iterable.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not an iterable.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; list(pairwise_iterable([1, 2, 3, 4]))\n[(1, 2), (2, 3), (3, 4)]\n</code></pre> <pre><code>&gt;&gt;&gt; list(pairwise_iterable('abcde'))\n[('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')]\n</code></pre> <pre><code>&gt;&gt;&gt; list(pairwise_iterable((10, 20, 30)))\n[(10, 20), (20, 30)]\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.read_header","title":"<code>read_header(filepath: str) -&gt; str</code>","text":"<p>Return the first line of a file on the local file system.</p> <p>Reads the first line and removes the newline/returncarriage symbol.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to a local/network file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The first line of the file as a string.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Example <p>read_header(\"folder/file.txt\") \"This is the first line of the file.\" read_header(\"folder/non_existing_file.txt\") FileNotFoundError: filepath='.../folder/non_existing_file.txt' cannot be found.</p>"},{"location":"reference/#rdsa_utils.helpers.python.setdiff","title":"<code>setdiff(a: Iterable, b: Iterable) -&gt; List[Any]</code>","text":"<p>Return a list of elements that are present in <code>a</code> but not in <code>b</code>.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Iterable</code> <p>The first iterable from which elements are to be selected.</p> required <code>b</code> <code>Iterable</code> <p>The second iterable containing elements to be excluded.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of elements that are in <code>a</code> but not in <code>b</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; setdiff([1, 2, 3, 4], [3, 4, 5, 6])\n[1, 2]\n&gt;&gt;&gt; setdiff('abcdef', 'bdf')\n['a', 'c', 'e']\n&gt;&gt;&gt; setdiff({1, 2, 3}, {2, 3, 4})\n[1]\n&gt;&gt;&gt; setdiff(range(5), range(2, 7))\n[0, 1]\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.python.time_it","title":"<code>time_it(*timer_args, **timer_kwargs) -&gt; Callable</code>","text":"<p>Measure the execution time of a function, with options to configure Timer.</p> <p>Parameters:</p> Name Type Description Default <code>timer_args</code> <p>Positional arguments to pass to the Timer object.</p> <code>()</code> <code>timer_kwargs</code> <p>Keyword arguments to pass to the Timer object.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A wrapped function that includes timing measurement.</p> Example <p>@time_it() def example_function():     # Function implementation</p>"},{"location":"reference/#rdsa_utils.helpers.python.tuple_convert","title":"<code>tuple_convert(obj: Any) -&gt; Tuple[Any]</code>","text":"<p>Convert object to tuple using more-itertools' <code>always_iterable</code>.</p>"},{"location":"reference/#rdsa_utils.helpers.python.write_string_to_file","title":"<code>write_string_to_file(content: bytes, filepath: str) -&gt; None</code>","text":"<p>Write a string into the specified file path on the local file system.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>bytes</code> <p>The content to write into the file.</p> required <code>filepath</code> <code>str</code> <p>The path to the file where the content will be written. If the file already exists, it will be overwritten.</p> required <p>Returns:</p> Type Description <code>None</code> Example <p>write_string_to_file(b\"Hello, World!\", \"example.txt\")</p>"},{"location":"reference/#rdsa_utils.helpers.python.write_string_to_file--the-content-hello-world-will-be-written-to-exampletxt","title":"The content \"Hello, World!\" will be written to \"example.txt\"","text":""},{"location":"reference/#io","title":"IO","text":""},{"location":"reference/#rdsa_utils.io.config","title":"<code>rdsa_utils.io.config</code>","text":"<p>Module for code relating to loading config files.</p>"},{"location":"reference/#rdsa_utils.io.config.LoadConfig","title":"<code>LoadConfig(config_path: Union[CloudPath, Path], config_overrides: Optional[Config] = None, config_type: Optional[Literal['json', 'toml', 'yaml']] = None, config_validators: Optional[Dict[str, BaseModel]] = None)</code>","text":"<p>Class for loading and storing a configuration file.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The loaded config stored as a dictionary.</p> <code>config_dir</code> <p>The logical parent directory of loaded <code>config_path</code>.</p> <code>config_file</code> <p>The file name of the loaded <code>config_path</code>.</p> <code>config_original</code> <p>The configuration dictionary as initially loaded, prior to applying any overrides or validation.</p> <code>config_overrides</code> <p>The configuration override dictionary, if provided.</p> <code>config_path</code> <p>The path of the loaded config file.</p> <code>config_type</code> <p>The file type of the loaded config file.</p> <code>config_validators</code> <p>The validators used to validate the loaded config, if provided.</p> <code>**attrs</code> <p>Every top level key in the loaded config is also set as an attribute to allow simpler access to each config section.</p> <p>Init method.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[CloudPath, Path]</code> <p>The path of the config file to be loaded.</p> required <code>config_overrides</code> <code>Optional[Config]</code> <p>A dictionary containing a subset of the keys and values of the config file that is initially loaded, by default None. If values are provided that are not in the initial config then a ConfigError is raised.</p> <code>None</code> <code>optional</code> <code>Optional[Config]</code> <p>A dictionary containing a subset of the keys and values of the config file that is initially loaded, by default None. If values are provided that are not in the initial config then a ConfigError is raised.</p> <code>None</code> <code>config_type</code> <code>Optional[Literal['json', 'toml', 'yaml']]</code> <p>The file type of the config file being loaded, by default None. If not specified then this is inferred from the <code>config_path</code>.</p> <code>None</code> <code>optional</code> <code>Optional[Literal['json', 'toml', 'yaml']]</code> <p>The file type of the config file being loaded, by default None. If not specified then this is inferred from the <code>config_path</code>.</p> <code>None</code> <code>config_validators</code> <code>Optional[Dict[str, BaseModel]]</code> <p>A dictionary made up of key, value pairs where the keys refer to the top level sections of the loaded config, and the values are a pydantic validation class for the section, by default None. If only some of the keys are specified with validators, warnings are raised to alert that they have not been validated.</p> <code>None</code> <code>optional</code> <code>Optional[Dict[str, BaseModel]]</code> <p>A dictionary made up of key, value pairs where the keys refer to the top level sections of the loaded config, and the values are a pydantic validation class for the section, by default None. If only some of the keys are specified with validators, warnings are raised to alert that they have not been validated.</p> <code>None</code>"},{"location":"reference/#rdsa_utils.io.input","title":"<code>rdsa_utils.io.input</code>","text":"<p>Module containing generic input functionality code.</p>"},{"location":"reference/#rdsa_utils.io.input.parse_json","title":"<code>parse_json(data: str) -&gt; Config</code>","text":"<p>Parse JSON formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard JSON-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p> <p>Raises:</p> Type Description <code>JSONDecodeError</code> <p>If the string format of config_overrides cannot be decoded by json.loads (i.e. converted to a dictionary).</p>"},{"location":"reference/#rdsa_utils.io.input.parse_toml","title":"<code>parse_toml(data: str) -&gt; Config</code>","text":"<p>Parse TOML formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard TOML-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p>"},{"location":"reference/#rdsa_utils.io.input.parse_yaml","title":"<code>parse_yaml(data: str) -&gt; Config</code>","text":"<p>Parse YAML formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard YAML-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p>"},{"location":"reference/#rdsa_utils.io.input.read_file","title":"<code>read_file(file: Union[CloudPath, Path]) -&gt; str</code>","text":"<p>Load contents of specified file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[CloudPath, Path]</code> <p>The absolute file path of the file to be read.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The contents of the provided file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided file does not exist.</p>"},{"location":"reference/#rdsa_utils.io.output","title":"<code>rdsa_utils.io.output</code>","text":"<p>Module containing generic output functionality code.</p>"},{"location":"reference/#rdsa_utils.io.output.zip_folder","title":"<code>zip_folder(source_dir: str, output_filename: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Zip the contents of the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>str</code> <p>The directory whose contents are to be zipped.</p> required <code>output_filename</code> <code>str</code> <p>The output zip file name. It must end with '.zip'.</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite the existing zip file if it exists. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the directory was zipped successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zip_folder('/path/to/source_dir', 'output.zip', overwrite=True)\nTrue\n</code></pre>"},{"location":"reference/#methods","title":"Methods","text":""},{"location":"reference/#rdsa_utils.methods.averaging_methods","title":"<code>rdsa_utils.methods.averaging_methods</code>","text":"<p>Weighted and unweighted averaging functions.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.get_weight_shares","title":"<code>get_weight_shares(weights: str, levels: Optional[Union[str, Sequence[str]]] = None) -&gt; SparkCol</code>","text":"<p>Divide weights by sum of weights for each group.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.unweighted_arithmetic_average","title":"<code>unweighted_arithmetic_average(val: str) -&gt; SparkCol</code>","text":"<p>Calculate the unweighted arithmetic average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.unweighted_geometric_average","title":"<code>unweighted_geometric_average(val: str) -&gt; SparkCol</code>","text":"<p>Calculate the unweighted geometric average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.weighted_arithmetic_average","title":"<code>weighted_arithmetic_average(val: str, weight: str) -&gt; SparkCol</code>","text":"<p>Calculate the weighted arithmetic average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.weighted_geometric_average","title":"<code>weighted_geometric_average(val: str, weight: str) -&gt; SparkCol</code>","text":"<p>Calculate the weighted geometric average.</p>"},{"location":"guides/cdp_aws_resource_estimator/","title":"Estimating AWS Costs from PySpark Event Logs in a Managed Cloudera Environment","text":"<p>This report details a methodology for estimating AWS EC2 costs using PySpark event logs in a managed Cloudera environment that leverages Kubernetes orchestration.</p> <p>It explains the challenges of cost estimation in a shared infrastructure scenario, outlines the assumptions made for a simplified estimate, and discusses the implications of the \u201cnoisy neighbour\u201d problem.</p>"},{"location":"guides/cdp_aws_resource_estimator/#1-managed-environment-on-cloudera-with-aws-backend","title":"1. Managed Environment on Cloudera with AWS Backend","text":""},{"location":"guides/cdp_aws_resource_estimator/#what-is-a-managed-environment","title":"What Is a Managed Environment?","text":"<p>In a managed environment like Cloudera deployed on AWS:</p> <ul> <li> <p>Automated Provisioning and Management: The platform handles provisioning,   configuration, scaling, and maintenance of the infrastructure.   Users do not manually select individual EC2 instances.</p> </li> <li> <p>Kubernetes-Based Orchestration: Spark applications are run as sets of pods   (driver and executors) managed by Kubernetes.   When you submit a PySpark pipeline, Cloudera automatically provisions   the necessary pods, and the Kubernetes scheduler assigns these pods to underlying   EC2 instances based on resource requirements.</p> </li> <li> <p>Shared Infrastructure: Although each pipeline (or mini-cluster) is logically isolated,   multiple pipelines can share the same pool of EC2 instances. The underlying nodes host   pods from different users, leading to efficient resource usage. AWS bills based on   the total duration the EC2 instances are active, regardless of how many pipelines   share those resources.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#opposite-a-self-managed-environment","title":"Opposite: A Self-Managed Environment","text":"<p>In a self-managed environment, you manually launch and configure EC2 instances, set up Spark, and handle scaling and maintenance. This approach offers granular control but increases operational overhead and risks misconfiguration.</p> <p>Moreover, in self-managed clusters, you might be able to tightly monitor resource usage per pipeline; however, cost optimisation becomes more complex when handling varied workloads.</p>"},{"location":"guides/cdp_aws_resource_estimator/#2-the-noisy-neighbour-problem-in-shared-kubernetes-environments","title":"2. The \u201cNoisy Neighbour\u201d Problem in Shared Kubernetes Environments","text":"<p>In a Kubernetes-managed Cloudera environment:</p> <ul> <li> <p>Resource Sharing: Multiple users\u2019 pipelines run concurrently on the same   underlying EC2 instances. Although each pipeline is isolated at the pod level,   they share the same hardware resources.</p> </li> <li> <p>Noisy Neighbours: One pipeline (or a \u201cnoisy neighbour\u201d) that consumes an unusually   high amount of CPU or memory may impact the performance of other pipelines sharing   the same node. However, the resource scheduler in Kubernetes generally mitigates   this by enforcing resource limits and priorities.</p> </li> <li> <p>Billing Implications: Since AWS charges are based on the total duration that   EC2 instances are active (not per pod), the cost estimation becomes challenging   when resources are shared. It is hard to assign a precise cost to an individual   pipeline when multiple pipelines run on the same node.</p> </li> <li> <p>Simplified Assumption for Estimation: For ease of estimation, we assume a   scenario where each pipeline spins up its own dedicated mini-cluster   (i.e., a set of pods that run exclusively on their own dedicated EC2 instance).   This assumption simplifies cost calculation by attributing the entire cost of an   EC2 instance to one pipeline, even though, in reality, the infrastructure is   shared among many users.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#3-methodology-for-cost-estimation","title":"3. Methodology for Cost Estimation","text":""},{"location":"guides/cdp_aws_resource_estimator/#session-duration-as-a-proxy-for-billed-time","title":"Session Duration as a Proxy for Billed Time.","text":"<ul> <li> <p>Extraction Method: By parsing the PySpark event logs, we extract the session\u2019s   start and end times. Events such as <code>SparkListenerApplicationStart</code>   and <code>SparkListenerApplicationEnd</code> mark these boundaries.</p> </li> <li> <p>Assumption: The total session duration reflects the time during which the cluster   (or the pods on Kubernetes) is provisioned. AWS bills for the full duration that   the EC2 instances are active, including idle periods.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#resource-configuration-and-mapping-to-ec2-billing","title":"Resource Configuration and Mapping to EC2 Billing","text":"<p>Key Spark configuration parameters are extracted:</p> <ul> <li><code>spark.dynamicAllocation.maxExecutors</code></li> <li><code>spark.executor.cores</code></li> <li><code>spark.executor.memory</code></li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#calculation","title":"Calculation","text":"<p>The maximum resources available for a pipeline are approximated by multiplying the number of executors by the per-executor cores (which roughly map to vCPUs) and memory.</p> <ul> <li>Simplified Assumption for Estimation: For cost estimation, we assume that each pipeline\u2019s mini-cluster runs on dedicated EC2 instances. This simplifies the model by treating the total vCPUs (or cores) and memory allocated to the pipeline as if they were provisioned on isolated instances. Although, in a shared Kubernetes environment, multiple pipelines share the underlying hardware, this assumption provides a conservative upper-bound estimate.</li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#why-this-estimate-is-a-reasonable-rough-guess","title":"Why This Estimate Is a Reasonable Rough Guess","text":"<ul> <li> <p>Billing Alignment: AWS bills based on the active time of EC2 instances. Using the   session start and end times captures the full duration during which resources   are provisioned, reflecting what you would be billed.</p> </li> <li> <p>Upper-Bound Estimate: Including idle periods provides an upper-bound cost estimate,   which is practical for budgeting and cost management discussions with management.</p> </li> <li> <p>Simplification in a Complex Environment: Aggregating fine-grained, task-level metrics   in a dynamic, multi-tenant environment is challenging due to overlapping tasks,   dynamic scaling, and shared resource usage. The session-based approach avoids   these pitfalls by focusing on the overall provisioning time.</p> </li> <li> <p>Assumed Dedicated Instance Model: While multiple users typically share EC2 instances,   the assumption that each pipeline gets its own dedicated instance simplifies   cost estimation. This assumption is particularly useful for explaining the   methodology to management, even though it might not precisely mirror the   actual shared infrastructure setup.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#4-visualising-the-architecture","title":"4. Visualising the Architecture","text":""},{"location":"guides/cdp_aws_resource_estimator/#41-cloudera-managed-environment-with-kubernetes-orchestration","title":"4.1. Cloudera-Managed Environment with Kubernetes Orchestration","text":"<p>Below is a Mermaid diagram that illustrates the logical flow of pipelines in a Cloudera-managed environment with Kubernetes orchestration:</p> <pre><code>flowchart TD\n    subgraph \"Pipeline 1 (User 1)\"\n        A1[PySpark Job Submission]\n        D1[Spark Driver Pod]\n        E1[Spark Executor Pods]\n        A1 --&gt; D1\n        A1 --&gt; E1\n    end\n\n    subgraph \"Pipeline 2 (User 2)\"\n        A2[PySpark Job Submission]\n        D2[Spark Driver Pod]\n        E2[Spark Executor Pods]\n        A2 --&gt; D2\n        A2 --&gt; E2\n    end\n\n    B[Cloudera Managed Environment]\n    C[Kubernetes Orchestration]\n    F[AWS EC2 Instances]\n\n    A1 --&gt; B\n    A2 --&gt; B\n    B --&gt; C\n    C --&gt; D1\n    C --&gt; E1\n    C --&gt; D2\n    C --&gt; E2\n    D1 --&gt; F\n    E1 --&gt; F\n    D2 --&gt; F\n    E2 --&gt; F\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#diagram-explanation","title":"Diagram Explanation:","text":"<ul> <li> <p>Pipeline Isolation: Each user\u2019s pipeline is represented as an   isolated mini-cluster with its own driver and executor pods.</p> </li> <li> <p>Shared Management &amp; Orchestration: The Cloudera managed environment,   together with Kubernetes, abstracts the underlying resource scheduling and management.</p> </li> <li> <p>Underlying Infrastructure: All pods are deployed on AWS EC2 instances.   Although each pipeline is logically isolated, they share the same pool of   EC2 instances, which may lead to \u201cnoisy neighbour\u201d challenges.</p> </li> <li> <p>Simplified Estimation Assumption: For the purposes of cost estimation,   we assume that each pipeline\u2019s pods run on dedicated EC2 instances\u2014even   though in reality, the infrastructure is shared. This simplifies mapping   session duration and resource configuration directly to AWS billing.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#42-diagram-comparison-dedicated-vs-shared-ec2-approaches","title":"4.2. Diagram Comparison: Dedicated vs. Shared EC2 Approaches","text":"<p>Below are two diagrams summarising alternative deployment approaches and their implications for cost estimation.</p>"},{"location":"guides/cdp_aws_resource_estimator/#dedicated-ec2-per-job","title":"Dedicated EC2 per Job","text":"<pre><code>flowchart TD\n    subgraph \"Pipeline (User Job)\"\n        A[PySpark Job Submission]\n        D[Spark Driver Pod]\n        E[Spark Executor Pods]\n        A --&gt; D\n        A --&gt; E\n    end\n\n    B[Cloudera Managed Environment]\n    C[Kubernetes Orchestration]\n    F[Dedicated EC2 Instance]\n\n    A --&gt; B\n    B --&gt; C\n    C --&gt; D\n    C --&gt; E\n    D --&gt; F\n    E --&gt; F\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#explanation","title":"Explanation:","text":"<p>In this model, a job\u2019s pods run exclusively on a dedicated EC2 instance. This approach makes it straightforward to map the allocated resources (e.g., total memory and vCPUs) to a specific instance\u2019s hourly cost, resulting in an easy-to-calculate billing estimate.</p>"},{"location":"guides/cdp_aws_resource_estimator/#shared-ec2-instances","title":"Shared EC2 Instances","text":"<pre><code>flowchart TD\n    subgraph \"Pipeline 1 (User 1)\"\n        A1[PySpark Job Submission]\n        D1[Spark Driver Pod]\n        E1[Spark Executor Pods]\n        A1 --&gt; D1\n        A1 --&gt; E1\n    end\n\n    subgraph \"Pipeline 2 (User 2)\"\n        A2[PySpark Job Submission]\n        D2[Spark Driver Pod]\n        E2[Spark Executor Pods]\n        A2 --&gt; D2\n        A2 --&gt; E2\n    end\n\n    B[Cloudera Managed Environment]\n    C[Kubernetes Orchestration]\n    F[AWS EC2 Instances -- Shared Pool]\n\n    A1 --&gt; B\n    A2 --&gt; B\n    B --&gt; C\n    C --&gt; D1\n    C --&gt; E1\n    C --&gt; D2\n    C --&gt; E2\n    D1 --&gt; F\n    E1 --&gt; F\n    D2 --&gt; F\n    E2 --&gt; F\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#explanation_1","title":"Explanation:","text":"<p>In this scenario, multiple pipelines share the same pool of EC2 instances. Although this shared model optimises resource usage, it complicates cost estimation since billing is aggregated across many users. Isolating the cost impact for a single job becomes challenging due to dynamic resource allocation, overlapping workloads, and the \u201cnoisy neighbour\u201d effect.</p>"},{"location":"guides/cdp_aws_resource_estimator/#43-conclusion","title":"4.3. Conclusion","text":"<ul> <li> <p>Dedicated Instances: A dedicated EC2 per job simplifies cost estimation.   The job\u2019s resource allocation is directly tied to a specific instance\u2019s pricing,   allowing for a clear calculation of the billing cost based on session duration   and configuration.</p> </li> <li> <p>Shared Architecture: While shared EC2 instances lead to higher overall   resource utilisation and efficiency, they complicate the process of accurately   attributing costs to individual jobs. In such environments, cost estimates   must account for the effects of resource sharing, dynamic scaling, and   potential performance interference from other pipelines.</p> </li> </ul>"},{"location":"guides/cdp_aws_resource_estimator/#conclusion","title":"Conclusion","text":"<p>Estimating AWS costs for PySpark pipelines in a Cloudera-managed, Kubernetes-orchestrated environment requires understanding both the shared nature of the infrastructure and the billing model of AWS. By:</p> <ul> <li> <p>Using overall session duration (from log events) as a proxy for resource   provisioning time,</p> </li> <li> <p>Extracting maximum resource configurations from Spark properties,</p> </li> <li> <p>Assuming a simplified model where each pipeline is treated as if it were   running on dedicated EC2 instances.</p> </li> </ul> <p>we arrive at a robust rough estimate that serves as an upper-bound cost approximation. This method, while simplified, captures the essential dynamics of a shared, managed environment and provides a clear, explainable basis for cost estimation discussions with management.</p>"},{"location":"guides/cdp_aws_resource_estimator/#how-to-use-rdsa-utils-for-pyspark-cost-estimation","title":"How to Use <code>rdsa-utils</code> for PySpark Cost Estimation","text":"<p>This section explains how to use the <code>rdsa-utils</code> library to estimate AWS EC2 costs from PySpark event logs in a Cloudera-managed environment.</p> <p>The steps include configuring PySpark to generate event logs, loading and processing log data, estimating costs, and generating reports.</p>"},{"location":"guides/cdp_aws_resource_estimator/#1-configuring-pyspark-for-event-logging","title":"1. Configuring PySpark for Event Logging","text":"<p>Before running PySpark jobs, add the following configurations to your PySpark setup to enable event logging:</p> <pre><code>.config(\"spark.eventLog.enabled\", \"true\")\n.config(\"spark.eventLog.dir\", \"s3a://&lt;bucket-name&gt;/&lt;folder&gt;/&lt;log_dir&gt;/\")\n</code></pre> <p>This ensures that Spark event logs are stored in an S3 bucket for later analysis.</p>"},{"location":"guides/cdp_aws_resource_estimator/#2-required-dependencies-and-setup","title":"2. Required Dependencies and Setup","text":"<p>Ensure the necessary modules from <code>rdsa-utils</code> are imported:</p> <pre><code>import logging\nimport boto3\nimport raz_client\n\nfrom rdsa_utils.cdp.helpers.s3_utils import list_files, load_json\nfrom rdsa_utils.helpers.pyspark_log_parser.ec2_pricing import calculate_pipeline_cost\nfrom rdsa_utils.helpers.pyspark_log_parser.parser import (\n    find_pyspark_log_files,\n    parse_pyspark_logs,\n    process_pyspark_logs,\n    filter_and_sort_logs_by_app_name,\n    logs_to_dataframe,\n)\nfrom rdsa_utils.helpers.pyspark_log_parser.report import generate_report\n</code></pre> <p>Set up logging and configure AWS S3 client with Ranger RAZ authentication:</p> <pre><code>logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.INFO)\n\nconfig = {\n    \"ssl_file\": \"&lt;add_ssl_file/path/here&gt;\",\n    \"s3_bucket\": \"&lt;add_s3_bucket_name_here&gt;\",\n}\n\nclient = boto3.client(\"s3\")\nraz_client.configure_ranger_raz(client, ssl_file=config[\"ssl_file\"])\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#3-parsing-a-single-pyspark-log-file","title":"3. Parsing a Single PySpark Log File","text":"<ol> <li>Load event log data from an S3 bucket.</li> <li>Parse the log data to extract relevant execution metrics.</li> <li>Estimate the cost based on extracted metrics.</li> </ol> <pre><code>log_data = load_json(\n    client,\n    config[\"s3_bucket\"],\n    \"&lt;add_s3_folder/path/here&gt;\",\n    multi_line=True,\n)\n\nmetrics_dict = parse_pyspark_logs(log_data)\ncost_analysis = calculate_pipeline_cost(metrics_dict, fetch_data=False)\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#4-processing-multiple-pyspark-log-files-at-once","title":"4. Processing Multiple PySpark Log Files at Once","text":"<p>This is the recommended approach for real-world usage when analysing multiple pipeline runs. It automates log retrieval and processing for multiple jobs:</p> <pre><code>user_folder = \"&lt;add_s3_folder/path/here&gt;\"\nlist_of_logs = process_pyspark_logs(client, config[\"s3_bucket\"], user_folder)\nfiltered_sorted_logs = filter_and_sort_logs_by_app_name(list_of_logs, \"&lt;app_name&gt;\")\n</code></pre> <p>Note: <code>&lt;app_name&gt;</code> should match the <code>spark.app.name</code> configuration set in the PySpark job.</p>"},{"location":"guides/cdp_aws_resource_estimator/#5-generating-an-html-report","title":"5. Generating an HTML Report","text":"<p>To create a human-readable report summarising cost estimates and execution details:</p> <pre><code>generate_report(filtered_sorted_logs, \"/home/cdsw/pyspark_log_report_output.html\")\n</code></pre>"},{"location":"guides/cdp_aws_resource_estimator/#6-converting-logs-to-a-pandas-dataframe","title":"6. Converting Logs to a Pandas DataFrame","text":"<p>For further analysis and visualisation, convert parsed logs to a Pandas DataFrame:</p> <pre><code>df = logs_to_dataframe(filtered_sorted_logs)\nprint(df.head())\n</code></pre>"}]}