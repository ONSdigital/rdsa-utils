{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"rdsa-utils","text":"<p>This site contains the project documentation for <code>rdsa-utils</code>, a suite of pyspark, pandas, and general pipeline utils for Reproducible Data Science and Analysis (RDSA) projects.</p>"},{"location":"#table-of-contents","title":"Table Of Contents","text":"<ol> <li>API Reference</li> <li>Contribution Guide</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>The following prerequisites are required for <code>rdsa-utils</code>:</p> <ul> <li>Python 3.8 or higher</li> <li>Poetry</li> </ul>"},{"location":"contribution_guide/","title":"Contribution Guide","text":"<p>We welcome contributions. To contribute, please follow these guidelines:</p>"},{"location":"contribution_guide/#pull-requests","title":"Pull Requests","text":"<ul> <li>All pull requests should be made to the <code>dev</code> branch, named <code>dev_&lt;feature_name&gt;</code>.</li> <li>Please make sure that your code passes all unit tests before submitting a pull request.</li> <li>Include unit tests with your code changes whenever possible, preferably written in pytest format.</li> <li>Make sure that all existing unit tests still pass with your code changes.</li> <li>Please ensure that your code is compliant with the project's coding style guidelines, which include:</li> <li>Writing docstrings in Scipy/numpy style format.</li> <li>Using type hints in Python functions.</li> <li>Adhering to the PEP 8 style guide for Python code.</li> <li>No wildcard imports.</li> <li>Import PySpark functions as <code>F</code>.</li> <li>Providing well-documented and easy-to-understand code, including clear variable and function names, as well as explanatory comments where necessary.</li> <li>If you are making a significant change to the codebase, please make sure to update the documentation to reflect the changes.</li> <li>If you are adding new functionality, please provide examples of how to use it in the project's documentation or in a separate README file.</li> <li>If you are fixing a bug, please include a description of the bug and how your changes address it.</li> <li>If you are adding a new dependency, please include a brief explanation of why it is necessary and what it does.</li> <li>If you are making significant changes to the project's architecture or design, please discuss your ideas with the project maintainers first to ensure they align with the project's goals and vision.</li> </ul>"},{"location":"contribution_guide/#issues","title":"Issues","text":"<p>If you find a bug or would like to request a feature, please open an issue on the project's GitHub page. When opening an issue, please provide as much detail as possible, including:</p> <ul> <li>A clear and descriptive title.</li> <li>A description of the problem you're experiencing, including steps to reproduce it.</li> <li>Any error messages or logs related to the issue.</li> <li>Your operating system and Python version (if relevant).</li> </ul> <p>Please search through the existing issues before opening a new one to avoid duplicates. If you find an existing issue that covers your problem, please add any additional information as a comment. Issues will be triaged and prioritized by the project maintainers.</p> <p>If you would like to contribute to the project by fixing an existing issue, please leave a comment on the issue to let the maintainers know that you are working on it.</p>"},{"location":"contribution_guide/#getting-started","title":"Getting Started","text":""},{"location":"contribution_guide/#installing-python","title":"Installing Python","text":"<p>Before getting started, you need to have Python 3.8 or higher installed on your system. You can download Python from the official website. Make sure to add Python to your <code>PATH</code> during the installation process.</p> <p>Alternatively, you can use Anaconda to create a Python 3.8 or higher virtual environment. Anaconda is a popular Python distribution that comes with many pre-installed scientific computing packages and tools. Here's how to create a new environment with Anaconda:</p> <ol> <li>Download and install Anaconda from the official website.</li> <li>Open the Anaconda prompt.</li> <li> <p>Create a new virtual environment with Python 3.8 or higher:</p> <p><code>conda create --name myenv python=3.8</code> 4. Activate the virtual environment:</p> <p><code>conda activate myenv</code></p> </li> </ol>"},{"location":"contribution_guide/#clone-the-repository","title":"Clone the Repository","text":"<p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/ONSdigital/rdsa-utils.git\ncd rdsa-utils\n</code></pre>"},{"location":"contribution_guide/#set-up-the-development-environment","title":"Set Up the Development Environment","text":"<p>We use a traditional <code>setup.py</code> approach for managing dependencies. To set up your development environment, first, ensure you have Python 3.8 to 3.10 installed.</p> <p>Then, to install the package in editable mode along with all development dependencies, run the following command:</p> <pre><code>pip3 install -e .[dev]\n</code></pre> <p>The <code>-e</code> (or <code>--editable</code>) option is used to install the package in a way that allows you to modify the source code and see the changes directly without having to reinstall the package. This is particularly useful for development.</p>"},{"location":"contribution_guide/#running-tests","title":"Running Tests","text":"<p>To run tests, ensure you're in the top-level directory of the project and execute:</p> <pre><code>pytest\n</code></pre> <p>This will run all the tests using the configurations set in the project.</p>"},{"location":"contribution_guide/#installing-pre-commit-hooks-in-your-development-environment","title":"Installing Pre-commit Hooks in Your Development Environment","text":"<p>Pre-commit hooks are used to automate checks and formatting before commits. Follow these steps to set them up:</p>"},{"location":"contribution_guide/#installation-steps","title":"Installation Steps","text":"<ol> <li>Install pre-commit: If you haven't already, install the pre-commit package:</li> </ol> <p><code>bash    pip install pre-commit</code></p> <ol> <li>Install pre-commit hooks: Install the hooks defined in <code>.pre-commit-config.yaml</code>:</li> </ol> <p><code>bash    pre-commit install</code></p> <p>This sets up the hooks to run automatically before each commit.</p>"},{"location":"contribution_guide/#usage","title":"Usage","text":"<p>The pre-commit hooks will automatically run on your modified files whenever you commit. To manually run all hooks on all files, use:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This can be useful for checking your codebase.</p> <p>By following these steps, your development environment for <code>rdsa-utils</code> will be ready, and you can start contributing to the project with ease.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the<code>rdsa-utils</code> codebase.</p>"},{"location":"reference/#general","title":"General","text":""},{"location":"reference/#rdsa_utils.exceptions","title":"<code>rdsa_utils.exceptions</code>","text":"<p>Common custom exceptions that can be raised in pipelines.</p> <p>The purpose of these is to provide a clearer indication of why an error is being raised over the standard builtin errors (e.g. <code>ColumnNotInDataframeError</code> vs <code>ValueError</code>).</p>"},{"location":"reference/#rdsa_utils.exceptions.ColumnNotInDataframeError","title":"<code>ColumnNotInDataframeError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom exception to raise when a column is not present in dataframe.</p>"},{"location":"reference/#rdsa_utils.exceptions.ConfigError","title":"<code>ConfigError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom exception to raise when there is an issue in a config object.</p>"},{"location":"reference/#rdsa_utils.exceptions.DataframeEmptyError","title":"<code>DataframeEmptyError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom exception to raise when a dataframe is empty.</p>"},{"location":"reference/#rdsa_utils.exceptions.PipelineError","title":"<code>PipelineError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom exception to raise when there is a generic pipeline issue.</p>"},{"location":"reference/#rdsa_utils.exceptions.TableNotFoundError","title":"<code>TableNotFoundError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Custom exception to raise when a table to be read is not found.</p>"},{"location":"reference/#rdsa_utils.logging","title":"<code>rdsa_utils.logging</code>","text":"<p>Contains the logging configuration for files and method to initialise it.</p>"},{"location":"reference/#rdsa_utils.logging.add_warning_message_to_function","title":"<code>add_warning_message_to_function(_func: Callable = None, *, message: Optional[str] = None) -&gt; Callable</code>","text":"<p>Apply decorator to log a warning message.</p> <p>If a message is passed, this decorator adds a warning log of the form function_name: message</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str]</code> <p>The message to be logged along with the function name.</p> <code>None</code>"},{"location":"reference/#rdsa_utils.logging.add_warning_message_to_function--notes","title":"Notes","text":"<p>Explainer on complex decorators (and template for decorator structure): https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread</p>"},{"location":"reference/#rdsa_utils.logging.add_warning_message_to_function--usage","title":"Usage","text":"<p>To use decorator to log a warning:</p> <pre><code>&gt;&gt;&gt; @_add_warning_message_to_function(message='here be dragons...')\n&gt;&gt;&gt; def my_func(some_args, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_output = my_func(...)\n\nWarning my_func: here be dragons...\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.init_logger_advanced","title":"<code>init_logger_advanced(log_level: int, handlers: Optional[List[logging.Handler]] = None, log_format: str = None, date_format: str = None) -&gt; None</code>","text":"<p>Instantiate a logger with provided handlers.</p> <p>This function allows the logger to be used across modules. Logs can be handled by any number of handlers, e.g., FileHandler, StreamHandler, etc., provided in the <code>handlers</code> list.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>The level of logging to be recorded. Can be defined either as the integer level or the logging. values in line with the definitions of the logging module. (see - https://docs.python.org/3/library/logging.html#levels) required <code>handlers</code> <code>Optional[List[Handler]]</code> <p>List of handler instances to be added to the logger. Each handler instance must be a subclass of <code>logging.Handler</code>. Default is an empty list, and in this case, basicConfig with <code>log_level</code>, <code>log_format</code>, and <code>date_format</code> is used.</p> <code>None</code> <code>log_format</code> <code>str</code> <p>The format of the log message. If not provided, a default format <code>'%(asctime)s %(levelname)s %(name)s: %(message)s'</code> is used.</p> <code>None</code> <code>date_format</code> <code>str</code> <p>The format of the date in the log message. If not provided, a default format <code>'%Y-%m-%d %H:%M:%S'</code> is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>The logger created by this function is available in any other modules by using <code>logging.getLogger(__name__)</code> at the global scope level in a module (i.e., below imports, not in a function).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any item in the <code>handlers</code> list is not an instance of <code>logging.Handler</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; file_handler = logging.FileHandler('logfile.log')\n&gt;&gt;&gt; rich_handler = RichHandler()\n&gt;&gt;&gt; init_logger_advanced(\n...     logging.DEBUG,\n...     [file_handler, rich_handler],\n...     \"%(levelname)s: %(message)s\",\n...     \"%H:%M:%S\"\n... )\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.init_logger_basic","title":"<code>init_logger_basic(log_level: int) -&gt; None</code>","text":"<p>Instantiate a basic logger object to be used across modules.</p> <p>By using this function to instantiate the logger, you also have access to <code>logger.dev</code> for log_level=15, as this is defined in the same module scope as this function.</p> <p>Parameters:</p> Name Type Description Default <code>log_level</code> <code>int</code> <p>The level of logging to be recorded. Can be defined either as the integer level or the logging. values in line with the definitions of the logging module (see - https://docs.python.org/3/library/logging.html#levels) required <p>Returns:</p> Type Description <code>None</code> <p>The logger created by this function is available in any other modules by using <code>logger = logging.getLogger(__name__)</code> at the global scope level in a module (i.e. below imports, not in a function).</p>"},{"location":"reference/#rdsa_utils.logging.log_dev","title":"<code>log_dev(self, message, *args, **kwargs)</code>","text":"<p>Create a custom log level between INFO and DEBUG named DEV.</p> <p>This is lifted from: https://stackoverflow.com/a/13638084</p>"},{"location":"reference/#rdsa_utils.logging.log_rows_in_spark_df","title":"<code>log_rows_in_spark_df(func: Callable) -&gt; Callable</code>","text":"<p>Apply decorator to log dataframe row count before and after a function.</p> <p>Requires that the function being decorated has a parameter called <code>df</code> and that the function is called with <code>df</code> being a keyword argument (e.g. <code>df=df</code>). If not the decorator will report back that it could not count the number of rows of the dataframe before running the decorated function.</p>"},{"location":"reference/#rdsa_utils.logging.log_rows_in_spark_df--usage","title":"Usage","text":"<pre><code>@log_rows_in_spark_df\ndef my_func_that_changes_no_rows(some_args, df, some_other_args):\n   ...\n   returns final_df\n\nsome_df = my_func_that_changes_no_rows(\n    some_args='hello',\n    df=input_df,\n    some_other_args='world'\n)\n\n&gt;&gt;&gt; Rows in dataframe before my_func_that_changes_no_rows : 12345\n&gt;&gt;&gt; Rows in dataframe after my_func_that_changes_no_rows  : 6789\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.log_rows_in_spark_df--warning","title":"Warning:","text":"<p><code>.count()</code> is an expensive spark operation to perform. Overuse of this decorator can be detrimental to performance. This decorator will cache the input dataframe prior to running the count and decorated function, as well as persisting the output dataframe prior to counting. The input dataframe is also unpersisted from memory prior to the decorator completing.</p>"},{"location":"reference/#rdsa_utils.logging.log_spark_df_schema","title":"<code>log_spark_df_schema(_func: Callable = None, *, log_schema_on_input: bool = True) -&gt; Callable</code>","text":"<p>Apply decorator to log dataframe schema before and after a function.</p> <p>If you use the `df.printSchema() method directly in a print/log statement the code is processed and printed regardless of logging leve. Instead you need to capture the output and pass this to the logger. See explanaition here - https://stackoverflow.com/a/59935109</p> <p>Requires that the function being decorated has a parameter called <code>df</code> and that the function is called with <code>df</code> being a keyword argument (e.g. <code>df=df</code>). If not the decorator will report back that it could not count the number of rows of the dataframe before running the decorated function.</p> <p>Parameters:</p> Name Type Description Default <code>log_schema_on_input</code> <code>bool</code> <p>If set to false, then no schema is attempted to be printed for the decorated function on input. This is useful for instance where function has no df input but does return one (such as when reading a table).</p> <code>True</code>"},{"location":"reference/#rdsa_utils.logging.log_spark_df_schema--notes","title":"Notes","text":"<p>Explainer on complex decorators (and template for decorator structure): https://realpython.com/primer-on-python-decorators/#both-please-but-never-mind-the-bread</p>"},{"location":"reference/#rdsa_utils.logging.log_spark_df_schema--usage","title":"Usage","text":"<p>To use decorator to record input and output schema:</p> <pre><code>&gt;&gt;&gt; @log_spark_df_schema\n&gt;&gt;&gt; def my_func_that_changes_some_columns(some_args, df, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;    returns final_df\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_df = my_func_that_changes_some_columns(\n&gt;&gt;&gt;     some_args='hello',\n&gt;&gt;&gt;     df=input_df,\n&gt;&gt;&gt;     some_other_args='world'\n&gt;&gt;&gt; )\n\nSchema of dataframe before my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n</code></pre> <p>To use decorator to record output schema only:</p> <pre><code>&gt;&gt;&gt; @log_spark_df_schema(log_schema_on_input=False)\n&gt;&gt;&gt; def my_func_that_changes_some_columns(some_args, df, some_other_args):\n&gt;&gt;&gt;    ...\n&gt;&gt;&gt;    returns final_df\n&gt;&gt;&gt;\n&gt;&gt;&gt; some_df = my_func_that_changes_some_columns(\n&gt;&gt;&gt;     some_args='hello',\n&gt;&gt;&gt;     df=input_df,\n&gt;&gt;&gt;     some_other_args='world'\n&gt;&gt;&gt; )\n\nNot printing schema of dataframe before my_func_that_changes_some_columns\n\nSchema of dataframe after my_func_that_changes_some_columns:\nroot\n|-- price: double (nullable = true)\n|-- quantity: long (nullable = true)\n|-- expenditure: double (nullable = true)\n</code></pre>"},{"location":"reference/#rdsa_utils.logging.print_full_table_and_raise_error","title":"<code>print_full_table_and_raise_error(df: pd.DataFrame, message: str, stop_pipeline: bool = False, show_records: bool = False) -&gt; None</code>","text":"<p>Output dataframe records to logger.</p> <p>The purpose of this function is to enable a user to output a message to the logger with the added functionality of stopping the pipeline and showing dataframe records in a table format. It may be used for instance if a user wants to check the records in a dataframe when it expected to be empty.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to display records from.</p> required <code>message</code> <code>str</code> <p>The message to output to the logger.</p> required <code>stop_pipeline</code> <code>bool</code> <p>Switch for the user to stop the pipeline and raise an error.</p> <code>False</code> <code>show_records</code> <code>bool</code> <p>Switch to show records in a dataframe.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays message to user however nothing is returned from function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises error and stops pipeline if switch applied.</p>"},{"location":"reference/#rdsa_utils.logging.timer_args","title":"<code>timer_args(name: str, logger: Optional[Callable[[str], None]] = logger.info) -&gt; Dict[str, str]</code>","text":"<p>Initialise timer args workaround for 'text' args in codetiming package.</p> <p>Works with codetiming==1.4.0</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the specific timer log.</p> required <code>logger</code> <code>Optional[Callable[[str], None]]</code> <p>Optional logger function that can accept a string argument.</p> <code>info</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of arguments to pass to specifc codetiming package Timer.</p>"},{"location":"reference/#rdsa_utils.test_utils","title":"<code>rdsa_utils.test_utils</code>","text":"<p>Functions and fixtures used with test suites.</p>"},{"location":"reference/#rdsa_utils.test_utils.Case","title":"<code>Case(label: Optional[str] = None, marks: Optional[MarkDecorator] = None, **kwargs)</code>","text":"<p>Container for a test case, with optional test ID.</p> <p>The Case class is to be used in conjunction with <code>parameterize_cases</code>.</p> <p>Attributes:</p> Name Type Description <code>    label</code> <pre><code>Optional test ID. Will be displayed for each test when\nrunning `pytest -v`.\n</code></pre> <p>marks     Optional pytest marks to denote any tests to skip etc. kwargs     Parameters used for the test cases.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Case(label=\"some test name\", foo=10, bar=\"some value\")\n\n&gt;&gt;&gt; Case(\n&gt;&gt;&gt;     label=\"some test name\",\n&gt;&gt;&gt;     marks=pytest.mark.skip(reason='not implemented'),\n&gt;&gt;&gt;     foo=10,\n&gt;&gt;&gt;     bar=\"some value\"\n&gt;&gt;&gt; )\n</code></pre>"},{"location":"reference/#rdsa_utils.test_utils.Case--see-also","title":"See Also","text":"<p>Modified from https://github.com/ckp95/pytest-parametrize-cases to allow pytest mark usage.</p> <p>Initialise objects.</p>"},{"location":"reference/#rdsa_utils.test_utils.Case.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Return string.</p>"},{"location":"reference/#rdsa_utils.test_utils.create_dataframe","title":"<code>create_dataframe(data: List[Tuple[str]], **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Create pandas df from tuple data with a header.</p>"},{"location":"reference/#rdsa_utils.test_utils.create_spark_df","title":"<code>create_spark_df(spark_session)</code>","text":"<p>Create Spark DataFrame from tuple data with first row as schema.</p>"},{"location":"reference/#rdsa_utils.test_utils.create_spark_df--example","title":"Example:","text":"<p>create_spark_df([     ('column1', 'column2', 'column3'),     ('aaaa', 1, 1.1) ])</p> <p>Can specify the schema alongside the column names: create_spark_df([     ('column1 STRING, column2 INT, column3 DOUBLE'),     ('aaaa', 1, 1.1) ])</p>"},{"location":"reference/#rdsa_utils.test_utils.parametrize_cases","title":"<code>parametrize_cases(*cases: Case)</code>","text":"<p>More user friendly parameterize cases testing.</p> <p>Utilise as a decorator on top of test function.</p> <p>Examples:</p> <pre><code>@parameterize_cases(\n    Case(\n        label=\"some test name\",\n        foo=10,\n        bar=\"some value\"\n    ),\n    Case(\n        label=\"some test name #2\",\n        foo=20,\n        bar=\"some other value\"\n    ),\n)\ndef test(foo, bar):\n    ...\n</code></pre>"},{"location":"reference/#rdsa_utils.test_utils.parametrize_cases--see-also","title":"See Also","text":"<p>Source: https://github.com/ckp95/pytest-parametrize-cases</p>"},{"location":"reference/#rdsa_utils.test_utils.spark_session","title":"<code>spark_session()</code>","text":"<p>Set up spark session fixture.</p>"},{"location":"reference/#rdsa_utils.test_utils.suppress_py4j_logging","title":"<code>suppress_py4j_logging()</code>","text":"<p>Suppress spark logging.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_date","title":"<code>to_date(dt: str) -&gt; datetime.date</code>","text":"<p>Convert date string to datetime.date type.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_datetime","title":"<code>to_datetime(dt: str) -&gt; datetime.datetime</code>","text":"<p>Convert datetime string to datetime.datetime type.</p>"},{"location":"reference/#rdsa_utils.test_utils.to_spark","title":"<code>to_spark(spark_session)</code>","text":"<p>Convert pandas df to spark.</p>"},{"location":"reference/#rdsa_utils.typing","title":"<code>rdsa_utils.typing</code>","text":"<p>Contains custom types for type hinting.</p>"},{"location":"reference/#rdsa_utils.validation","title":"<code>rdsa_utils.validation</code>","text":"<p>Functions that support the use of pydantic validators.</p>"},{"location":"reference/#rdsa_utils.validation.allowed_date_format","title":"<code>allowed_date_format(date: str) -&gt; str</code>","text":"<p>Ensure that the date string can be converted to a useable datetime.</p> <p>Parameters:</p> Name Type Description Default <code>date</code> <code>str</code> <p>The specified date string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The input date.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the date is not one of the predefined allowed formats.</p>"},{"location":"reference/#rdsa_utils.validation.apply_validation","title":"<code>apply_validation(config: Mapping[str, Any], Validator: Optional[BaseModel]) -&gt; Mapping[str, Any]</code>","text":"<p>Apply validation model to config.</p> <p>If no Validator is passed, then a warning will be logged and the input config returned without validation. This mechanism is to allow the use of this function to aid in tracking config sections that are unvalidated.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Mapping[str, Any]</code> <p>The config for validating.</p> required <code>Validator</code> <code>Optional[BaseModel]</code> <p>Validator class for the config.</p> required <code>optional</code> <code>Optional[BaseModel]</code> <p>Validator class for the config.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Any]</code> <p>The input config after being passed through the validator.</p>"},{"location":"reference/#rdsa_utils.validation.list_convert_validator","title":"<code>list_convert_validator(*args, **kwargs) -&gt; Callable</code>","text":"<p>Wrapper to set kwargs for list_convert validator.</p>"},{"location":"reference/#cdsw","title":"CDSW","text":""},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils","title":"<code>rdsa_utils.cdsw.helpers.hdfs_utils</code>","text":"<p>Utility functions for interacting with HDFS.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.change_permissions","title":"<code>change_permissions(path: str, permission: str, recursive: bool = False) -&gt; bool</code>","text":"<p>Change directory and file permissions in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file or directory in HDFS.</p> required <code>permission</code> <code>str</code> <p>The permission to be set, e.g., 'go+rwx' or '777'.</p> required <code>recursive</code> <code>bool</code> <p>If True, changes permissions for all subdirectories and files within a directory.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.copy","title":"<code>copy(from_path: str, to_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Copy a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The source path of the file in HDFS.</p> required <code>to_path</code> <code>str</code> <p>The target path of the file in HDFS.</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing file at the target path will be overwritten, default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.copy_local_to_hdfs","title":"<code>copy_local_to_hdfs(from_path: str, to_path: str) -&gt; bool</code>","text":"<p>Copy a local file to HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the local file.</p> required <code>to_path</code> <code>str</code> <p>The path to the HDFS directory where the file will be copied.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.create_dir","title":"<code>create_dir(path: str) -&gt; bool</code>","text":"<p>Create a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path where the directory should be created.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory created), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.create_txt_from_string","title":"<code>create_txt_from_string(path: str, string_to_write: str, replace: Optional[bool] = False) -&gt; None</code>","text":"<p>Create and populate a text file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the new file to be created, for example, '/some/directory/newfile.txt'.</p> required <code>string_to_write</code> <code>str</code> <p>The string that will populate the new text file.</p> required <code>replace</code> <code>Optional[bool]</code> <p>Flag determining whether an existing file should be replaced. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>This function doesn't return anything; it's used for its side effect of creating a text file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>replace</code> is False and the file already exists.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.delete_dir","title":"<code>delete_dir(path: str) -&gt; bool</code>","text":"<p>Delete a directory from HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path to the directory to be deleted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory deleted), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.delete_file","title":"<code>delete_file(path: str) -&gt; bool</code>","text":"<p>Delete a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file in HDFS to be deleted.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file was successfully deleted (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.file_exists","title":"<code>file_exists(path: str) -&gt; bool</code>","text":"<p>Check whether a file exists in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file in HDFS to be checked for existence.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.get_date_modified","title":"<code>get_date_modified(filepath: str) -&gt; str</code>","text":"<p>Return the last modified date of a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the file in HDFS.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The date the file was last modified.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.isdir","title":"<code>isdir(path: str) -&gt; bool</code>","text":"<p>Test if a directory exists in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The HDFS path to the directory to be tested.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation is successful (directory exists), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.move_local_to_hdfs","title":"<code>move_local_to_hdfs(from_path: str, to_path: str) -&gt; bool</code>","text":"<p>Move a local file to HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the local file.</p> required <code>to_path</code> <code>str</code> <p>The path to the HDFS directory where the file will be moved.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.read_dir","title":"<code>read_dir(path: str) -&gt; List[str]</code>","text":"<p>Read the contents of a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of full paths of the items found in the directory.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.read_dir_files","title":"<code>read_dir_files(path: str) -&gt; List[str]</code>","text":"<p>Read the filenames in a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of filenames in the directory.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.read_dir_files_recursive","title":"<code>read_dir_files_recursive(path: str, return_path: bool = True) -&gt; List[str]</code>","text":"<p>Recursively reads the contents of a directory in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory in HDFS.</p> required <code>return_path</code> <code>bool</code> <p>If True, returns the full path of the files, otherwise just the filename.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of files in the directory.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.hdfs_utils.rename","title":"<code>rename(from_path: str, to_path: str, overwrite: bool = False) -&gt; bool</code>","text":"<p>Rename (i.e., move using full path) a file in HDFS.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The source path of the file in HDFS.</p> required <code>to_path</code> <code>str</code> <p>The target path of the file in HDFS.</p> required <code>overwrite</code> <code>bool</code> <p>If True, the existing file at the target path will be overwritten, default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the operation was successful (command return code 0), otherwise False.</p> <p>Raises:</p> Type Description <code>TimeoutExpired</code> <p>If the process does not complete within the default timeout.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.impala","title":"<code>rdsa_utils.cdsw.helpers.impala</code>","text":"<p>Utilities for working with Impala.</p>"},{"location":"reference/#rdsa_utils.cdsw.helpers.impala.invalidate_impala_metadata","title":"<code>invalidate_impala_metadata(table: str, impalad_address_port: str, impalad_ca_cert: str, keep_stderr: Optional[bool] = False)</code>","text":"<p>Automate the invalidation of a table's metadata using impala-shell.</p> <p>This function uses the impala-shell command with the given impalad_address_port and impalad_ca_cert, to invalidate a specified table's metadata.</p> <p>It proves useful during a data pipeline's execution after writing to an intermediate Hive table. Using Impala Query Editor in Hue, end-users often need to run \"INVALIDATE METADATA\" command to refresh a table's metadata. However, this manual step can be missed, leading to potential use of outdated metadata.</p> <p>The function automates the \"INVALIDATE METADATA\" command for a given table, ensuring up-to-date metadata for future queries. This reduces manual intervention, making outdated metadata issues less likely to occur.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Name of the table for metadata invalidation.</p> required <code>impalad_address_port</code> <code>str</code> <p>'address:port' of the impalad instance.</p> required <code>impalad_ca_cert</code> <code>str</code> <p>Path to impalad's CA certificate file.</p> required <code>keep_stderr</code> <code>Optional[bool]</code> <p>If True, will print impala-shell command's stderr output.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem'\n... )\n&gt;&gt;&gt; invalidate_impala_metadata(\n...     'my_table',\n...     'localhost:21050',\n...     '/path/to/ca_cert.pem',\n...     keep_stderr=True\n... )\n</code></pre>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog","title":"<code>rdsa_utils.cdsw.io.pipeline_runlog</code>","text":"<p>Utilities for managing a Pipeline Runlog using Hive Tables.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.add_runlog_entry","title":"<code>add_runlog_entry(spark: SparkSession, desc: str, version: str, config: Union[ConfigParser, Dict[str, str]], pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog', run_id: Optional[int] = None) -&gt; DataFrame</code>","text":"<p>Add an entry to a target runlog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session.</p> required <code>desc</code> <code>str</code> <p>Description to attach to the log entry.</p> required <code>version</code> <code>str</code> <p>Version of the pipeline.</p> required <code>config</code> <code>Union[ConfigParser, Dict[str, str]]</code> <p>Configuration object for the run.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>Pipeline name. If None, uses the spark application name.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>Target runlog table. If database not set, this should include the database.</p> <code>'pipeline_runlog'</code> <code>run_id</code> <code>Optional[int]</code> <p>Run id to use if already reserved. If not specified, a new one is generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The log entry returned as a spark dataframe.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.create_runlog_entry","title":"<code>create_runlog_entry(spark: SparkSession, run_id: int, desc: str, version: str, config: Union[ConfigParser, Dict[str, str]], pipeline: Optional[str] = None) -&gt; DataFrame</code>","text":"<p>Create an entry for the runlog.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session.</p> required <code>run_id</code> <code>int</code> <p>Entry run id.</p> required <code>desc</code> <code>str</code> <p>Description to attach to the log entry.</p> required <code>version</code> <code>str</code> <p>Version of the pipeline.</p> required <code>config</code> <code>Union[ConfigParser, Dict[str, str]]</code> <p>Configuration object for the run.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>Pipeline name. If None, derives from spark app name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The log entry returned as a spark dataframe.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.create_runlog_table","title":"<code>create_runlog_table(spark: SparkSession, database: str, tablename: Optional[str] = 'pipeline_runlog') -&gt; None</code>","text":"<p>Create runlog and _reserved_ids tables in the target database if needed.</p> <p>This function executes two SQL queries to create two tables, if they do not already exist in the target database. The first table's structure includes columns for run_id, desc, user, datetime, pipeline_name, pipeline_version, and config, while the second table includes run_id and reserved_date.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running spark session which will be used to execute SQL queries.</p> required <code>database</code> <code>str</code> <p>The name of the target database where tables will be created.</p> required <code>tablename</code> <code>Optional[str]</code> <p>The name of the main table to be created (default is \"pipeline_runlog\"). The associated _reserved_ids table will be suffixed with this name.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.appName(\"test_session\").getOrCreate()\n&gt;&gt;&gt; create_runlog_table(spark, \"test_db\", \"test_table\")\n</code></pre>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.get_last_run_id","title":"<code>get_last_run_id(spark: SparkSession, pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog') -&gt; Optional[int]</code>","text":"<p>Retrieve the last run_id, either in general or for a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running Spark session.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>If specified, the result will be for the listed pipeline only.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>The target runlog table. If the database is not set, this should include the database.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The id of the last run. Returns None if the log table is empty.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.get_penultimate_run_id","title":"<code>get_penultimate_run_id(spark: SparkSession, pipeline: Optional[str] = None, log_table: str = 'pipeline_runlog') -&gt; Optional[int]</code>","text":"<p>Retrieve penultimate run_id in general or a specific pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running Spark session.</p> required <code>pipeline</code> <code>Optional[str]</code> <p>If specified, the result will be for the listed pipeline only.</p> <code>None</code> <code>log_table</code> <code>str</code> <p>The target runlog table. If the database is not set, this should include the database.</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The id of the penultimate run. Returns None if the log table is empty or has less than two entries.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.reserve_id","title":"<code>reserve_id(spark: SparkSession, log_table: Optional[str] = 'pipeline_runlog') -&gt; int</code>","text":"<p>Reserve a run id in the reserved ids table linked to the runlog table.</p> <p>The function reads the last run id from the reserved ids table, increments it to create a new id,and writes the new id with the current timestamp to the reserved ids table.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running SparkSession instance.</p> required <code>log_table</code> <code>Optional[str]</code> <p>The name of the main pipeline runlog table associated with this reserved id table, by default \"pipeline_runlog\".</p> <code>'pipeline_runlog'</code> <p>Returns:</p> Type Description <code>int</code> <p>The new run id.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.pipeline_runlog.write_runlog_file","title":"<code>write_runlog_file(spark: SparkSession, runlog_table: str, runlog_id: int, path: str) -&gt; None</code>","text":"<p>Write metadata from runlog entry to a text file.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>A running SparkSession instance.</p> required <code>runlog_table</code> <code>str</code> <p>The name of the table containing the runlog entries.</p> required <code>runlog_id</code> <code>int</code> <p>The id of the desired entry.</p> required <code>path</code> <code>str</code> <p>The HDFS path where the file will be written.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function doesn't return anything; it's used for its side effect of creating a text file.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.input","title":"<code>rdsa_utils.cdsw.io.input</code>","text":"<p>Read inputs on CDSW.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.input.extract_database_name","title":"<code>extract_database_name(spark: SparkSession, long_table_name: str) -&gt; Tuple[str, str]</code>","text":"<p>Extract the database component and table name from a compound table name.</p> <p>This function can handle multiple scenarios:</p> <ol> <li> <p>For GCP's naming format '..',    the function will return the database and table name. <li> <p>If the name is formatted as 'db_name.table_name', the function will    extract and return the database and table names.</p> </li> <li> <p>If the long_table_name contains only the table name (e.g., 'table_name'),    the function will use the current database of the SparkSession.</p> </li> <li> <p>For any other incorrectly formatted names, the function will raise    a ValueError.</p> </li> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>long_table_name</code> <code>str</code> <p>Full name of the table, which can include the GCP project and/or database name.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>A tuple containing the name of the database and the table name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table name doesn't match any of the expected formats.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.input.get_current_database","title":"<code>get_current_database(spark: SparkSession) -&gt; str</code>","text":"<p>Retrieve the current database from the active SparkSession.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.input.load_and_validate_table","title":"<code>load_and_validate_table(spark: SparkSession, table_name: str, skip_validation: bool = False, err_msg: str = None, filter_cond: str = None) -&gt; SparkDF</code>","text":"<p>Load a table and validate if it is not empty after applying a filter.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>table_name</code> <code>str</code> <p>Name of the table to load.</p> required <code>skip_validation</code> <code>bool</code> <p>If True, skips validation step, by default False.</p> <code>False</code> <code>err_msg</code> <code>str</code> <p>Error message to return if table is empty, by default None.</p> <code>None</code> <code>filter_cond</code> <code>str</code> <p>Condition to apply to SparkDF once read, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Loaded SparkDF if validated, subject to options above.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If there's an issue accessing the table or if the table does not exist in the specified database.</p> <code>ValueError</code> <p>If the table is empty after loading, or if it becomes empty after applying a filter condition.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.output","title":"<code>rdsa_utils.cdsw.io.output</code>","text":"<p>Write outputs on CDSW.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.output.insert_df_to_hive_table","title":"<code>insert_df_to_hive_table(spark: SparkSession, df: SparkDF, table_name: str, overwrite: bool = False, fill_missing_cols: bool = False) -&gt; None</code>","text":"<p>Write the SparkDF contents to a Hive table.</p> <p>This function writes data from a SparkDF into a Hive table, allowing optional handling of missing columns. The table's column order is ensured to match that of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>df</code> <code>DataFrame</code> <p>SparkDF containing data to be written.</p> required <code>table_name</code> <code>str</code> <p>Name of the Hive table to write data into.</p> required <code>overwrite</code> <code>bool</code> <p>If True, existing data in the table will be overwritten, by default False.</p> <code>False</code> <code>fill_missing_cols</code> <code>bool</code> <p>If True, missing columns will be filled with nulls, by default False.</p> <code>False</code> <p>Raises:</p> Type Description <code>AnalysisException</code> <p>If there's an error reading the table. This can occur if the table doesn't exist or if there's no access to it.</p> <code>ValueError</code> <p>If the SparkDF schema does not match the Hive table schema and 'fill_missing_cols' is set to False.</p> <code>Exception</code> <p>For other general exceptions when writing data to the table.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.output.save_csv_to_hdfs","title":"<code>save_csv_to_hdfs(spark: SparkSession, df: SparkDF, file_name: str, file_path: str, overwrite: bool = True, coalesce_data: bool = True) -&gt; None</code>","text":"<p>Save DataFrame as CSV on HDFS, with optional coalescing.</p> <p>This function saves a PySpark DataFrame to HDFS in CSV format. Coalescing the DataFrame into a single partition is optional.</p> <p>Without coalescing, the DataFrame is saved in multiple parts, and these parts are merged into a single file. However, this does not guarantee the order of rows as in the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession instance.</p> required <code>df</code> <code>DataFrame</code> <p>PySpark DataFrame to save.</p> required <code>file_name</code> <code>str</code> <p>The name of the CSV file. The file name must include the \".csv\" extension.</p> required <code>file_path</code> <code>str</code> <p>The path in HDFS where the CSV file should be saved.</p> required <code>overwrite</code> <code>bool</code> <p>If True, any existing file with the same name in the given path will be overwritten. If False, an exception will be raised if a file with the same name already exists.</p> <code>True</code> <code>coalesce_data</code> <code>bool</code> <p>If True, coalesces the DataFrame into a single partition before saving. This preserves the order of rows but may impact performance for large DataFrames.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided file_name doesn't end with \".csv\".</p> <code>IOError</code> <p>If <code>overwrite</code> is False and the file already exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; save_to_hdfs_csv(df, \"example.csv\", \"/user/hadoop/data/\")\n</code></pre>"},{"location":"reference/#rdsa_utils.cdsw.io.output.write_and_read_hive_table","title":"<code>write_and_read_hive_table(spark: SparkSession, df: SparkDF, table_name: str, database: str, filter_id: Union[int, str], filter_col: str = 'run_id', fill_missing_cols: bool = False) -&gt; SparkDF</code>","text":"<p>Write a SparkDF to an existing Hive table and then read it back.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Active SparkSession.</p> required <code>df</code> <code>DataFrame</code> <p>The SparkDF to be written to the Hive table.</p> required <code>table_name</code> <code>str</code> <p>The name of the Hive table to write to and read from.</p> required <code>database</code> <code>str</code> <p>The Hive database name.</p> required <code>filter_id</code> <code>Union[int, str]</code> <p>The identifier to filter on when reading data back from the Hive table.</p> required <code>filter_col</code> <code>str</code> <p>The column name to use for filtering data when reading back from the Hive table, by default 'run_id'.</p> <code>'run_id'</code> <code>fill_missing_cols</code> <code>bool</code> <p>If True, missing columns in the DataFrame will be filled with nulls when writing to the Hive table, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame read from the Hive table.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified Hive table does not exist in the given database or if the provided DataFrame doesn't contain the specified filter column.</p> <code>Exception</code> <p>For general exceptions encountered during execution.</p>"},{"location":"reference/#rdsa_utils.cdsw.io.output.write_and_read_hive_table--notes","title":"Notes","text":"<p>This function assumes the Hive table already exists. The DataFrame <code>df</code> should have the same schema as the Hive table for the write to succeed.</p> <p>The function allows for more effective memory management when dealing with large PySpark DataFrames by leveraging Hive's on-disk storage.</p> <p>Predicate pushdown is used when reading the data back into a PySpark DataFrame, minimizing the memory usage and optimizing the read operation.</p> <p>As part of the design, there is always a column called filter_col in the DataFrame and Hive table to track pipeline runs.</p> <p>The Hive table contains all the runs, and we only read back the run that we just wrote to the Hive Table using the <code>filter_id</code> parameter. If no <code>filter_col</code> is specified, 'run_id' is used as default.</p>"},{"location":"reference/#gcp","title":"GCP","text":""},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils","title":"<code>rdsa_utils.gcp.helpers.gcp_utils</code>","text":"<p>General helper functions for GCP.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.get_table_columns","title":"<code>get_table_columns(table_path) -&gt; List[str]</code>","text":"<p>Return the column names for given bigquery table.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.load_config_gcp","title":"<code>load_config_gcp(config_path: str) -&gt; Tuple[Dict, Dict]</code>","text":"<p>Load the config and dev_config files to dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path of the config file in a yaml format.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict, Dict]</code> <p>The contents of the config files.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.run_bq_query","title":"<code>run_bq_query(query: str) -&gt; bigquery.QueryJob</code>","text":"<p>Run an SQL query in BigQuery.</p>"},{"location":"reference/#rdsa_utils.gcp.helpers.gcp_utils.table_exists","title":"<code>table_exists(table_path: TablePath) -&gt; bool</code>","text":"<p>Check the big query catalogue to see if a table exists.</p> <p>Returns True if a table exists. See code sample explanation here: https://cloud.google.com/bigquery/docs/samples/bigquery-table-exists#bigquery_table_exists-python</p> <p>Parameters:</p> Name Type Description Default <code>table_path</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <p>Returns:</p> Type Description <code>bool</code> <p>Returns True if table exists and False if table does not exist.</p>"},{"location":"reference/#rdsa_utils.gcp.io.inputs","title":"<code>rdsa_utils.gcp.io.inputs</code>","text":"<p>Read from BigQuery.</p>"},{"location":"reference/#rdsa_utils.gcp.io.inputs.build_sql_query","title":"<code>build_sql_query(table_path: TablePath, columns: Optional[Sequence[str]] = None, date_column: Optional[str] = None, date_range: Optional[Sequence[str]] = None, column_filter_dict: Optional[Dict[str, Sequence[str]]] = None, partition_column: Optional[str] = None, partition_type: Optional[str] = None, partition_value: Optional[Union[Tuple[str, str], str]] = None) -&gt; str</code>","text":"<p>Create SQL query to load data with the specified filter conditions.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>Spark session.</p> required <code>table_path</code> <code>TablePath</code> <p>BigQuery table path in format \"database_name.table_name\".</p> required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>The column selection. Selects all columns if None passed.</p> <code>None</code> <code>date_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter the date range on.</p> <code>None</code> <code>date_range</code> <code>Optional[Sequence[str]]</code> <p>Sequence with two values, a lower and upper value for dates to load in.</p> <code>None</code> <code>column_filter_dict</code> <code>Optional[Dict[str, Sequence[str]]]</code> <p>A dictionary containing column: [values] where the values correspond to terms in the column that are to be filtered by.</p> <code>None</code> <code>partition_column</code> <code>Optional[str]</code> <p>The name of the column that the table is partitioned by.</p> <code>None</code> <code>partition_type</code> <code>Optional[str]</code> <p>The unit of time the table is partitioned by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <code>None</code> <code>partition_value</code> <code>Optional[Union[Tuple[str, str], str]]</code> <p>The value or pair of values for filtering the partition column to.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The string containing the SQL query.</p>"},{"location":"reference/#rdsa_utils.gcp.io.inputs.read_table","title":"<code>read_table(spark: SparkSession, table_path: TablePath, columns: Optional[Sequence[str]] = None, date_column: Optional[str] = None, date_range: Optional[Sequence[str]] = None, column_filter_dict: Optional[Dict[str, Sequence[str]]] = None, run_id_column: Optional[str] = 'run_id', run_id: Optional[str] = None, flatten_struct_cols: bool = False, partition_column: Optional[str] = None, partition_type: Optional[BigQueryTimePartitions] = None, partition_value: Optional[Union[Tuple[str, str], str]] = None) -&gt; SparkDF</code>","text":"<p>Read BigQuery table given table path and column selection.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>Spark session.</p> required <code>table_path</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <code>columns</code> <code>Optional[Sequence[str]]</code> <p>The column selection. Selects all columns if None passed.</p> <code>None</code> <code>date_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter the date range on.</p> <code>None</code> <code>date_range</code> <code>Optional[Sequence[str]]</code> <p>Sequence with two values, a lower and upper value for dates to load in.</p> <code>None</code> <code>column_filter_dict</code> <code>Optional[Dict[str, Sequence[str]]]</code> <p>A dictionary containing column: [values] where the values correspond to terms in the column that are to be filtered by.</p> <code>None</code> <code>run_id_column</code> <code>Optional[str]</code> <p>The name of the column to be used to filter to the specified run_id.</p> <code>'run_id'</code> <code>run_id</code> <code>Optional[str]</code> <p>The unique identifier for a run within the table that the read data is filtered to.</p> <code>None</code> <code>partition_column</code> <code>Optional[str]</code> <p>The name of the column that the table is partitioned by.</p> <code>None</code> <code>partition_type</code> <code>Optional[BigQueryTimePartitions]</code> <p>The unit of time the table is partitioned by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <code>None</code> <code>partition_value</code> <code>Optional[Union[Tuple[str, str], str]]</code> <p>The value or pair of values for filtering the partition column to.</p> <code>None</code> <code>flatten_struct_cols</code> <code>bool</code> <p>When true, any struct type columns in the loaded dataframe are replaced with individual columns for each of the fields in the structs.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code>"},{"location":"reference/#rdsa_utils.gcp.io.outputs","title":"<code>rdsa_utils.gcp.io.outputs</code>","text":"<p>Write outputs to GCP.</p>"},{"location":"reference/#rdsa_utils.gcp.io.outputs.write_table","title":"<code>write_table(df: Union[PandasDF, SparkDF], table_name: TablePath, mode: Literal['append', 'error', 'ignore', 'overwrite'] = 'error', partition_col: Optional[str] = None, partition_type: Optional[BigQueryTimePartitions] = None, partition_expiry_days: Optional[float] = None, clustered_fields: Optional[Union[str, List[str]]] = None) -&gt; None</code>","text":"<p>Write dataframe out to a Google BigQuery table.</p> <p>In the case the table already exists, behavior of this function depends on the save mode, specified by the mode function (default to throwing an exception). When mode is Overwrite, the schema of the DataFrame does not need to be the same as that of the existing table (the column order doesn't need be the same).</p> <p>If you use the <code>df.printSchema()</code> method directly in a print/log statement the code is processed and printed regardless of logging level. Instead you need to capture the output and pass this to the logger. See explanation here - https://stackoverflow.com/a/59935109</p> <p>To learn more about the partitioning of tables and how to use them in BigQuery: https://cloud.google.com/bigquery/docs/partitioned-tables</p> <p>To learn more about the clustering of tables and how to use them in BigQuery: https://cloud.google.com/bigquery/docs/clustered-tables</p> <p>To learn more about how spark dataframes are saved to BigQuery: https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, DataFrame]</code> <p>The dataframe to be saved.</p> required <code>table_name</code> <code>TablePath</code> <p>The target BigQuery table name of form: .. required <code>mode</code> <code>Literal['append', 'error', 'ignore', 'overwrite']</code> <p>Whether to overwrite or append to the BigQuery table.     * <code>append</code>: Append contents of this :class:<code>DataFrame</code> to table.     * <code>overwrite</code>: Overwrite existing data.     * <code>error</code>: Throw exception if data already exists.     * <code>ignore</code>: Silently ignore this operation if data already exists.</p> <code>'error'</code> <code>partition_col</code> <code>Optional[str]</code> <p>A date or timestamp type column in the dataframe to use for the table partitioning.</p> <code>None</code> <code>partition_type</code> <code>Optional[BigQueryTimePartitions]</code> <p>The unit of time to partition the table by, must be one of:     * <code>hour</code>     * <code>day</code>     * <code>month</code>     * <code>year</code></p> <p>If <code>partition_col</code> is specified and <code>partition_type = None</code> then BigQuery will default to using <code>day</code> partition type.</p> <p>If 'partition_type<code>is specified and</code>partition_col = None<code>then the table will be partitioned by the ingestion time pseudo column, and can be referenced in BigQuery via either</code>_PARTITIONTIME as pt<code>or</code>_PARTITIONDATE' as pd`.</p> <p>See https://cloud.google.com/bigquery/docs/querying-partitioned-tables for more information on querying partitioned tables.</p> <code>None</code> <code>partition_expiry_days</code> <code>Optional[float]</code> <p>If specified, this is the number of days (any decimal values are converted to that proportion of a day) that BigQuery keeps the data in each partition.</p> <code>None</code> <code>clustered_fields</code> <code>Optional[Union[str, List[str]]]</code> <p>If specified, the columns (up to four) in the dataframe to cluster the data by when outputting. The order the columns are specified is important as will be the ordering of the clustering on the BigQuery table.</p> <p>See: https://cloud.google.com/bigquery/docs/querying-clustered-tables for more information on querying clustered tables.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code>"},{"location":"reference/#helpers","title":"Helpers","text":""},{"location":"reference/#rdsa_utils.helpers.pyspark","title":"<code>rdsa_utils.helpers.pyspark</code>","text":"<p>A selection of helper functions for building in pyspark.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.calc_median_price","title":"<code>calc_median_price(groups: Union[str, Sequence[str]], price_col: str = 'price') -&gt; SparkCol</code>","text":"<p>Calculate the median price per grouping level.</p> <p>Parameters:</p> Name Type Description Default <code>groups</code> <code>Union[str, Sequence[str]]</code> <p>The grouping levels for calculating the average price.</p> required <code>price_col</code> <code>str</code> <p>Column name containing the product prices.</p> <code>'price'</code> <p>Returns:</p> Type Description <code>Column</code> <p>A single entry for each grouping level, and its median price.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.convert_cols_to_struct_col","title":"<code>convert_cols_to_struct_col(df: SparkDF, struct_col_name: str, struct_cols: Optional[Sequence[str]], no_struct_col_type: T.DataTypeSingleton = T.BooleanType(), no_struct_col_value: Any = None) -&gt; SparkDF</code>","text":"<p>Convert specified selection of columns to a single struct column.</p> <p>As BigQuery tables do not take to having an empty struct column appended to them, this function will create a placeholder column to put into the struct column if no column names to combine are passed.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe that contains the columns for combining.</p> required <code>struct_col_name</code> <code>str</code> <p>The name of the resulting struct column.</p> required <code>struct_cols</code> <code>Optional[Sequence[str]]</code> <p>A sequence of columns present in df for combining.</p> required <code>no_struct_col_type</code> <code>DataTypeSingleton</code> <p>If no struct_cols are present, this is the type that the dummy column to place in the struct will be, default = BooleanType.</p> <code>BooleanType()</code> <code>no_struct_col_value</code> <code>Any</code> <p>If no struct_cols are present, this is the value that will be used in the dummy column, default = None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    The input dataframe with the specified struct_cols dropped and replaced</code> <p>with a single struct type column containing those columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not all the specified struct_cols are present in df.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.convert_struc_col_to_columns","title":"<code>convert_struc_col_to_columns(df: SparkDF, convert_nested_structs: bool = False) -&gt; SparkDF</code>","text":"<p>Flatten struct columns in pyspark dataframe to individual columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe that may or may not contain struct type columns.</p> required <code>convert_nested_structs</code> <code>bool</code> <p>If true, function will recursively call until no structs are left. Inversely, when false, only top level structs are flattened; if these contain subsequent structs they would remain.</p> <code>False</code> <p>Returns:</p> Type Description <code>    The input dataframe but with any struct type columns dropped, and in</code> <p>its place the individual fields within the struct column as individual columns.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_colname_to_value_map","title":"<code>create_colname_to_value_map(cols: Sequence[str]) -&gt; SparkCol</code>","text":"<p>Create a column name to value MapType column.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_spark_session","title":"<code>create_spark_session(app_name: Optional[str] = None, size: Optional[Literal['small', 'medium', 'large', 'extra-large']] = None, extra_configs: Optional[Dict[str, str]] = None) -&gt; SparkSession</code>","text":"<p>Create a PySpark Session based on the specified size.</p> <p>This function creates a PySpark session with different configurations based on the size specified.</p> <p>The size can be 'default', 'small', 'medium', 'large', or 'extra-large'. Extra Spark configurations can be passed as a dictionary. If no size is given, then a basic Spark session is spun up.</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>Optional[str]</code> <p>The spark session app name.</p> <code>None</code> <code>size</code> <code>Optional[Literal['small', 'medium', 'large', 'extra-large']]</code> <p>The size of the spark session to be created. It can be 'default', 'small', 'medium', 'large', or 'extra-large'.</p> <code>None</code> <code>extra_configs</code> <code>Optional[Dict[str, str]]</code> <p>Mapping of additional spark session config settings and the desired value for it. Will override any default settings.</p> <code>None</code> <p>Returns:</p> Type Description <code>SparkSession</code> <p>The created PySpark session.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified 'size' parameter is not one of the valid options: 'small', 'medium', 'large', or 'extra-large'.</p> <code>Exception</code> <p>If any other error occurs during the Spark session creation process.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = create_spark_session('medium', {'spark.ui.enabled': 'false'})\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_spark_session--session-details","title":"Session Details:","text":"<p>'small':     This is the smallest session that will realistically be used. It uses     only 1g of memory and 3 executors, and only 1 core. The number of     partitions are limited to 12, which can improve performance with     smaller data. It's recommended for simple data exploration of small     survey data or for training and demonstrations when several people     need to run Spark sessions simultaneously. 'medium':     A standard session used for analysing survey or synthetic datasets.     Also used for some Production pipelines based on survey and/or smaller     administrative data.It uses 6g of memory and 3 executors, and 3 cores.     The number of partitions are limited to 18, which can improve     performance with smaller data. 'large':     Session designed for running Production pipelines on large     administrative data, rather than just survey data. It uses 10g of     memory and 5 executors, 1g of memory overhead, and 5 cores. It uses the     default number of 200 partitions. 'extra-large':     Used for the most complex pipelines, with huge administrative     data sources and complex calculations. It uses 20g of memory and     12 executors, 2g of memory overhead, and 5 cores. It uses 240     partitions; not significantly higher than the default of 200,     but it is best for these to be a multiple of cores and executors.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.create_spark_session--references","title":"References","text":"<p>The session sizes and their details are taken directly from the following resource: \"https://best-practice-and-impact.github.io/ons-spark/spark-overview/example-spark-sessions.html\"</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.cut_lineage","title":"<code>cut_lineage(df: SparkDF) -&gt; SparkDF</code>","text":"<p>Convert the SparkDF to a Java RDD and back again.</p> <p>This function is helpful in instances where Catalyst optimizer is causing memory errors or problems, as it only tries to optimize till the conversion point.</p> <p>Note: This uses internal members and may break between versions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>SparkDF to convert.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>New SparkDF created from Java RDD.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the lineage cutting process, particularly during conversion between SparkDF and Java RDD or accessing internal members.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = rdd.toDF()\n&gt;&gt;&gt; new_df = cut_lineage(df)\n&gt;&gt;&gt; new_df.count()\n3\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.find_spark_dataframes","title":"<code>find_spark_dataframes(locals_dict: Dict[str, Union[SparkDF, Dict]]) -&gt; Dict[str, Union[SparkDF, Dict]]</code>","text":"<p>Extract SparkDF's objects from a given dictionary.</p> <p>This function scans the dictionary and returns another containing only entries where the value is a SparkDF. It also handles dictionaries within the input, including them in the output if their first item is a SparkDF.</p> <p>Designed to be used with locals() in Python, allowing extraction of all SparkDF variables in a function's local scope.</p> <p>Parameters:</p> Name Type Description Default <code>locals_dict</code> <code>Dict[str, Union[DataFrame, Dict]]</code> <p>A dictionary usually returned by locals(), with variable names as keys and their corresponding objects as values.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>A dictionary with entries from locals_dict where the value is a SparkDF or a dictionary with a SparkDF as its first item.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dfs = find_spark_dataframes(locals())\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.get_window_spec","title":"<code>get_window_spec(partition_cols: Optional[Union[str, Sequence[str]]] = None, order_cols: Optional[Union[str, Sequence[str]]] = None) -&gt; WindowSpec</code>","text":"<p>Return ordered and partitioned WindowSpec, defaulting to whole df.</p> <p>Particularly useful when you don't know if the variable being used for partition_cols will contain values or not in advance.</p> <p>Parameters:</p> Name Type Description Default <code>partition_cols</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>If present the columns to partition a spark dataframe on.</p> <code>None</code> <code>order_cols</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>If present the columns to order a spark dataframe on (where order in sequence is order that orderBy is applied).</p> <code>None</code> <p>Returns:</p> Type Description <code>WindowSpec</code> <p>The WindowSpec object to be applied.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.get_window_spec--usage","title":"Usage","text":"<p>window_spec = get_window_spec(...)</p> <p>F.sum(values).over(window_spec)</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.is_df_empty","title":"<code>is_df_empty(df: SparkDF) -&gt; bool</code>","text":"<p>Check whether a spark dataframe contains any records.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.map_column_names","title":"<code>map_column_names(df: SparkDF, mapper: Mapping[str, str]) -&gt; SparkDF</code>","text":"<p>Map column names to the given values in the mapper.</p> <p>If the column name is not in the mapper the name doesn't change.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.melt","title":"<code>melt(df: SparkDF, id_vars: Union[str, Sequence[str]], value_vars: Union[str, Sequence[str]], var_name: str = 'variable', value_name: str = 'value') -&gt; SparkDF</code>","text":"<p>Melt a spark dataframe in a pandas like fashion.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark dataframe to melt.</p> required <code>id_vars</code> <code>Union[str, Sequence[str]]</code> <p>The names of the columns to use as identifier variables.</p> required <code>value_vars</code> <code>Union[str, Sequence[str]]</code> <p>The names of the columns containing the data to unpivot.</p> required <code>var_name</code> <code>str</code> <p>The name of the target column containing variable names (i.e. the original column names).</p> <code>'variable'</code> <code>value_name</code> <code>str</code> <p>The name of the target column containing the unpivoted data.</p> <code>'value'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The \"melted\" input data as a pyspark data frame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame(\n...     [[1, 2, 3, 4],\n...      [5, 6, 7, 8],\n...      [9, 10, 11, 12]],\n...     [\"col1\", \"col2\", \"col3\", \"col4\"])\n&gt;&gt;&gt; melt(df=df, id_vars=\"col1\", value_vars=[\"col2\", \"col3\"]).show()\n+----+--------+-----+\n|col1|variable|value|\n+----+--------+-----+\n|   1|    col2|    2|\n|   1|    col3|    3|\n|   5|    col2|    6|\n|   5|    col3|    7|\n|   9|    col2|   10|\n|   9|    col3|   11|\n+----+--------+-----+\n</code></pre> <pre><code>&gt;&gt;&gt; melt(df=df, id_vars=[\"col1\", \"col2\"], value_vars=[\"col3\", \"col4\"]\n... ).show()\n+----+----+--------+-----+\n|col1|col2|variable|value|\n+----+----+--------+-----+\n|   1|   2|    col3|    3|\n|   1|   2|    col4|    4|\n|   5|   6|    col3|    7|\n|   5|   6|    col4|    8|\n|   9|  10|    col3|   11|\n|   9|  10|    col4|   12|\n+----+----+--------+-----+\n</code></pre>"},{"location":"reference/#rdsa_utils.helpers.pyspark.rank_numeric","title":"<code>rank_numeric(numeric: Union[str, Sequence[str]], group: Union[str, Sequence[str]], ascending: bool = False) -&gt; SparkCol</code>","text":"<p>Rank a numeric and assign a unique value to each row.</p> <p>The <code>F.row_number()</code> method has been selected as a method to rank as gives a unique number to each row. Other methods such as <code>F.rank()</code> and <code>F.dense_rank()</code> do not assign unique values per row.</p> <p>Parameters:</p> Name Type Description Default <code>numeric</code> <code>Union[str, Sequence[str]]</code> <p>The column name or list of column names containing values which will be ranked.</p> required <code>group</code> <code>Union[str, Sequence[str]]</code> <p>The grouping levels to rank the numeric column or columns over.</p> required <code>ascending</code> <code>bool</code> <p>Dictates whether high or low values are ranked as the top value.</p> <code>False</code> <p>Returns:</p> Type Description <code>Column</code> <p>Contains a rank for the row in its grouping level.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.select_first_obs_appearing_in_group","title":"<code>select_first_obs_appearing_in_group(df: SparkDF, group: Sequence[str], date_col: str, ascending: bool) -&gt; SparkDF</code>","text":"<p>Rank and select observation in group based on earliest or latest date.</p> <p>Given that there can be multiple observations per group, select observation that appears first or last (depending on whether ascending is set to True or False, respectively).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe that contains the group and date_col.</p> required <code>group</code> <code>Sequence[str]</code> <p>The grouping levels required to find the observation that appears first or last (depending on whether ascending is set to True or False, respectively)</p> required <code>date_col</code> <code>str</code> <p>Column name containing the dates of each observation.</p> required <code>ascending</code> <code>bool</code> <p>Dictates whether first or last observation within a grouping is selected (depending on whether ascending is set to True or False, respectively).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe that contains each observation per group that appeared first or last (depending on whether ascending is set to True or False, respectively) according to date_col.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.set_df_columns_nullable","title":"<code>set_df_columns_nullable(df: SparkDF, column_list: List[str], nullable: Optional[bool] = True) -&gt; SparkDF</code>","text":"<p>Change specified columns nullable value.</p> <p>Sometimes find that spark creates columns that have the nullable attribute set to False, which can cause issues if this dataframe is saved to a table as it will set the schema for that column to not allow missing values.</p> <p>Changing this parameter for a column appears to be very difficult (and also potentially costly [see so answer comments] - SO USE ONLY IF NEEDED).</p> <p>The solution implemented is taken from Stack Overflow post: https://stackoverflow.com/a/51821437</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe with columns to have nullable attribute changed.</p> required <code>column_list</code> <code>List[str]</code> <p>List of columns to change nullable attribute.</p> required <code>nullable</code> <code>Optional[bool]</code> <p>The value to set the nullable attribute to for the specified columns.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input dataframe but with nullable attribute changed for specified columns.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.to_list","title":"<code>to_list(df: SparkDF) -&gt; List[Union[Any, List[Any]]]</code>","text":"<p>Convert Spark DF to a list.</p> <p>Returns:</p> Type Description <code>list or list of lists</code> <p>If the input DataFrame has a single column then a list of column values will be returned. If the DataFrame has multiple columns then a list of row data as lists will be returned.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.to_spark_col","title":"<code>to_spark_col(_func=None, *, exclude: Sequence[str] = None) -&gt; Callable</code>","text":"<p>Convert str args to Spark Column if not already.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.to_spark_col--usage","title":"Usage","text":"<p>Use as a decorator on a function.</p> <p>To convert all string arguments to spark column</p> <p>@to_spark_col def my_func(arg1, arg2)</p> <p>To exclude a string arguments from being converted to a spark column</p> <p>@to_spark_col(exclude=['arg2']) def my_func(arg1, arg2)</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.transform","title":"<code>transform(self, f, *args, **kwargs)</code>","text":"<p>Chain Pyspark function.</p>"},{"location":"reference/#rdsa_utils.helpers.pyspark.unpack_list_col","title":"<code>unpack_list_col(df: SparkDF, list_col: str, unpacked_col: str) -&gt; SparkDF</code>","text":"<p>Unpack a spark column containing a list into multiple rows.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Contains the list column to unpack.</p> required <code>list_col</code> <code>str</code> <p>The name of the column which contains lists.</p> required <code>unpacked_col</code> <code>str</code> <p>The name of the column containing the unpacked list items.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Contains a new row for each unpacked list item.</p>"},{"location":"reference/#rdsa_utils.helpers.python","title":"<code>rdsa_utils.helpers.python</code>","text":"<p>Miscellaneous helper functions for Python.</p>"},{"location":"reference/#rdsa_utils.helpers.python.always_iterable_local","title":"<code>always_iterable_local(obj: Any) -&gt; Callable</code>","text":"<p>Supplement more-itertools <code>always_iterable</code> to also exclude dicts.</p> <p>By default it would convert a dictionary to an iterable of just its keys, dropping all the values. This change makes it so dictionaries are not altered (similar to how strings aren't broken down).</p>"},{"location":"reference/#rdsa_utils.helpers.python.calc_product_of_dict_values","title":"<code>calc_product_of_dict_values(**kwargs: Mapping[str, Union[str, float, Iterable]]) -&gt; Mapping[str, any]</code>","text":"<p>Create cartesian product of values for each kwarg.</p> <p>In order to create product of values, the values are converted to a list so that product of values can be derived.</p> <p>Yields:</p> Type Description <code>    Next result of cartesian product of kwargs values.</code>"},{"location":"reference/#rdsa_utils.helpers.python.calc_product_of_dict_values--example","title":"Example","text":"<p>my_dict = {     'key1': 1,     'key2': [2, 3, 4] }</p> <p>list(calc_product_of_dict_values(**my_dict))</p> <p>[{'key1': 1, 'key2': 2}, {'key1': 1, 'key2': 3}, {'key1': 1, 'key2': 4}]</p>"},{"location":"reference/#rdsa_utils.helpers.python.calc_product_of_dict_values--notes","title":"Notes","text":"<p>Modified from: https://stackoverflow.com/a/5228294</p>"},{"location":"reference/#rdsa_utils.helpers.python.convert_date_strings_to_datetimes","title":"<code>convert_date_strings_to_datetimes(start_date: str, end_date: str) -&gt; Tuple[pd.Timestamp, pd.Timestamp]</code>","text":"<p>Convert start and end dates from strings to timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Datetime like object which is used to define the start date for filter. Acceptable string formats include (but not limited to): MMMM YYYY, YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified the start_date is set as first day of month.</p> required <code>end_date</code> <code>str</code> <p>Datetime like object which is used to define the start date for filter. Acceptable string formats include (but not limited to): MMMM YYYY, YYYY-MM, YYYY-MM-DD, DD MMM YYYY etc. If only month and year specified the end_date is set as final day of month.</p> required <p>Returns:</p> Type Description <code>tuple[Timestamp, Timestamp]</code> <p>Tuple where the first value is the start date and the second the end date.</p>"},{"location":"reference/#rdsa_utils.helpers.python.extend_lists","title":"<code>extend_lists(sections: List[List[str]], elements_to_add: List[str]) -&gt; None</code>","text":"<p>Check list elements are unique then append to existing list.</p> <p>Note the <code>.extend</code> method in Python overwrites each section. There is no need to assign a variable to this function, the section will update automatically.</p> <p>The function can be used with the <code>load_config</code> function to extend a value list in a config yaml file. For example with a <code>config.yaml</code> file as per below:</p> <pre><code>input_columns\n    - col_a\n    - col_b\n\noutput_columns\n    - col_b\n</code></pre> <p>To add column <code>col_c</code> the function can be utilised as follows:</p> <pre><code>config = load_config(\"config.yaml\")\n\nsections = [config['input_columns'], config['output_columns']]\nelements_to_add = ['col_c']\n\nextend_lists(sections, elements_to_add)\n</code></pre> <p>The output will be as follows.</p> <pre><code>input_columns\n    - col_a\n    - col_b\n    - col_c\n\noutput_columns\n    - col_b\n    - col_c\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sections</code> <code>List[List[str]]</code> <p>The section to be updated with the extra elements.</p> required <code>elements_to_add</code> <code>List[str]</code> <p>The new elements to add to the specified sections.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Note the <code>.extend</code> method in Python overwrites the sections. There is no need to assign a variable to this function, the section will update automatically.</p>"},{"location":"reference/#rdsa_utils.helpers.python.list_convert","title":"<code>list_convert(obj: Any) -&gt; List[Any]</code>","text":"<p>Convert object to list using more-itertools' <code>always_iterable</code>.</p>"},{"location":"reference/#rdsa_utils.helpers.python.overwrite_dictionary","title":"<code>overwrite_dictionary(base_dict: Mapping[str, Any], override_dict: Mapping[str, Any]) -&gt; Dict[str, Any]</code>","text":"<p>Overwrite dictionary values with user defined values.</p> <p>The following restrictions are in place: * base_dict and override_dict have the same value which is not dictionary   then override_dict has priority. * If base_dict contains dictionary and override_dict contains a value (e.g.   string or list) with the same key, priority is upon base_dict and   the override_dict value is ignored. * If key is in override_dict but not in base_dict then an Exception is   raised and code stops. * Any other restrictions will require code changes.</p> <p>Parameters:</p> Name Type Description Default <code>base_dict</code> <code>Mapping[str, Any]</code> <p>Dictionary containing existing key value pairs.</p> required <code>override_dict</code> <code>Mapping[str, Any]</code> <p>Dictionary containing new keys/values to inset into base_dict.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The base_dict with any keys matching the override_dict being replaced. Any keys not present in base_dict are appended.</p>"},{"location":"reference/#rdsa_utils.helpers.python.overwrite_dictionary--example","title":"Example","text":"<p>dic1 = {\"var1\": \"value1\", \"var2\": {\"var3\": 1.1, \"var4\": 4.4}, \"var5\": [1, 2, 3]} dic2 = {\"var2\": {\"var3\": 9.9}}</p> <p>overwrite_dictionary(dic1, dic2) {'var1': 'value1', 'var2': {'var3': 9.9, 'var4': 4.4}, 'var5': [1, 2, 3]}</p> <p>dic3 = {\"var2\": {\"var3\": 9.9}, \"var6\": -1} overwrite_dictionary(dic1, dic3) ERROR main: ('var6', -1) not in base_dict</p>"},{"location":"reference/#rdsa_utils.helpers.python.overwrite_dictionary--notes","title":"Notes","text":"<p>Modified from: https://stackoverflow.com/a/58742155</p>"},{"location":"reference/#rdsa_utils.helpers.python.overwrite_dictionary--warning","title":"Warning","text":"<p>Due to recursive nature of function, the function will overwrite the base_dict object that is passed into the original function.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a key is present in override_dict but not base_dict.</p>"},{"location":"reference/#rdsa_utils.helpers.python.tuple_convert","title":"<code>tuple_convert(obj: Any) -&gt; Tuple[Any]</code>","text":"<p>Convert object to tuple using more-itertools' <code>always_iterable</code>.</p>"},{"location":"reference/#io","title":"IO","text":""},{"location":"reference/#rdsa_utils.io.config","title":"<code>rdsa_utils.io.config</code>","text":"<p>Module for code relating to loading config files.</p>"},{"location":"reference/#rdsa_utils.io.config.LoadConfig","title":"<code>LoadConfig(config_path: Union[CloudPath, Path], config_overrides: Optional[Config] = None, config_type: Optional[Literal['json', 'toml', 'yaml']] = None, config_validators: Optional[Dict[str, BaseModel]] = None)</code>","text":"<p>Class for loading and storing a configuration file.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>The loaded config stored as a dictionary.</p> <code>config_dir</code> <p>The logical parent directory of loaded <code>config_path</code>.</p> <code>config_file</code> <p>The file name of the loaded <code>config_path</code>.</p> <code>config_original</code> <p>The configuration dictionary as initially loaded, prior to applying any overrides or validation.</p> <code>config_overrides</code> <p>The configuration override dictionary, if provided.</p> <code>config_path</code> <p>The path of the loaded config file.</p> <code>config_type</code> <p>The file type of the loaded config file.</p> <code>config_validators</code> <p>The validators used to validate the loaded config, if provided.</p> <code>**attrs</code> <p>Every top level key in the loaded config is also set as an attribute to allow simpler access to each config section.</p> <p>Init method.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[CloudPath, Path]</code> <p>The path of the config file to be loaded.</p> required <code>config_overrides</code> <code>Optional[Config]</code> <p>A dictionary containing a subset of the keys and values of the config file that is initially loaded, by default None. If values are provided that are not in the initial config then a ConfigError is raised.</p> <code>None</code> <code>optional</code> <code>Optional[Config]</code> <p>A dictionary containing a subset of the keys and values of the config file that is initially loaded, by default None. If values are provided that are not in the initial config then a ConfigError is raised.</p> <code>None</code> <code>config_type</code> <code>Optional[Literal['json', 'toml', 'yaml']]</code> <p>The file type of the config file being loaded, by default None. If not specified then this is inferred from the <code>config_path</code>.</p> <code>None</code> <code>optional</code> <code>Optional[Literal['json', 'toml', 'yaml']]</code> <p>The file type of the config file being loaded, by default None. If not specified then this is inferred from the <code>config_path</code>.</p> <code>None</code> <code>config_validators</code> <code>Optional[Dict[str, BaseModel]]</code> <p>A dictionary made up of key, value pairs where the keys refer to the top level sections of the loaded config, and the values are a pydantic validation class for the section, by default None. If only some of the keys are specified with validators, warnings are raised to alert that they have not been validated.</p> <code>None</code> <code>optional</code> <code>Optional[Dict[str, BaseModel]]</code> <p>A dictionary made up of key, value pairs where the keys refer to the top level sections of the loaded config, and the values are a pydantic validation class for the section, by default None. If only some of the keys are specified with validators, warnings are raised to alert that they have not been validated.</p> <code>None</code>"},{"location":"reference/#rdsa_utils.io.input","title":"<code>rdsa_utils.io.input</code>","text":"<p>Module containing generic input functionality code.</p>"},{"location":"reference/#rdsa_utils.io.input.parse_json","title":"<code>parse_json(data: str) -&gt; Config</code>","text":"<p>Parse JSON formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard JSON-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p> <p>Raises:</p> Type Description <code>JSONDecodeError</code> <p>If the string format of config_overrides cannot be decoded by json.loads (i.e. converted to a dictionary).</p>"},{"location":"reference/#rdsa_utils.io.input.parse_toml","title":"<code>parse_toml(data: str) -&gt; Config</code>","text":"<p>Parse TOML formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard TOML-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p>"},{"location":"reference/#rdsa_utils.io.input.parse_yaml","title":"<code>parse_yaml(data: str) -&gt; Config</code>","text":"<p>Parse YAML formatted string into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String containing standard YAML-formatted data.</p> required <p>Returns:</p> Type Description <code>Config</code> <p>A dictionary containing the parsed data.</p>"},{"location":"reference/#rdsa_utils.io.input.read_file","title":"<code>read_file(file: Union[CloudPath, Path]) -&gt; str</code>","text":"<p>Load contents of specified file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[CloudPath, Path]</code> <p>The absolute file path of the file to be read.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The contents of the provided file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the provided file does not exist.</p>"},{"location":"reference/#rdsa_utils.io.output","title":"<code>rdsa_utils.io.output</code>","text":"<p>Module containing generic output functionality code.</p>"},{"location":"reference/#methods","title":"Methods","text":""},{"location":"reference/#rdsa_utils.methods.averaging_methods","title":"<code>rdsa_utils.methods.averaging_methods</code>","text":"<p>Weighted and unweighted averaging functions.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.get_weight_shares","title":"<code>get_weight_shares(weights: str, levels: Optional[Union[str, Sequence[str]]] = None) -&gt; SparkCol</code>","text":"<p>Divide weights by sum of weights for each group.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.unweighted_arithmetic_average","title":"<code>unweighted_arithmetic_average(val: str) -&gt; SparkCol</code>","text":"<p>Calculate the unweighted arithmetic average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.unweighted_geometric_average","title":"<code>unweighted_geometric_average(val: str) -&gt; SparkCol</code>","text":"<p>Calculate the unweighted geometric average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.weighted_arithmetic_average","title":"<code>weighted_arithmetic_average(val: str, weight: str) -&gt; SparkCol</code>","text":"<p>Calculate the weighted arithmetic average.</p>"},{"location":"reference/#rdsa_utils.methods.averaging_methods.weighted_geometric_average","title":"<code>weighted_geometric_average(val: str, weight: str) -&gt; SparkCol</code>","text":"<p>Calculate the weighted geometric average.</p>"}]}